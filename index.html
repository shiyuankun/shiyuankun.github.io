<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>彼格海德的笔记空间</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="python">
<meta property="og:type" content="website">
<meta property="og:title" content="彼格海德的笔记空间">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="彼格海德的笔记空间">
<meta property="og:description" content="python">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="彼格海德的笔记空间">
<meta name="twitter:description" content="python">
  
    <link rel="alternative" href="/atom.xml" title="彼格海德的笔记空间" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-62861406-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  
<!-- Baidu Statistics -->
<script type="text/javascript">
    var _hmt = _hmt || [];
(function() {
 var hm = document.createElement("script");
 hm.src = "//hm.baidu.com/hm.js?77faa6f8da10ec42fb1c01f2947de873";
 var s = document.getElementsByTagName("script")[0]; 
 s.parentNode.insertBefore(hm, s);
 })();
</script>
<!-- End Baidu Statistics -->


</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">彼格海德的笔记空间</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">a notebook for python</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/about/index.html">About</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="q" value="site:http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-explore-spark-distributed-mllib" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/06/21/explore-spark-distributed-mllib/" class="article-date">
  <time datetime="2015-06-21T04:26:50.000Z" itemprop="datePublished">2015-06-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/06/21/explore-spark-distributed-mllib/">spark的mllib到底是不是分布式的</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>端午节，群里有个人问了一个spark关于mllib扩展性的问题，不太确定，继续看代码。正好上周1.4也发布了，也把代码下下来比较了一下。<br>比较以后才知道，原来scala版本的计算和pyspark差了好多，pyspark关于linalg的计算都是直接调用numpy，也就是非并行的。例如linalg.py里随处可见的np.dot</p>
<pre><code>assert <span class="function"><span class="title">len</span><span class="params">(self)</span></span> == other<span class="class">.shape</span>[<span class="number">0</span>], <span class="string">"dimension mismatch"</span>
             return other.<span class="function"><span class="title">transpose</span><span class="params">()</span></span>.<span class="function"><span class="title">dot</span><span class="params">(self.toArray()</span></span>)
         <span class="keyword">else</span>:
             assert <span class="function"><span class="title">len</span><span class="params">(self)</span></span> == _vector_size(other), <span class="string">"dimension mismatch"</span>
             <span class="keyword">if</span> <span class="function"><span class="title">isinstance</span><span class="params">(other, SparseVector)</span></span>:
                 return other.<span class="function"><span class="title">dot</span><span class="params">(self)</span></span>
             elif <span class="function"><span class="title">isinstance</span><span class="params">(other, Vector)</span></span>:
                 return np.<span class="function"><span class="title">dot</span><span class="params">(self.toArray()</span></span>, other.<span class="function"><span class="title">toArray</span><span class="params">()</span></span>)
             <span class="keyword">else</span>:
                 return np.<span class="function"><span class="title">dot</span><span class="params">(self.toArray()</span></span>, other)
</code></pre><p>但scala版本的linalg多了一个distributed的目录，下面就定义了好几种分布式矩阵</p>
<pre><code>:~/spark-<span class="number">1.4</span>.<span class="number">0</span>$ ls mllib/src/<span class="keyword">main</span>/scala/org/apache/spark/mllib/linalg/distributed/*
mllib/src/<span class="keyword">main</span>/scala/org/apache/spark/mllib/linalg/distributed/<span class="constant">Block</span>Matrix.scala
mllib/src/<span class="keyword">main</span>/scala/org/apache/spark/mllib/linalg/distributed/<span class="constant">Coordinate</span>Matrix.scala
mllib/src/<span class="keyword">main</span>/scala/org/apache/spark/mllib/linalg/distributed/<span class="constant">Distributed</span>Matrix.scala
mllib/src/<span class="keyword">main</span>/scala/org/apache/spark/mllib/linalg/distributed/<span class="constant">Indexed</span>RowMatrix.scala
mllib/src/<span class="keyword">main</span>/scala/org/apache/spark/mllib/linalg/distributed/<span class="constant">Row</span>Matrix.scala
</code></pre><p>除此以外还多了SVD，PCA一些常见矩阵运算。当然，在linalg中的BLAS.scala里面，依旧是调用了本地的blas库来计算矩阵乘法</p>
<pre><code>private <span class="function"><span class="keyword">def</span> <span class="title">dot</span><span class="params">(x: DenseVector, y: DenseVector)</span>:</span> Double = {
  val n = x.size
  f2jBLAS.ddot(n, x.values, <span class="number">1</span>, y.values, <span class="number">1</span>)
}     
</code></pre><p>这里仅限于非稀疏矩阵，spark中所有的稀疏矩阵都是自己写的没有调用额外的库</p>
<pre><code><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">dot</span>(</span>x: <span class="type">SparseVector</span>, y: <span class="type">SparseVector</span>): <span class="type">Double</span> = {
     <span class="function"><span class="keyword">val</span> <span class="title">xValues</span> =</span> x.values
     <span class="function"><span class="keyword">val</span> <span class="title">xIndices</span> =</span> x.indices
     <span class="function"><span class="keyword">val</span> <span class="title">yValues</span> =</span> y.values
     <span class="function"><span class="keyword">val</span> <span class="title">yIndices</span> =</span> y.indices
     <span class="function"><span class="keyword">val</span> <span class="title">nnzx</span> =</span> xIndices.size
     <span class="function"><span class="keyword">val</span> <span class="title">nnzy</span> =</span> yIndices.size

     <span class="keyword">var</span> kx = <span class="number">0</span>
     <span class="keyword">var</span> ky = <span class="number">0</span>
     <span class="keyword">var</span> sum = <span class="number">0.0</span>
     <span class="comment">// y catching x</span>
     <span class="keyword">while</span> (kx &lt; nnzx &amp;&amp; ky &lt; nnzy) {
       <span class="function"><span class="keyword">val</span> <span class="title">ix</span> =</span> xIndices(kx)
       <span class="keyword">while</span> (ky &lt; nnzy &amp;&amp; yIndices(ky) &lt; ix) {
         ky += <span class="number">1</span>
       }
       <span class="keyword">if</span> (ky &lt; nnzy &amp;&amp; yIndices(ky) == ix) {
         sum += xValues(kx) * yValues(ky)
         ky += <span class="number">1</span>
       }
       kx += <span class="number">1</span>
     }
     sum
   }
</code></pre><p>矩阵乘法也是一样处理，都是牵涉到稀疏矩阵计算的就用自己写的方法</p>
<pre><code>private def gemm(
       alpha: Double,
       A: DenseMatrix,
       B: DenseMatrix,
       beta: Double,
       C: DenseMatrix): <span class="variable">Unit =</span> {
     val <span class="variable">tAstr =</span> <span class="keyword">if</span> (A.isTransposed) <span class="string">"T"</span> <span class="keyword">else</span> <span class="string">"N"</span>
     val <span class="variable">tBstr =</span> <span class="keyword">if</span> (B.isTransposed) <span class="string">"T"</span> <span class="keyword">else</span> <span class="string">"N"</span>
     val <span class="variable">lda =</span> <span class="keyword">if</span> (!A.isTransposed) A.numRows <span class="keyword">else</span> A.numCols
     val <span class="variable">ldb =</span> <span class="keyword">if</span> (!B.isTransposed) B.numRows <span class="keyword">else</span> B.numCols

     require(A.<span class="variable">numCols =</span>= B.numRows,
       s<span class="string">"The columns of A don't match the rows of B. A: <span class="subst">${A.numCols}</span>, B: <span class="subst">${B.numRows}</span>"</span>)
     require(A.<span class="variable">numRows =</span>= C.numRows,
       s<span class="string">"The rows of C don't match the rows of A. C: <span class="subst">${C.numRows}</span>, A: <span class="subst">${A.numRows}</span>"</span>)
     require(B.<span class="variable">numCols =</span>= C.numCols,
       s<span class="string">"The columns of C don't match the columns of B. C: <span class="subst">${C.numCols}</span>, A: <span class="subst">${B.numCols}</span>"</span>)
     nativeBLAS.dgemm(tAstr, tBstr, A.numRows, B.numCols, A.numCols, alpha, A.values, lda,
       B.values, ldb, beta, C.values, C.numRows)
   }
</code></pre><p>然后，我们回到spark的分布式矩阵，分布式矩阵的好处是直接利用rdd就能将对应的值转化为矩阵，例如这样</p>
<pre><code>val rows = sc.textFile(args(<span class="number">0</span>)).<span class="built_in">map</span> { <span class="built_in">line</span> =&gt;
       val values = <span class="built_in">line</span>.<span class="built_in">split</span>(<span class="string">' '</span>).<span class="built_in">map</span>(_.toDouble)
       Vectors.dense(values)
     }
     val mat = <span class="keyword">new</span> RowMatrix(rows)
</code></pre><p>这样，mat就是一个分布式的RowMatrix了，所做的就是赋值一个rdd。对于做SVD这类计算来说，RowMatrix定义了三种方法LocalARPACK，LocalLAPACK和DistARPACK，根据m，n的大小来自动决定到底用哪个计算</p>
<pre><code><span class="keyword">if</span> (n &lt; <span class="number">100</span> || (k &gt; n / <span class="number">2</span> &amp;&amp; n &lt;= <span class="number">15000</span>)) {
          <span class="regexp">//</span> If n <span class="keyword">is</span> small <span class="keyword">or</span> k <span class="keyword">is</span> large compared <span class="reserved">with</span> n, we better compute the Gramian matrix first
          <span class="regexp">//</span> <span class="keyword">and</span> <span class="keyword">then</span> compute its eigenvalues locally, instead <span class="keyword">of</span> making multiple passes.
          <span class="keyword">if</span> (k &lt; n / <span class="number">3</span>) {
            SVDMode.LocalARPACK
          } <span class="keyword">else</span> {
            SVDMode.LocalLAPACK
          }
        } <span class="keyword">else</span> {
          <span class="regexp">//</span> If k <span class="keyword">is</span> small compared <span class="reserved">with</span> n, we use ARPACK <span class="reserved">with</span> distributed multiplication.
          SVDMode.DistARPACK
        }
</code></pre><p>基本上矩阵尺寸不大的时候就用local的方法来计算了，似乎作者认为ARPACK在计算方阵的时候效率不如lapack。所以当k&gt;n/3的时候选择了lapack。选完了以后就开始计算对角化矩阵</p>
<pre><code><span class="function"><span class="keyword">val</span> (</span>sigmaSquares: <span class="type">BDV</span>[<span class="type">Double</span>], u: <span class="type">BDM</span>[<span class="type">Double</span>]) = computeMode <span class="keyword">match</span> {
       <span class="keyword">case</span> <span class="type">SVDMode</span>.<span class="type">LocalARPACK</span> =&gt;
         require(k &lt; n, s<span class="string">"k must be smaller than n in local-eigs mode but got k=$k and n=$n."</span>)
         <span class="function"><span class="keyword">val</span> <span class="title">G</span> =</span> computeGramianMatrix().toBreeze.asInstanceOf[<span class="type">BDM</span>[<span class="type">Double</span>]]
         <span class="type">EigenValueDecomposition</span>.symmetricEigs(v =&gt; <span class="type">G</span> * v, n, k, tol, maxIter)
       <span class="keyword">case</span> <span class="type">SVDMode</span>.<span class="type">LocalLAPACK</span> =&gt;
         <span class="comment">// breeze (v0.10) svd latent constraint, 7 * n * n + 4 * n &lt; Int.MaxValue</span>
         require(n &lt; <span class="number">17515</span>, s<span class="string">"$n exceeds the breeze svd capability"</span>)
         <span class="function"><span class="keyword">val</span> <span class="title">G</span> =</span> computeGramianMatrix().toBreeze.asInstanceOf[<span class="type">BDM</span>[<span class="type">Double</span>]]
         <span class="function"><span class="keyword">val</span> <span class="title">brzSvd</span>.<span class="title">SVD</span>(</span>uFull: <span class="type">BDM</span>[<span class="type">Double</span>], sigmaSquaresFull: <span class="type">BDV</span>[<span class="type">Double</span>], _) = brzSvd(<span class="type">G</span>)
         (sigmaSquaresFull, uFull)
       <span class="keyword">case</span> <span class="type">SVDMode</span>.<span class="type">DistARPACK</span> =&gt;
         <span class="keyword">if</span> (rows.getStorageLevel == <span class="type">StorageLevel</span>.<span class="type">NONE</span>) {
           logWarning(<span class="string">"The input data is not directly cached, which may hurt performance if its"</span>
             + <span class="string">" parent RDDs are also uncached."</span>)
         }
         require(k &lt; n, s<span class="string">"k must be smaller than n in dist-eigs mode but got k=$k and n=$n."</span>)
         <span class="type">EigenValueDecomposition</span>.symmetricEigs(multiplyGramianMatrixBy, n, k, tol, maxIter)
         }
</code></pre><p>这里不管lapack怎么算（其实两者最后都是调用了brzSvd去计算的），ARPACK是调用了EigenValueDecomposition去计算的，并且要求一个Gramian矩阵</p>
<pre><code><span class="keyword">private</span>[mllib] <span class="function"><span class="keyword">def</span> <span class="title">multiplyGramianMatrixBy</span>(</span>v: <span class="type">BDV</span>[<span class="type">Double</span>]): <span class="type">BDV</span>[<span class="type">Double</span>] = {
     <span class="function"><span class="keyword">val</span> <span class="title">n</span> =</span> numCols().toInt
     <span class="function"><span class="keyword">val</span> <span class="title">vbr</span> =</span> rows.context.broadcast(v)
     rows.treeAggregate(<span class="type">BDV</span>.zeros[<span class="type">Double</span>](n))(
       seqOp = (<span class="type">U</span>, r) =&gt; {
         <span class="function"><span class="keyword">val</span> <span class="title">rBrz</span> =</span> r.toBreeze
         <span class="function"><span class="keyword">val</span> <span class="title">a</span> =</span> rBrz.dot(vbr.value)
         rBrz <span class="keyword">match</span> {
           <span class="comment">// use specialized axpy for better performance</span>
           <span class="keyword">case</span> _: <span class="type">BDV</span>[_] =&gt; brzAxpy(a, rBrz.asInstanceOf[<span class="type">BDV</span>[<span class="type">Double</span>]], <span class="type">U</span>)
           <span class="keyword">case</span> _: <span class="type">BSV</span>[_] =&gt; brzAxpy(a, rBrz.asInstanceOf[<span class="type">BSV</span>[<span class="type">Double</span>]], <span class="type">U</span>)
           <span class="keyword">case</span> _ =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnsupportedOperationException</span>(
             s<span class="string">"Do not support vector operation from type ${rBrz.getClass.getName}."</span>)
         }
         <span class="type">U</span>
       }, combOp = (<span class="type">U1</span>, <span class="type">U2</span>) =&gt; <span class="type">U1</span> += <span class="type">U2</span>)
   }
</code></pre><p>这个计算用到了broadcast，头一次看到将数据分发出去-_-bb。然后就在各自本地机器上计算。给定一个矩阵x，他的列向量的Gramian矩阵其实就是x’<em>x，他的行向量的Gramian矩阵就是x</em>x’。</p>
<pre><code><span class="keyword">private</span>[mllib] <span class="class"><span class="keyword">object</span> <span class="title">EigenValueDecomposition</span> {</span>
  <span class="comment">/**
   * Compute the leading k eigenvalues and eigenvectors on a symmetric square matrix using ARPACK.
   * The caller needs to ensure that the input matrix is real symmetric. This function requires
   * memory for `n*(4*k+4)` doubles.
   *
   * @param mul a function that multiplies the symmetric matrix with a DenseVector.
   * @param n dimension of the square matrix (maximum Int.MaxValue).
   * @param k number of leading eigenvalues required, 0 &lt; k &lt; n.
   * @param tol tolerance of the eigs computation.
   * @param maxIterations the maximum number of Arnoldi update iterations.
   * @return a dense vector of eigenvalues in descending order and a dense matrix of eigenvectors
   *         (columns of the matrix).
   * @note The number of computed eigenvalues might be smaller than k when some Ritz values do not
   *       satisfy the convergence criterion specified by tol (see ARPACK Users Guide, Chapter 4.6
   *       for more details). The maximum number of Arnoldi update iterations is set to 300 in this
   *       function.
   */</span>
  <span class="keyword">private</span>[mllib] <span class="function"><span class="keyword">def</span> <span class="title">symmetricEigs</span>(</span>
      mul: <span class="type">BDV</span>[<span class="type">Double</span>] =&gt; <span class="type">BDV</span>[<span class="type">Double</span>],
      n: <span class="type">Int</span>,
      k: <span class="type">Int</span>,
      tol: <span class="type">Double</span>,
      maxIterations: <span class="type">Int</span>): (<span class="type">BDV</span>[<span class="type">Double</span>], <span class="type">BDM</span>[<span class="type">Double</span>]) = {
    <span class="comment">// TODO: remove this function and use eigs in breeze when switching breeze version</span>
    require(n &gt; k, s<span class="string">"Number of required eigenvalues $k must be smaller than matrix dimension $n"</span>)

    <span class="function"><span class="keyword">val</span> <span class="title">arpack</span> =</span> <span class="type">ARPACK</span>.getInstance()

    <span class="comment">// tolerance used in stopping criterion</span>
    <span class="function"><span class="keyword">val</span> <span class="title">tolW</span> =</span> <span class="keyword">new</span> doubleW(tol)
    <span class="comment">// number of desired eigenvalues, 0 &lt; nev &lt; n</span>
    <span class="function"><span class="keyword">val</span> <span class="title">nev</span> =</span> <span class="keyword">new</span> intW(k)
    <span class="comment">// nev Lanczos vectors are generated in the first iteration</span>
    <span class="comment">// ncv-nev Lanczos vectors are generated in each subsequent iteration</span>
    <span class="comment">// ncv must be smaller than n</span>
    <span class="function"><span class="keyword">val</span> <span class="title">ncv</span> =</span> math.min(<span class="number">2</span> * k, n)

    <span class="comment">// "I" for standard eigenvalue problem, "G" for generalized eigenvalue problem</span>
    <span class="function"><span class="keyword">val</span> <span class="title">bmat</span> =</span> <span class="string">"I"</span>
    <span class="comment">// "LM" : compute the NEV largest (in magnitude) eigenvalues</span>
    <span class="function"><span class="keyword">val</span> <span class="title">which</span> =</span> <span class="string">"LM"</span>

    <span class="keyword">var</span> iparam = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](<span class="number">11</span>)
    <span class="comment">// use exact shift in each iteration</span>
    iparam(<span class="number">0</span>) = <span class="number">1</span>
    <span class="comment">// maximum number of Arnoldi update iterations, or the actual number of iterations on output</span>
    iparam(<span class="number">2</span>) = maxIterations
    <span class="comment">// Mode 1: A*x = lambda*x, A symmetric</span>
    iparam(<span class="number">6</span>) = <span class="number">1</span>

    require(n * ncv.toLong &lt;= <span class="type">Integer</span>.<span class="type">MAX_VALUE</span> &amp;&amp; ncv * (ncv.toLong + <span class="number">8</span>) &lt;= <span class="type">Integer</span>.<span class="type">MAX_VALUE</span>,
      s<span class="string">"k = $k and/or n = $n are too large to compute an eigendecomposition"</span>)

    <span class="keyword">var</span> ido = <span class="keyword">new</span> intW(<span class="number">0</span>)
    <span class="keyword">var</span> info = <span class="keyword">new</span> intW(<span class="number">0</span>)
    <span class="keyword">var</span> resid = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Double</span>](n)
    <span class="keyword">var</span> v = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Double</span>](n * ncv)
    <span class="keyword">var</span> workd = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Double</span>](n * <span class="number">3</span>)
    <span class="keyword">var</span> workl = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Double</span>](ncv * (ncv + <span class="number">8</span>))
    <span class="keyword">var</span> ipntr = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](<span class="number">11</span>)

    <span class="comment">// call ARPACK's reverse communication, first iteration with ido = 0</span>
    arpack.dsaupd(ido, bmat, n, which, nev.`<span class="function"><span class="keyword">val</span>`, <span class="title">tolW</span>, <span class="title">resid</span>, <span class="title">ncv</span>, <span class="title">v</span>, <span class="title">n</span>, <span class="title">iparam</span>, <span class="title">ipntr</span>, <span class="title">workd</span>,
</span>      workl, workl.length, info)

    <span class="function"><span class="keyword">val</span> <span class="title">w</span> =</span> <span class="type">BDV</span>(workd)

    <span class="comment">// ido = 99 : done flag in reverse communication</span>
    <span class="keyword">while</span> (ido.`<span class="function"><span class="keyword">val</span>` <span class="title">!=</span> 99) {</span>
      <span class="keyword">if</span> (ido.`<span class="function"><span class="keyword">val</span>` <span class="title">!=</span> <span class="title">-1</span> <span class="title">&amp;&amp;</span> <span class="title">ido</span>.`<span class="title">val</span>` <span class="title">!=</span> 1) {</span>
        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"ARPACK returns ido = "</span> + ido.`<span class="function"><span class="keyword">val</span>` <span class="title">+</span>
</span>            <span class="string">" This flag is not compatible with Mode 1: A*x = lambda*x, A symmetric."</span>)
      }
      <span class="comment">// multiply working vector with the matrix</span>
      <span class="function"><span class="keyword">val</span> <span class="title">inputOffset</span> =</span> ipntr(<span class="number">0</span>) - <span class="number">1</span>
      <span class="function"><span class="keyword">val</span> <span class="title">outputOffset</span> =</span> ipntr(<span class="number">1</span>) - <span class="number">1</span>
      <span class="function"><span class="keyword">val</span> <span class="title">x</span> =</span> w.slice(inputOffset, inputOffset + n)
      <span class="function"><span class="keyword">val</span> <span class="title">y</span> =</span> w.slice(outputOffset, outputOffset + n)
      y := mul(x)
      <span class="comment">// call ARPACK's reverse communication</span>
      arpack.dsaupd(ido, bmat, n, which, nev.`<span class="function"><span class="keyword">val</span>`, <span class="title">tolW</span>, <span class="title">resid</span>, <span class="title">ncv</span>, <span class="title">v</span>, <span class="title">n</span>, <span class="title">iparam</span>, <span class="title">ipntr</span>,
</span>        workd, workl, workl.length, info)
    }

    <span class="keyword">if</span> (info.`<span class="function"><span class="keyword">val</span>` <span class="title">!=</span> 0) {</span>
      info.`<span class="function"><span class="keyword">val</span>` <span class="title">match</span> {</span>
        <span class="keyword">case</span> <span class="number">1</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"ARPACK returns non-zero info = "</span> + info.`<span class="function"><span class="keyword">val</span>` <span class="title">+</span>
</span>            <span class="string">" Maximum number of iterations taken. (Refer ARPACK user guide for details)"</span>)
        <span class="keyword">case</span> <span class="number">3</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"ARPACK returns non-zero info = "</span> + info.`<span class="function"><span class="keyword">val</span>` <span class="title">+</span>
</span>            <span class="string">" No shifts could be applied. Try to increase NCV. "</span> +
            <span class="string">"(Refer ARPACK user guide for details)"</span>)
        <span class="keyword">case</span> _ =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"ARPACK returns non-zero info = "</span> + info.`<span class="function"><span class="keyword">val</span>` <span class="title">+</span>
</span>            <span class="string">" Please refer ARPACK user guide for error message."</span>)
      }
    }

    <span class="function"><span class="keyword">val</span> <span class="title">d</span> =</span> <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Double</span>](nev.`<span class="function"><span class="keyword">val</span>`)
</span>    <span class="function"><span class="keyword">val</span> <span class="title">select</span> =</span> <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Boolean</span>](ncv)
    <span class="comment">// copy the Ritz vectors</span>
    <span class="function"><span class="keyword">val</span> <span class="title">z</span> =</span> java.util.<span class="type">Arrays</span>.copyOfRange(v, <span class="number">0</span>, nev.`<span class="function"><span class="keyword">val</span>` <span class="title">*</span> <span class="title">n</span>)
</span>
    <span class="comment">// call ARPACK's post-processing for eigenvectors</span>
    arpack.dseupd(<span class="literal">true</span>, <span class="string">"A"</span>, select, d, z, n, <span class="number">0.0</span>, bmat, n, which, nev, tol, resid, ncv, v, n,
      iparam, ipntr, workd, workl, workl.length, info)

    <span class="comment">// number of computed eigenvalues, might be smaller than k</span>
    <span class="function"><span class="keyword">val</span> <span class="title">computed</span> =</span> iparam(<span class="number">4</span>)

    <span class="function"><span class="keyword">val</span> <span class="title">eigenPairs</span> =</span> java.util.<span class="type">Arrays</span>.copyOfRange(d, <span class="number">0</span>, computed).zipWithIndex.map { r =&gt;
      (r._1, java.util.<span class="type">Arrays</span>.copyOfRange(z, r._2 * n, r._2 * n + n))
    }

    <span class="comment">// sort the eigen-pairs in descending order</span>
    <span class="function"><span class="keyword">val</span> <span class="title">sortedEigenPairs</span> =</span> eigenPairs.sortBy(- _._1)

    <span class="comment">// copy eigenvectors in descending order of eigenvalues</span>
    <span class="function"><span class="keyword">val</span> <span class="title">sortedU</span> =</span> <span class="type">BDM</span>.zeros[<span class="type">Double</span>](n, computed)
    sortedEigenPairs.zipWithIndex.foreach { r =&gt;
      <span class="function"><span class="keyword">val</span> <span class="title">b</span> =</span> r._2 * n
      <span class="keyword">var</span> i = <span class="number">0</span>
      <span class="keyword">while</span> (i &lt; n) {
        sortedU.data(b + i) = r._1._2(i)
        i += <span class="number">1</span>
      }
    }

    (<span class="type">BDV</span>[<span class="type">Double</span>](sortedEigenPairs.map(_._1)), sortedU)
  }
}
</code></pre><p>最后来到关键的地方，所以其实spark内部的分布式求矩阵都是用svd的近似，不断调用Gramian方法来传递向量，用迭代的方式来求特征值和特征向量。最后返回的也是一个BDV，也就是breeze.linalg.DenseVector。我个人认为这种方法相比mpi，似乎一路上也没有checkpoint，效率可能还不一定比mpi做的高，毕竟MPI，BLAS，SCALAPACK这些库都是久经考验得工业标准了。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/06/21/explore-spark-distributed-mllib/" data-id="cib610ix90019yjv4enejb0ci" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/mllib/">mllib</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/scala/">scala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-spark-task-and-scheduler" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/06/09/spark-task-and-scheduler/" class="article-date">
  <time datetime="2015-06-09T13:27:45.000Z" itemprop="datePublished">2015-06-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/06/09/spark-task-and-scheduler/">spark 的调度和任务</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>一晃快一个月过去了，这个月有点小忙做了很多项目一直都没空闲重新研究spark的代码。今天晚上下决心看完。花了一点时间追了一下代码。基本算是搞清楚了spark的调度。<br>上次说到，spark会把记录了操作的rdd提交交给调度器等待运行。那调度器具体是怎么执行计算的呢？随手提交了一个作业，有如下日志</p>
<pre><code><span class="number">15</span><span class="regexp">/06/</span><span class="number">10</span> <span class="number">03</span>:<span class="number">05</span>:<span class="number">09</span> INFO <span class="string">DAGScheduler:</span> Submitting <span class="number">1</span> missing tasks from Stage <span class="number">1</span> (MapPartitionsRDD[<span class="number">2</span>] at map at Task2a.<span class="string">scala:</span><span class="number">8</span>)
<span class="number">15</span><span class="regexp">/06/</span><span class="number">10</span> <span class="number">03</span>:<span class="number">05</span>:<span class="number">09</span> INFO <span class="string">TaskSchedulerImpl:</span> Adding task set <span class="number">1.0</span> with <span class="number">1</span> tasks
<span class="number">15</span><span class="regexp">/06/</span><span class="number">10</span> <span class="number">03</span>:<span class="number">05</span>:<span class="number">09</span> INFO <span class="string">TaskSetManager:</span> Starting task <span class="number">0.0</span> <span class="keyword">in</span> stage <span class="number">1.0</span> (TID <span class="number">1</span>, localhost, PROCESS_LOCAL, <span class="number">1348</span> bytes)
<span class="number">15</span><span class="regexp">/06/</span><span class="number">10</span> <span class="number">03</span>:<span class="number">05</span>:<span class="number">09</span> INFO <span class="string">Executor:</span> Running task <span class="number">0.0</span> <span class="keyword">in</span> stage <span class="number">1.0</span> (TID <span class="number">1</span>)
<span class="number">15</span><span class="regexp">/06/</span><span class="number">10</span> <span class="number">03</span>:<span class="number">05</span>:<span class="number">09</span> INFO <span class="string">HadoopRDD:</span> Input <span class="string">split:</span> <span class="string">file:</span><span class="regexp">/home/</span>cloudera/<span class="number">1</span>millionTweets.<span class="string">tsv:</span><span class="number">0</span>+<span class="number">28775886</span>
<span class="number">15</span><span class="regexp">/06/</span><span class="number">10</span> <span class="number">03</span>:<span class="number">05</span>:<span class="number">10</span> INFO <span class="string">BlockManager:</span> Removing broadcast <span class="number">1</span>
<span class="number">15</span><span class="regexp">/06/</span><span class="number">10</span> <span class="number">03</span>:<span class="number">05</span>:<span class="number">10</span> INFO <span class="string">BlockManager:</span> Removing block broadcast_1_piece0
<span class="number">15</span><span class="regexp">/06/</span><span class="number">10</span> <span class="number">03</span>:<span class="number">05</span>:<span class="number">10</span> INFO <span class="string">MemoryStore:</span> Block broadcast_1_piece0 of size <span class="number">2039</span> dropped from memory (free <span class="number">280032800</span>)
</code></pre><p>我们可以看到，DAGScheduler将任务提交了以后其实是TaskSetManager执行任务的</p>
<pre><code>logInfo(<span class="string">"Starting <span class="variable">%s</span> (TID <span class="variable">%d</span>, <span class="variable">%s</span>, <span class="variable">%s</span>, <span class="variable">%d</span> bytes)"</span>.<span class="keyword">format</span>(
taskName, taskId, host, taskLocality, serializedTask.limit))
sched.dagScheduler.taskStarted(task, info)
<span class="keyword">return</span> Some(new TaskDescription(taskId = taskId, attemptNumber = attemptNum, execId,
taskName, <span class="keyword">index</span>, serializedTask))
</code></pre><p>这里调用了dagScheduler的taskstarted开始任务的而最后到了Executor执行，也就是这一段</p>
<pre><code><span class="comment">// Run the actual task and measure its runtime.</span>
taskStart = System.<span class="function"><span class="title">currentTimeMillis</span><span class="params">()</span></span>
val value = task.<span class="function"><span class="title">run</span><span class="params">(taskAttemptId = taskId, attemptNumber = attemptNumber)</span></span>
val taskFinish = System.<span class="function"><span class="title">currentTimeMillis</span><span class="params">()</span></span>
</code></pre><p>这里调用的是task.run来执行</p>
<pre><code>final <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(taskAttemptId: Long, attemptNumber: Int)</span>:</span> T = {
  context = new TaskContextImpl(stageId = stageId, partitionId = partitionId,
    taskAttemptId = taskAttemptId, attemptNumber = attemptNumber, runningLocally = false)
  TaskContextHelper.setTaskContext(context)
  context.taskMetrics.setHostname(Utils.localHostName())
  taskThread = Thread.currentThread()
  <span class="keyword">if</span> (_killed) {
    kill(interruptThread = false)
  }
  <span class="keyword">try</span> {
    runTask(context)
  } <span class="keyword">finally</span> {
    context.markTaskCompleted()
    TaskContextHelper.unset()
  }
 }
</code></pre><p>这是一个抽象类的final方法，基本可以看出大致的逻辑，调用自己的runTask执行任务完事。<br>例如，对于shuffle任务有一个</p>
<pre><code><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">ShuffleMapTask</span>(</span>
  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">runTask</span>(</span>context: <span class="type">TaskContext</span>): <span class="type">MapStatus</span> = {
    <span class="comment">// Deserialize the RDD using the broadcast variable.</span>
    <span class="function"><span class="keyword">val</span> <span class="title">ser</span> =</span> <span class="type">SparkEnv</span>.get.closureSerializer.newInstance()
    <span class="function"><span class="keyword">val</span> (</span>rdd, dep) = ser.deserialize[(<span class="type">RDD</span>[_], <span class="type">ShuffleDependency</span>[_, _, _])](
      <span class="type">ByteBuffer</span>.wrap(taskBinary.value), <span class="type">Thread</span>.currentThread.getContextClassLoader)

    metrics = <span class="type">Some</span>(context.taskMetrics)
    <span class="keyword">var</span> writer: <span class="type">ShuffleWriter</span>[<span class="type">Any</span>, <span class="type">Any</span>] = <span class="literal">null</span>
    <span class="keyword">try</span> {
      <span class="function"><span class="keyword">val</span> <span class="title">manager</span> =</span> <span class="type">SparkEnv</span>.get.shuffleManager
      writer = manager.getWriter[<span class="type">Any</span>, <span class="type">Any</span>](dep.shuffleHandle, partitionId, context)
      writer.write(rdd.iterator(partition, context).asInstanceOf[<span class="type">Iterator</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">Any</span>, <span class="type">Any</span>]]])
      <span class="keyword">return</span> writer.stop(success = <span class="literal">true</span>).get
    } <span class="keyword">catch</span> {
      <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;
        <span class="keyword">try</span> {
          <span class="keyword">if</span> (writer != <span class="literal">null</span>) {
            writer.stop(success = <span class="literal">false</span>)
          }
        } <span class="keyword">catch</span> {
          <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;
            log.debug(<span class="string">"Could not stop writer"</span>, e)
        }
        <span class="keyword">throw</span> e
    }
  }
</code></pre><p>每一个Task子类都有自己的衍生的Runtask方法，取决于各自的目的。<br>终于算是找到了核心运算单元了，继续看mllib和dataframe的代码</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/06/09/spark-task-and-scheduler/" data-id="cib610is30009yjv4aekjq81n" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/scala/">scala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/scheduler/">scheduler</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/task/">task</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-explore-spark-RDD-source-code" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/15/explore-spark-RDD-source-code/" class="article-date">
  <time datetime="2015-05-15T08:14:17.000Z" itemprop="datePublished">2015-05-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/15/explore-spark-RDD-source-code/">expore spark RDD source code</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>一鼓作气再而衰三而竭。<br>开始阅读spark部分的代码，好在之前在coursera上看过一点scala虽然习题没做完但是不至于全忘。在spark里面，RDD的lazy evaluation也是通过和pyspark差不多的方法实现的，定义一个RDD类，每次操作都返回一个MapParitionsRDD，在这个MapParitionsRDD里记下前一个RDD和对应的函数。</p>
<pre><code>abstract <span class="class"><span class="keyword">class</span> <span class="title">RDD</span>[<span class="title">T</span>:</span> ClassTag](
    <span class="decorator">@transient private var _sc: SparkContext,</span>
    <span class="decorator">@transient private var deps: Seq[Dependency[_]]</span>
  ) extends Serializable <span class="keyword">with</span> Logging {

  <span class="keyword">if</span> (classOf[RDD[_]].isAssignableFrom(elementClassTag.runtimeClass)) {
    // This <span class="keyword">is</span> a warning instead of an exception <span class="keyword">in</span> order to avoid breaking user programs that
    // might have defined nested RDDs without running jobs <span class="keyword">with</span> them.
    logWarning(<span class="string">"Spark does not support nested RDDs (see SPARK-5063)"</span>)
  }

  private <span class="function"><span class="keyword">def</span> <span class="title">sc</span>:</span> SparkContext = {
    <span class="keyword">if</span> (_sc == null) {
      throw new SparkException(
        <span class="string">"RDD transformations and actions can only be invoked by the driver, not inside of other "</span> +
        <span class="string">"transformations; for example, rdd1.map(x =&gt; rdd2.values.count() * x) is invalid because "</span> +
        <span class="string">"the values transformation and count action cannot be performed inside of the rdd1.map "</span> +
        <span class="string">"transformation. For more information, see SPARK-5063."</span>)
    }
    _sc
  }


  <span class="function"><span class="keyword">def</span> <span class="title">map</span>[<span class="title">U</span>:</span> ClassTag](f: T =&gt; U): RDD[U] = {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.map(cleanF))
  }

  <span class="function"><span class="keyword">def</span> <span class="title">filter</span><span class="params">(f: T =&gt; Boolean)</span>:</span> RDD[T] = {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[T, T](
      this,
      (context, pid, iter) =&gt; iter.filter(cleanF),
      preservesPartitioning = true)
  }

  <span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsWithIndex</span>[<span class="title">U</span>:</span> ClassTag](
      f: (Int, Iterator[T]) =&gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = {
    val func = (context: TaskContext, index: Int, iter: Iterator[T]) =&gt; f(index, iter)
    new MapPartitionsRDD(this, sc.clean(func), preservesPartitioning)
  }
</code></pre><p>而MapPartitionsRDD仅仅记录下了动作和父RDD</p>
<pre><code>private[spark] class <span class="type">MapPartitionsRDD</span>[U: <span class="type">ClassTag</span>, T: <span class="type">ClassTag</span>](
    prev: <span class="type">RDD</span>[T],
    f: (<span class="type">TaskContext</span>, <span class="type">Int</span>, <span class="type">Iterator</span>[T]) =&gt; <span class="type">Iterator</span>[U],  // (<span class="type">TaskContext</span>, partition index, <span class="keyword">iterator</span>)
    preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>)
  extends <span class="type">RDD</span>[U](prev) {

  override val partitioner = <span class="keyword">if</span> (preservesPartitioning) firstParent[T].partitioner <span class="keyword">else</span> <span class="type">None</span>

  override def getPartitions: <span class="type">Array</span>[<span class="type">Partition</span>] = firstParent[T].partitions

  override def compute(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>) =
    f(context, split.index, firstParent[T].<span class="keyword">iterator</span>(split, context))
}
</code></pre><p>而只有collect和reduce这类函数是例外的，</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">collect</span><span class="params">()</span>:</span> Array[T] = {
    val results = sc.runJob(this, (iter: Iterator[T]) =&gt; iter.toArray)
    Array.concat(results: _*)
  }

<span class="function"><span class="keyword">def</span> <span class="title">reduce</span><span class="params">(f: <span class="params">(T, T)</span> =&gt; T)</span>:</span> T = {
    val cleanF = sc.clean(f)
    val reducePartition: Iterator[T] =&gt; Option[T] = iter =&gt; {
      <span class="keyword">if</span> (iter.hasNext) {
        Some(iter.reduceLeft(cleanF))
      } <span class="keyword">else</span> {
        <span class="keyword">None</span>
      }
    }
    var jobResult: Option[T] = <span class="keyword">None</span>
    val mergeResult = (index: Int, taskResult: Option[T]) =&gt; {
      <span class="keyword">if</span> (taskResult.isDefined) {
        jobResult = jobResult match {
          case Some(value) =&gt; Some(f(value, taskResult.get))
          case <span class="keyword">None</span> =&gt; taskResult
        }
      }
    }
    sc.runJob(this, reducePartition, mergeResult)
    // Get the final result out of our Option, <span class="keyword">or</span> throw an exception <span class="keyword">if</span> the RDD was empty
    jobResult.getOrElse(throw new UnsupportedOperationException(<span class="string">"empty collection"</span>))
  }
</code></pre><p>看代码可以知道collect这个动作做的时候其实是调用sc.runJob了，不再累积RDD类。而从SparkContext里的runJob里可以看到，scala shell其实是通过调用DAGScheduler.runjob让spark干活的</p>
<pre><code>def runJob[T, U: ClassTag](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) =&gt; U,
      partitions: Seq[<span class="typename">Int</span>],
      allowLocal: <span class="typename">Boolean</span>,
      resultHandler: (<span class="typename">Int</span>, U) =&gt; <span class="typename">Unit</span>) {
    <span class="keyword">if</span> (stopped) {
      <span class="keyword">throw</span> new IllegalStateException(<span class="string">"SparkContext has been shutdown"</span>)
    }
    <span class="variable"><span class="keyword">val</span> callSite</span> = getCallSite
    <span class="variable"><span class="keyword">val</span> cleanedFunc</span> = clean(func)
    logInfo(<span class="string">"Starting job: "</span> + callSite.shortForm)
    <span class="keyword">if</span> (conf.getBoolean(<span class="string">"spark.logLineage"</span>, <span class="literal">false</span>)) {
      logInfo(<span class="string">"RDD's recursive dependencies:\n"</span> + rdd.toDebugString)
    }
    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, allowLocal,
      resultHandler, localProperties.<span class="keyword">get</span>)
    progressBar.foreach(_.finishAll())
    rdd.doCheckpoint()
  }
</code></pre><p>对于dag调度器，他的runJob是用submitJob来提交的</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">runJob</span>[</span><span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](
      rdd: <span class="type">RDD</span>[<span class="type">T</span>],
      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,
      partitions: <span class="type">Seq</span>[<span class="type">Int</span>],
      callSite: <span class="type">CallSite</span>,
      allowLocal: <span class="type">Boolean</span>,
      resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>,
      properties: <span class="type">Properties</span>): <span class="type">Unit</span> = {
    <span class="function"><span class="keyword">val</span> <span class="title">start</span> =</span> <span class="type">System</span>.nanoTime
    <span class="function"><span class="keyword">val</span> <span class="title">waiter</span> =</span> submitJob(rdd, func, partitions, callSite, allowLocal, resultHandler, properties)
    waiter.awaitResult() <span class="keyword">match</span> {
      <span class="keyword">case</span> <span class="type">JobSucceeded</span> =&gt; {
        logInfo(<span class="string">"Job %d finished: %s, took %f s"</span>.format
          (waiter.jobId, callSite.shortForm, (<span class="type">System</span>.nanoTime - start) / <span class="number">1e9</span>))
      }
      <span class="keyword">case</span> <span class="type">JobFailed</span>(exception: <span class="type">Exception</span>) =&gt;
        logInfo(<span class="string">"Job %d failed: %s, took %f s"</span>.format
          (waiter.jobId, callSite.shortForm, (<span class="type">System</span>.nanoTime - start) / <span class="number">1e9</span>))
        <span class="keyword">throw</span> exception
    }
  }

  <span class="function"><span class="keyword">def</span> <span class="title">submitJob</span>[</span><span class="type">T</span>, <span class="type">U</span>](
      rdd: <span class="type">RDD</span>[<span class="type">T</span>],
      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,
      partitions: <span class="type">Seq</span>[<span class="type">Int</span>],
      callSite: <span class="type">CallSite</span>,
      allowLocal: <span class="type">Boolean</span>,
      resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>,
      properties: <span class="type">Properties</span>): <span class="type">JobWaiter</span>[<span class="type">U</span>] = {
    <span class="comment">// Check to make sure we are not launching a task on a partition that does not exist.</span>
    <span class="function"><span class="keyword">val</span> <span class="title">maxPartitions</span> =</span> rdd.partitions.length
    partitions.find(p =&gt; p &gt;= maxPartitions || p &lt; <span class="number">0</span>).foreach { p =&gt;
      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(
        <span class="string">"Attempting to access a non-existent partition: "</span> + p + <span class="string">". "</span> +
          <span class="string">"Total number of partitions: "</span> + maxPartitions)
    }

    <span class="function"><span class="keyword">val</span> <span class="title">jobId</span> =</span> nextJobId.getAndIncrement()
    <span class="keyword">if</span> (partitions.size == <span class="number">0</span>) {
      <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">JobWaiter</span>[<span class="type">U</span>](<span class="keyword">this</span>, jobId, <span class="number">0</span>, resultHandler)
    }

    assert(partitions.size &gt; <span class="number">0</span>)
    <span class="function"><span class="keyword">val</span> <span class="title">func2</span> =</span> func.asInstanceOf[(<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _]
    <span class="function"><span class="keyword">val</span> <span class="title">waiter</span> =</span> <span class="keyword">new</span> <span class="type">JobWaiter</span>(<span class="keyword">this</span>, jobId, partitions.size, resultHandler)
    eventProcessLoop.post(<span class="type">JobSubmitted</span>(
      jobId, rdd, func2, partitions.toArray, allowLocal, callSite, waiter, properties))
    waiter
  }
</code></pre><p>做了一系列安全性检查，起了一个jobwaiter等结果，然后继续调用JobSubmitted比把它提交到DAGSchedulerEventProcessLoop类里去，</p>
<pre><code><span class="keyword">private</span>[scheduler] <span class="class"><span class="keyword">class</span> <span class="title">DAGSchedulerEventProcessLoop</span>(</span>dagScheduler: <span class="type">DAGScheduler</span>)
  <span class="keyword">extends</span> <span class="type">EventLoop</span>[<span class="type">DAGSchedulerEvent</span>](<span class="string">"dag-scheduler-event-loop"</span>) <span class="keyword">with</span> <span class="type">Logging</span> {

  <span class="comment">/**
   * The main event loop of the DAG scheduler.
   */</span>
  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onReceive</span>(</span>event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = event <span class="keyword">match</span> {
    <span class="keyword">case</span> <span class="type">JobSubmitted</span>(jobId, rdd, func, partitions, allowLocal, callSite, listener, properties) =&gt;
      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, allowLocal, callSite,
        listener, properties)

...

  <span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">handleJobSubmitted</span>(</span>jobId: <span class="type">Int</span>,
      finalRDD: <span class="type">RDD</span>[_],
      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,
      partitions: <span class="type">Array</span>[<span class="type">Int</span>],
      allowLocal: <span class="type">Boolean</span>,
      callSite: <span class="type">CallSite</span>,
      listener: <span class="type">JobListener</span>,
      properties: <span class="type">Properties</span>) {
    <span class="keyword">var</span> finalStage: <span class="type">Stage</span> = <span class="literal">null</span>
    <span class="keyword">try</span> {
      <span class="comment">// New stage creation may throw an exception if, for example, jobs are run on a</span>
      <span class="comment">// HadoopRDD whose underlying HDFS files have been deleted.</span>
      finalStage = newStage(finalRDD, partitions.size, <span class="type">None</span>, jobId, callSite)
    } <span class="keyword">catch</span> {
      <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;
        logWarning(<span class="string">"Creating new stage failed due to exception - job: "</span> + jobId, e)
        listener.jobFailed(e)
        <span class="keyword">return</span>
    }
    <span class="keyword">if</span> (finalStage != <span class="literal">null</span>) {
      <span class="function"><span class="keyword">val</span> <span class="title">job</span> =</span> <span class="keyword">new</span> <span class="type">ActiveJob</span>(jobId, finalStage, func, partitions, callSite, listener, properties)
      clearCacheLocs()
      logInfo(<span class="string">"Got job %s (%s) with %d output partitions (allowLocal=%s)"</span>.format(
        job.jobId, callSite.shortForm, partitions.length, allowLocal))
      logInfo(<span class="string">"Final stage: "</span> + finalStage + <span class="string">"("</span> + finalStage.name + <span class="string">")"</span>)
      logInfo(<span class="string">"Parents of final stage: "</span> + finalStage.parents)
      logInfo(<span class="string">"Missing parents: "</span> + getMissingParentStages(finalStage))
      <span class="function"><span class="keyword">val</span> <span class="title">shouldRunLocally</span> =</span>
        localExecutionEnabled &amp;&amp; allowLocal &amp;&amp; finalStage.parents.isEmpty &amp;&amp; partitions.length == <span class="number">1</span>
      <span class="function"><span class="keyword">val</span> <span class="title">jobSubmissionTime</span> =</span> clock.getTimeMillis()
      <span class="keyword">if</span> (shouldRunLocally) {
        <span class="comment">// Compute very short actions like first() or take() with no parent stages locally.</span>
        listenerBus.post(
          <span class="type">SparkListenerJobStart</span>(job.jobId, jobSubmissionTime, <span class="type">Seq</span>.empty, properties))
        runLocally(job)
      } <span class="keyword">else</span> {
        jobIdToActiveJob(jobId) = job
        activeJobs += job
        finalStage.resultOfJob = <span class="type">Some</span>(job)
        <span class="function"><span class="keyword">val</span> <span class="title">stageIds</span> =</span> jobIdToStageIds(jobId).toArray
        <span class="function"><span class="keyword">val</span> <span class="title">stageInfos</span> =</span> stageIds.flatMap(id =&gt; stageIdToStage.get(id).map(_.latestInfo))
        listenerBus.post(
          <span class="type">SparkListenerJobStart</span>(job.jobId, jobSubmissionTime, stageInfos, properties))
        submitStage(finalStage)
      }
    }
    submitWaitingStages()
  }
</code></pre><p>包装了一大堆东西，然后扔到listenerBus里准备运行（这次是真的要运行了吧？）</p>
<pre><code><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">LiveListenerBus</span>
</span>  <span class="keyword">extends</span> <span class="type">AsynchronousListenerBus</span>[<span class="type">SparkListener</span>, <span class="type">SparkListenerEvent</span>](<span class="string">"SparkListenerBus"</span>)
  <span class="keyword">with</span> <span class="type">SparkListenerBus</span> {

  <span class="keyword">private</span> <span class="function"><span class="keyword">val</span> <span class="title">logDroppedEvent</span> =</span> <span class="keyword">new</span> <span class="type">AtomicBoolean</span>(<span class="literal">false</span>)

  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onDropEvent</span>(</span>event: <span class="type">SparkListenerEvent</span>): <span class="type">Unit</span> = {
    <span class="keyword">if</span> (logDroppedEvent.compareAndSet(<span class="literal">false</span>, <span class="literal">true</span>)) {
      <span class="comment">// Only log the following message once to avoid duplicated annoying logs.</span>
      logError(<span class="string">"Dropping SparkListenerEvent because no remaining room in event queue. "</span> +
        <span class="string">"This likely means one of the SparkListeners is too slow and cannot keep up with "</span> +
        <span class="string">"the rate at which tasks are being started by the scheduler."</span>)
    }
  }

}

<span class="keyword">private</span>[spark] <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">AsynchronousListenerBus</span>[</span><span class="type">L</span> &lt;: <span class="type">AnyRef</span>, <span class="type">E</span>](name: <span class="type">String</span>)
  <span class="keyword">extends</span> <span class="type">ListenerBus</span>[<span class="type">L</span>, <span class="type">E</span>] {
  <span class="function"><span class="keyword">def</span> <span class="title">post</span>(</span>event: <span class="type">E</span>) {
    <span class="keyword">if</span> (stopped.get) {
      <span class="comment">// Drop further events to make `listenerThread` exit ASAP</span>
      logError(s<span class="string">"$name has already stopped! Dropping event $event"</span>)
      <span class="keyword">return</span>
    }
    <span class="function"><span class="keyword">val</span> <span class="title">eventAdded</span> =</span> eventQueue.offer(event)
    <span class="keyword">if</span> (eventAdded) {
      eventLock.release()
    } <span class="keyword">else</span> {
      onDropEvent(event)
    }
  }
</code></pre><p>扔完以后开始submitStage把最后状态提交开始运行<br>所以跳到最后就是把作业提交完事，胸闷，还是没有看到RDD是怎么在不同节点间交互的代码。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/05/15/explore-spark-RDD-source-code/" data-id="cib610ixj001dyjv4f8zjgbwy" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RDD/">RDD</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/scala/">scala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-spark-machine-learning-lib-for-python" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/15/spark-machine-learning-lib-for-python/" class="article-date">
  <time datetime="2015-05-15T05:29:26.000Z" itemprop="datePublished">2015-05-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/15/spark-machine-learning-lib-for-python/">spark machine learning lib for python</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>今天看了一下pyspark的mllib这个库。作为一个大数据计算平台，spark这点比hadoop向前走了一大步。提供了种类繁多的数据计算的模型，从随机数生成到线性代数到回归聚类。<br>粗粗的看了一下，pyspark在做这个mllib的时候实现有两种，一种是spark原生型的api调用，比如刚才提到的回归聚类，都是直接输入rdd然后不断迭代返回结果，比如</p>
<pre><code><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegressionWithSGD</span><span class="params">(object)</span>:</span>

    <span class="decorator">@classmethod</span>
    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(cls, data, iterations=<span class="number">100</span>, step=<span class="number">1.0</span>, miniBatchFraction=<span class="number">1.0</span>,
              initialWeights=None, regParam=<span class="number">0.01</span>, regType=<span class="string">"l2"</span>, intercept=False)</span>:</span>
        <span class="string">"""
        Train a logistic regression model on the given data.

        :param data:              The training data, an RDD of LabeledPoint.
        :param iterations:        The number of iterations (default: 100).
        :param step:              The step parameter used in SGD
                                  (default: 1.0).
        :param miniBatchFraction: Fraction of data to be used for each SGD
                                  iteration.
        :param initialWeights:    The initial weights (default: None).
        :param regParam:          The regularizer parameter (default: 0.01).
        :param regType:           The type of regularizer used for training
                                  our model.

                                  :Allowed values:
                                     - "l1" for using L1 regularization
                                     - "l2" for using L2 regularization
                                     - None for no regularization

                                     (default: "l2")

        :param intercept:         Boolean parameter which indicates the use
                                  or not of the augmented representation for
                                  training data (i.e. whether bias features
                                  are activated or not).
        """</span>
        <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(rdd, i)</span>:</span>
            <span class="keyword">return</span> callMLlibFunc(<span class="string">"trainLogisticRegressionModelWithSGD"</span>, rdd, int(iterations),
                                 float(step), float(miniBatchFraction), i, float(regParam), regType,
                                 bool(intercept))

        <span class="keyword">return</span> _regression_train_wrapper(train, LogisticRegressionModel, data, initialWeights)


<span class="function"><span class="keyword">def</span> <span class="title">callMLlibFunc</span><span class="params">(name, *args)</span>:</span>
    <span class="string">""" Call API in PythonMLLibAPI """</span>
    sc = SparkContext._active_spark_context
    api = getattr(sc._jvm.PythonMLLibAPI(), name)
    <span class="keyword">return</span> callJavaFunc(sc, api, *args)
</code></pre><p>第一类api全都把工作丢给了spark去做，也就是其实是跨节点并行计算。<br>第二类api就比较奇怪了，linalg里面全是这类函数，如</p>
<pre><code><span class="class"><span class="keyword">class</span> <span class="title">DenseVector</span><span class="params">(Vector)</span>:</span>
    <span class="string">"""
    A dense vector represented by a value array. We use numpy array for
    storage and arithmetics will be delegated to the underlying numpy
    array.

    def dot(self, other):
        """</span>
        Compute the dot product of two Vectors. We support
        (Numpy array, list, SparseVector, <span class="keyword">or</span> SciPy sparse)
        <span class="keyword">and</span> a target NumPy array that <span class="keyword">is</span> either <span class="number">1</span>- <span class="keyword">or</span> <span class="number">2</span>-dimensional.
        Equivalent to calling numpy.dot of the two vectors.
        <span class="keyword">if</span> type(other) == np.ndarray:
            <span class="keyword">if</span> other.ndim &gt; <span class="number">1</span>:
                <span class="keyword">assert</span> len(self) == other.shape[<span class="number">0</span>], <span class="string">"dimension mismatch"</span>
            <span class="keyword">return</span> np.dot(self.array, other)
        <span class="keyword">elif</span> _have_scipy <span class="keyword">and</span> scipy.sparse.issparse(other):
            <span class="keyword">assert</span> len(self) == other.shape[<span class="number">0</span>], <span class="string">"dimension mismatch"</span>
            <span class="keyword">return</span> other.transpose().dot(self.toArray())
        <span class="keyword">else</span>:
            <span class="keyword">assert</span> len(self) == _vector_size(other), <span class="string">"dimension mismatch"</span>
            <span class="keyword">if</span> isinstance(other, SparseVector):
                <span class="keyword">return</span> other.dot(self)
            <span class="keyword">elif</span> isinstance(other, Vector):
                <span class="keyword">return</span> np.dot(self.toArray(), other.toArray())
            <span class="keyword">else</span>:
                <span class="keyword">return</span> np.dot(self.toArray(), other)
</code></pre><p>居然全部都是丢给numpy去做，虽然numpy做这类计算的效率很高不假但这样就没有spark的跨节点并行的优势了，在计算的时候要注意。另外听说spark的运行效率首先于带宽，根据我有限的hpc经验来看，网络的延迟影响可能也会很大。而spark所在的网络最多也就是万兆网络，在做矩阵并行计算这类对网络延迟依赖很大的计算的时候可能会很慢，开发人员会不会因此考虑将不少矩阵计算本地化呢？不得而知，我很好奇下一个版本如果他们给出矩阵求逆以及对焦化或者svd是不是依然也使用本地计算。如果这个问题不解决可能对spark要称霸大数据平台有点影响。</p>
<p>好了，pyspark的代码看的差不多了，接下去就是用scala写的spark了，cross fingers</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/05/15/spark-machine-learning-lib-for-python/" data-id="cib610iue000qyjv4zlrx98mz" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/mllib/">mllib</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-explore-the-source-of-pyspark-dataframe" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/14/explore-the-source-of-pyspark-dataframe/" class="article-date">
  <time datetime="2015-05-14T07:24:15.000Z" itemprop="datePublished">2015-05-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/14/explore-the-source-of-pyspark-dataframe/">pyspark dataframe的实现</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>dataframe是spark 1.3今年新推出的东西，但其实早期叫做SchemaRDD，这还可以在源代码里看到。不过按照SPARK east summit 2015上的说法，dataframe速度很快，这得益于全部用内核实现计算。<br>随便摘一段出来就能发现，Dataframe这个类里的大多数计算都是交由self._jdf实现的。</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">count</span><span class="params">(self)</span>:</span>
    <span class="string">"""Returns the number of rows in this :class:`DataFrame`.

    &gt;&gt;&gt; df.count()
    2L
    """</span>
    <span class="keyword">return</span> self._jdf.count()

<span class="function"><span class="keyword">def</span> <span class="title">collect</span><span class="params">(self)</span>:</span>
    <span class="string">"""Returns all the records as a list of :class:`Row`.

    &gt;&gt;&gt; df.collect()
    [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]
    """</span>
    <span class="keyword">with</span> SCCallSiteSync(self._sc) <span class="keyword">as</span> css:
        port = self._sc._jvm.PythonRDD.collectAndServe(self._jdf.javaToPython().rdd())
    rs = list(_load_from_socket(port, BatchedSerializer(PickleSerializer())))
    cls = _create_cls(self.schema)
    <span class="keyword">return</span> [cls(r) <span class="keyword">for</span> r <span class="keyword">in</span> rs]

<span class="function"><span class="keyword">def</span> <span class="title">limit</span><span class="params">(self, num)</span>:</span>
    <span class="string">"""Limits the result count to the number specified.

    &gt;&gt;&gt; df.limit(1).collect()
    [Row(age=2, name=u'Alice')]
    &gt;&gt;&gt; df.limit(0).collect()
    []
    """</span>
    jdf = self._jdf.limit(num)
    <span class="keyword">return</span> DataFrame(jdf, self.sql_ctx)
</code></pre><p>而self._jdf来自类初始化传入的值，通常如果用SQLContext创建DataFrame的话都来自这段代码</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">createDataFrame</span><span class="params">(self, data, schema=None, samplingRatio=None)</span>:</span>
   ...
   <span class="keyword">if</span> isinstance(schema, (list, tuple)):
       first = rdd.first()
       <span class="keyword">if</span> <span class="keyword">not</span> isinstance(first, (list, tuple)):
           <span class="keyword">raise</span> ValueError(<span class="string">"each row in `rdd` should be list or tuple, "</span>
                            <span class="string">"but got %r"</span> % type(first))
       row_cls = Row(*schema)
       schema = self._inferSchema(rdd.map(<span class="keyword">lambda</span> r: row_cls(*r)), samplingRatio)

   <span class="comment"># take the first few rows to verify schema</span>
   rows = rdd.take(<span class="number">10</span>)
   <span class="comment"># Row() cannot been deserialized by Pyrolite</span>
   <span class="keyword">if</span> rows <span class="keyword">and</span> isinstance(rows[<span class="number">0</span>], tuple) <span class="keyword">and</span> rows[<span class="number">0</span>].__class__.__name__ == <span class="string">'Row'</span>:
       rdd = rdd.map(tuple)
       rows = rdd.take(<span class="number">10</span>)

   <span class="keyword">for</span> row <span class="keyword">in</span> rows:
       _verify_type(row, schema)

   <span class="comment"># convert python objects to sql data</span>
   converter = _python_to_sql_converter(schema)
   rdd = rdd.map(converter)

   jrdd = self._jvm.SerDeUtil.toJavaArray(rdd._to_java_object_rdd())
   df = self._ssql_ctx.applySchemaToPythonRDD(jrdd.rdd(), schema.json())
   <span class="keyword">return</span> DataFrame(df, self)
</code></pre><p>而这里的self._ssql_ctx来自于同一个类里的</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">_ssql_ctx</span><span class="params">(self)</span>:</span>
    <span class="string">"""Accessor for the JVM Spark SQL context.

    Subclasses can override this property to provide their own
    JVM Contexts.
    """</span>
    <span class="keyword">if</span> self._scala_SQLContext <span class="keyword">is</span> <span class="keyword">None</span>:
        self._scala_SQLContext = self._jvm.SQLContext(self._jsc.sc())
    <span class="keyword">return</span> self._scala_SQLContext
</code></pre><p>是对scala_SQLContext的一个封装。包括sql语句的执行也是直接调用_ssql_ctx</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">sql</span><span class="params">(self, sqlQuery)</span>:</span>
    <span class="string">"""Returns a :class:`DataFrame` representing the result of the given query.

    &gt;&gt;&gt; sqlContext.registerDataFrameAsTable(df, "table1")
    &gt;&gt;&gt; df2 = sqlContext.sql("SELECT field1 AS f1, field2 as f2 from table1")
    &gt;&gt;&gt; df2.collect()
    [Row(f1=1, f2=u'row1'), Row(f1=2, f2=u'row2'), Row(f1=3, f2=u'row3')]
    """</span>
    <span class="keyword">return</span> DataFrame(self._ssql_ctx.sql(sqlQuery), self)
</code></pre><p>而原先的SchemaRDD则不用了，不过看1.2.2的代码似乎也是调用_ssql_ctx来实现计算的，不能明白为什么1.3和1.2能性能差那么多。这还有待于阅读spark的代码来解答。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/05/14/explore-the-source-of-pyspark-dataframe/" data-id="cib610iwu0014yjv4esxfujwa" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/dataframe/">dataframe</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-spark-rdd-pipeline-lazyevaluation" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/14/spark-rdd-pipeline-lazyevaluation/" class="article-date">
  <time datetime="2015-05-14T02:28:14.000Z" itemprop="datePublished">2015-05-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/14/spark-rdd-pipeline-lazyevaluation/">spark rdd, pipeline, lazyevaluation</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>一直以来写代码不求甚解，感觉这样不好，从今天开始起读各数据框架的源代码，学习学习再学习  </p>
<p>今天看的是pyspark里lazy evaluation的处理，python和scala不同不是函数式的。那这是怎么办到的呢？  </p>
<p>首先所有的数据集在spark内部都叫做rdd，这在pyspark里也有定义：  </p>
<pre><code><span class="keyword">class</span> RDD(object):

    <span class="string">""</span>"
    A Resilient Distributed Dataset (RDD), the basic abstraction <span class="keyword">in</span> Spark.
    Represents <span class="keyword">an</span> immutable, partitioned collection of elements that can be
    operated <span class="keyword">on</span> <span class="keyword">in</span> parallel.
</code></pre><p>RDD内部实现了很多函数，有map，filter这类一个集合对一个集合的映射，也有collect，reduce这种一个集合到一个值的映射。</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">map</span><span class="params">(self, f, preservesPartitioning=False)</span>:</span>
    <span class="string">"""
    Return a new RDD by applying a function to each element of this RDD.

    &gt;&gt;&gt; rdd = sc.parallelize(["b", "a", "c"])
    &gt;&gt;&gt; sorted(rdd.map(lambda x: (x, 1)).collect())
    [('a', 1), ('b', 1), ('c', 1)]
    """</span>
    <span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(_, iterator)</span>:</span>
        <span class="keyword">return</span> imap(f, iterator)
    <span class="keyword">return</span> self.mapPartitionsWithIndex(func, preservesPartitioning)

<span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsWithIndex</span><span class="params">(self, f, preservesPartitioning=False)</span>:</span>
    <span class="string">"""
    Return a new RDD by applying a function to each partition of this RDD,
    while tracking the index of the original partition.

    &gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4], 4)
    &gt;&gt;&gt; def f(splitIndex, iterator): yield splitIndex
    &gt;&gt;&gt; rdd.mapPartitionsWithIndex(f).sum()
    6
    """</span>
    <span class="keyword">return</span> PipelinedRDD(self, f, preservesPartitioning)
</code></pre><p>对于map filter这类函数来说，他们每次操作都是产生一个叫做PipelinedRDD的对象，那这个PipelinedRDD又是干什么的呢？</p>
<pre><code><span class="class"><span class="keyword">class</span> <span class="title">PipelinedRDD</span><span class="params">(RDD)</span>:</span>

    <span class="string">"""
    Pipelined maps:

    &gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4])
    &gt;&gt;&gt; rdd.map(lambda x: 2 * x).cache().map(lambda x: 2 * x).collect()
    [4, 8, 12, 16]
    &gt;&gt;&gt; rdd.map(lambda x: 2 * x).map(lambda x: 2 * x).collect()
    [4, 8, 12, 16]

    Pipelined reduces:
    &gt;&gt;&gt; from operator import add
    &gt;&gt;&gt; rdd.map(lambda x: 2 * x).reduce(add)
    20
    &gt;&gt;&gt; rdd.flatMap(lambda x: [x, x]).reduce(add)
    20
    """</span>

    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, prev, func, preservesPartitioning=False)</span>:</span>
        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(prev, PipelinedRDD) <span class="keyword">or</span> <span class="keyword">not</span> prev._is_pipelinable():
            <span class="comment"># This transformation is the first in its stage:</span>
            self.func = func
            self.preservesPartitioning = preservesPartitioning
            self._prev_jrdd = prev._jrdd
            self._prev_jrdd_deserializer = prev._jrdd_deserializer
        <span class="keyword">else</span>:
            prev_func = prev.func

            <span class="function"><span class="keyword">def</span> <span class="title">pipeline_func</span><span class="params">(split, iterator)</span>:</span>
                <span class="keyword">return</span> func(split, prev_func(split, iterator))
            self.func = pipeline_func
            self.preservesPartitioning = \
                prev.preservesPartitioning <span class="keyword">and</span> preservesPartitioning
            self._prev_jrdd = prev._prev_jrdd  <span class="comment"># maintain the pipeline</span>
            self._prev_jrdd_deserializer = prev._prev_jrdd_deserializer
        self.is_cached = <span class="keyword">False</span>
        self.is_checkpointed = <span class="keyword">False</span>
        self.ctx = prev.ctx
        self.prev = prev
        self._jrdd_val = <span class="keyword">None</span>
        self._id = <span class="keyword">None</span>
        self._jrdd_deserializer = self.ctx.serializer
        self._bypass_serializer = <span class="keyword">False</span>
        self.partitioner = prev.partitioner <span class="keyword">if</span> self.preservesPartitioning <span class="keyword">else</span> <span class="keyword">None</span>
        self._broadcast = <span class="keyword">None</span>
</code></pre><p>我们可以看到，PipelinedRDD只是记录下当前操作但不执行所以每做一次rdd操作，只是记录下了对应的映射关系，数据集还是在原始状态。只有当使用到了reduce这类函数时才会被执行计算。</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">mean</span><span class="params">(self)</span>:</span>
    <span class="string">"""
    Compute the mean of this RDD's elements.

    &gt;&gt;&gt; sc.parallelize([1, 2, 3]).mean()
    2.0
    """</span>
    <span class="keyword">return</span> self.stats().mean()

<span class="function"><span class="keyword">def</span> <span class="title">stats</span><span class="params">(self)</span>:</span>
    <span class="string">"""
    Return a L{StatCounter} object that captures the mean, variance
    and count of the RDD's elements in one operation.
    """</span>
    <span class="function"><span class="keyword">def</span> <span class="title">redFunc</span><span class="params">(left_counter, right_counter)</span>:</span>
        <span class="keyword">return</span> left_counter.mergeStats(right_counter)

    <span class="keyword">return</span> self.mapPartitions(<span class="keyword">lambda</span> i: [StatCounter(i)]).reduce(redFunc)
</code></pre><p>这里，也是回到了PipelinedRDD，但是这次就不只保存待执行的函数了，而是通过jrdd执行</p>
<pre><code>@property
def _jrdd(self):
    <span class="keyword">if</span> self._jrdd_val:
        return self._jrdd_val
    <span class="keyword">if</span> self._bypass_serializer:
        self._jrdd_deserializer = <span class="function"><span class="title">NoOpSerializer</span><span class="params">()</span></span>

    <span class="keyword">if</span> self<span class="class">.ctx</span><span class="class">.profiler_collector</span>:
        profiler = self<span class="class">.ctx</span><span class="class">.profiler_collector</span><span class="class">.new_profiler</span>(self.ctx)
    <span class="keyword">else</span>:
        profiler = None

    command = (self<span class="class">.func</span>, profiler, self._prev_jrdd_deserializer,
               self._jrdd_deserializer)
    pickled_cmd, bvars, env, includes = _prepare_for_python_RDD(self<span class="class">.ctx</span>, command, self)
    python_rdd = self<span class="class">.ctx</span>._jvm.<span class="function"><span class="title">PythonRDD</span><span class="params">(self._prev_jrdd.rdd()</span></span>,
                                         <span class="function"><span class="title">bytearray</span><span class="params">(pickled_cmd)</span></span>,
                                         env, includes, self<span class="class">.preservesPartitioning</span>,
                                         self<span class="class">.ctx</span><span class="class">.pythonExec</span>,
                                         bvars, self<span class="class">.ctx</span>._javaAccumulator)
    self._jrdd_val = python_rdd.<span class="function"><span class="title">asJavaRDD</span><span class="params">()</span></span>

    <span class="keyword">if</span> profiler:
        self._id = self._jrdd_val.<span class="function"><span class="title">id</span><span class="params">()</span></span>
        self<span class="class">.ctx</span><span class="class">.profiler_collector</span><span class="class">.add_profiler</span>(self._id, profiler)
    return self._jrdd_val
</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/05/14/spark-rdd-pipeline-lazyevaluation/" data-id="cib610ith000iyjv4yqfuzg8b" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/pyspark/">pyspark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/rdd/">rdd</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-comparison-between-high-performance-packages-for-python" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/12/comparison-between-high-performance-packages-for-python/" class="article-date">
  <time datetime="2015-05-12T10:01:30.000Z" itemprop="datePublished">2015-05-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/12/comparison-between-high-performance-packages-for-python/">comparison_between_high_performance_packages_for_python</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>一直以来大家提到python的性能总是一个字慢，然后顺带提一下GIL表示自己也知道。但其实python有不少模块可以显著提高性能，而且GIL也并不总是影响性能。这里就做了一个很简单的测试。二维矩阵求和。<br>鉴于运行效率，以及内存连续性考虑，这里的矩阵都是用numpy提供的array而没有使用list。numpy的数组和list相比更接近于c fortran的数组。</p>
<pre><code>Include/listobject.h
typedef struct {
    PyObject_VAR_HEAD
    /* Vector <span class="keyword">of</span> pointers <span class="keyword">to</span> <span class="type">list</span> elements.  <span class="type">list</span>[<span class="number">0</span>] <span class="keyword">is</span> ob_item[<span class="number">0</span>], etc. */
    PyObject **ob_item;

    /* ob_item <span class="keyword">contains</span> <span class="constant">space</span> <span class="keyword">for</span> 'allocated' elements.  The <span class="type">number</span>
     * currently <span class="keyword">in</span> use <span class="keyword">is</span> ob_size.
     * Invariants:
     *     <span class="number">0</span> &lt;= ob_size &lt;= allocated
     *     len(<span class="type">list</span>) == ob_size
     *     ob_item == NULL implies ob_size == allocated == <span class="number">0</span>
     * <span class="type">list</span>.sort() temporarily sets allocated <span class="keyword">to</span> -<span class="number">1</span> <span class="keyword">to</span> detect mutations.
     *
     * Items must normally <span class="keyword">not</span> be NULL, except during construction when
     * <span class="keyword">the</span> <span class="type">list</span> <span class="keyword">is</span> <span class="keyword">not</span> yet visible outside <span class="keyword">the</span> function <span class="keyword">that</span> builds <span class="keyword">it</span>.
     */
    Py_ssize_t allocated;
} PyListObject;
</code></pre><p>拿一个包含浮点数的list来说，list里面的东西其实是PyFloatObject</p>
<pre><code>Include/floatobject.h
<span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> {</span>
    PyObject_HEAD
    <span class="built_in">double</span> ob_fval;
} PyFloatObject;
</code></pre><p>也就是一个PyObject_HEAD的头加上具体的值，看源代码可以发现PyObject_HEAD的定义是这样子的</p>
<pre><code>Include/<span class="keyword">object</span>.h
<span class="preprocessor">#<span class="keyword">define</span> PyObject_HEAD                   \</span>
    _PyObject_HEAD_EXTRA                \
    Py_ssize_t ob_refcnt;               \
    <span class="keyword">struct</span> _typeobject *ob_type;
</code></pre><p>一堆零零碎碎的东西。本身list存object的时候已经不能保证一个list内的对象是内存连续了，对象内部也掺杂着其他信息，这样子的内存排列方式对于计算其实没有多少帮助。<br>例如在做浮点计算的时候要用到的sse/avx指令集就要求2/4个double必须连续排列。<br>而对于numpy来说，事情就简单的多了</p>
<pre><code>numpy/core/include/numpy/ndarraytypes.h
typedef struct tagPyArrayObject_fields {
    PyObject_HEAD
    /<span class="keyword">*</span> Pointer to the raw data buffer <span class="keyword">*</span>/
    char <span class="keyword">*</span>data;
    /<span class="keyword">*</span> The number of dimensions, also called 'ndim' <span class="keyword">*</span>/
    int nd;
    /<span class="keyword">*</span> The size in each dimension, also called 'shape' <span class="keyword">*</span>/
    npy_intp <span class="keyword">*</span>dimensions;
    /<span class="keyword">*</span>
     <span class="keyword">*</span> Number of bytes to jump to get to the
     <span class="keyword">*</span> next element in each dimension
     <span class="keyword">*</span>/
    npy_intp <span class="keyword">*</span>strides;
    /<span class="keyword">*</span>
     <span class="keyword">*</span> This object is decref'd upon
     <span class="keyword">*</span> deletion of array. Except in the
     <span class="keyword">*</span> case of UPDATEIFCOPY which has
     <span class="keyword">*</span> special handling.
     <span class="keyword">*</span>
     <span class="keyword">*</span> For views it points to the original
     <span class="keyword">*</span> array, collapsed so no chains of
     <span class="keyword">*</span> views occur.
     <span class="keyword">*</span>
     <span class="keyword">*</span> For creation from buffer object it
     <span class="keyword">*</span> points to an object that shold be
     <span class="keyword">*</span> decref'd on deletion
     <span class="keyword">*</span>
     <span class="keyword">*</span> For UPDATEIFCOPY flag this is an
     <span class="keyword">*</span> array to-be-updated upon deletion
     <span class="keyword">*</span> of this one
     <span class="keyword">*</span>/
    PyObject <span class="keyword">*</span>base;
    /<span class="keyword">*</span> Pointer to type structure <span class="keyword">*</span>/
    PyArray_Descr <span class="keyword">*</span>descr;
    /<span class="keyword">*</span> Flags describing array -- see below <span class="keyword">*</span>/
    int flags;
    /<span class="keyword">*</span> For weak references <span class="keyword">*</span>/
    PyObject <span class="keyword">*</span>weakreflist;
} PyArrayObject_fields;
</code></pre><p>除了ndim，size这些零碎以外，核心部分的数据都是连续存放在一个char *data里的，这就能保证了浮点数据的连续存放。在做浮点运算的时候也能利用上指令集了。<br>好了说完这些我们回过头来看python的这些加速库就能知道孰优孰劣了</p>
<p>首先是比较的条件，7000×7000的矩阵，全都是随机数的求和。虚拟机<br>当然是最差的cpython</p>
<pre><code>def sum2d(arr):
    M, N = arr.shape
    <span class="literal">result</span> = <span class="number">0</span>.<span class="number">0</span>
    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="type">range</span>(M):
        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="type">range</span>(N):
            <span class="literal">result</span> += arr[i, j]
    <span class="keyword">return</span> <span class="literal">result</span>
</code></pre><p>时间是9.8秒</p>
<p>然后试试看f2py</p>
<pre><code><span class="function"><span class="keyword">subroutine</span></span> sum2dfort(x, y, n, m)
    <span class="type">implicit</span> <span class="type">none</span>
    <span class="type">integer</span>(<span class="keyword">kind</span> = <span class="number">4</span>) :: n, m
    <span class="type">real</span>(<span class="keyword">kind</span> = <span class="number">8</span>), <span class="type">dimension</span>(n, m), <span class="type">intent</span>(<span class="type">in</span>) :: x
    <span class="type">real</span>(<span class="keyword">kind</span> = <span class="number">8</span>), <span class="type">intent</span>(<span class="type">out</span>) :: y
    <span class="type">integer</span>(<span class="keyword">kind</span> = <span class="number">4</span>):: i, j
    y = <span class="number">0.0</span>
    <span class="keyword">do</span> j = <span class="number">1</span>, m
        <span class="keyword">do</span> i = <span class="number">1</span>, n
                y = y + x(i, j)
        <span class="keyword">end</span> <span class="keyword">do</span>
    <span class="keyword">end</span> <span class="keyword">do</span>
    <span class="keyword">return</span>
<span class="keyword">end</span> <span class="function"><span class="keyword">subroutine</span></span> sum2dfort
</code></pre><p>时间是5.83秒，快了很多哟。不过这里还是要吐嘈一下f2py，为啥矩阵一大就要用swap了，其他几个方法都没有这么大的内存需求量</p>
<p>然后是cython</p>
<pre><code>def sum2dcyt(arr):
    cdef <span class="type">int</span> m=arr.shape[<span class="number">0</span>]
    cdef <span class="type">int</span> n=arr.shape[<span class="number">1</span>]
    cdef double <span class="literal">result</span>=<span class="number">0</span>.<span class="number">0</span>
    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="type">range</span>(m):
        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="type">range</span>(n):
            <span class="literal">result</span> += arr[i,j]
    <span class="keyword">return</span> <span class="literal">result</span>
</code></pre><p>一定是我写的方式不对，为什么只有9秒？</p>
<p>最后是numba</p>
<pre><code><span class="keyword">from</span> numba <span class="keyword">import</span> jit
@jit
def sum2djit(arr):
    M,N = arr.shape
    <span class="literal">result</span> = <span class="number">0</span>.<span class="number">0</span>
    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="type">range</span>(M):
        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="type">range</span>(N):
            <span class="literal">result</span> += arr[i, j]
    <span class="keyword">return</span> <span class="literal">result</span>
</code></pre><p>居然到了惊人的0.22秒</p>
<p>当然还有一个numpy自带的求和是0.04秒</p>
<p>最后来一个<img src="/images/compare_python_performance.png" alt="汇总"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/05/12/comparison-between-high-performance-packages-for-python/" data-id="cib610ixt001iyjv4g3amwxnc" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/benchmark/">benchmark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cython/">cython</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/f2py/">f2py</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/numba/">numba</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/numpy/">numpy</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-把博客迁到github上来了" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/12/把博客迁到github上来了/" class="article-date">
  <time datetime="2015-05-12T01:52:53.000Z" itemprop="datePublished">2015-05-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/12/把博客迁到github上来了/">把博客迁到github上来了</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>昨天建站搞了一下午，晚上又忙了一会，终于把博客从cnblogs全迁移到github上来了。这里要赞一下hexo的高效。<br>本来没打算迁移的，结果在学习hexo的过程中发现了一篇<a href="http://www.huangyunkun.com/2014/03/17/migrate_from_cnblogs_to_hexo/" target="_blank" rel="external">文章</a>就想试一下。<br>结果很沮丧的发现作者的<a href="https://github.com/htynkn/hexo-migrator-cnblogs/" target="_blank" rel="external">github</a>去年9月份以后就不更新了。<br>没办法，fork一个下来自己改，javascript一点都不懂，只好console.log一个一个点追过来。<br>最后发现cnblogs改版了以后有些标签换掉了，而且hexo从2升级到3以后hexo.util已经没有这个包了。<br>最后下了独立出来的hexo-fs替换了事。<br>没仔细测试过，不过我这里的cnblogs文章都读出来了。<br>最后欢迎大家使用我更新过的<a href="https://github.com/shiyuankun/hexo-migrator-cnblogs" target="_blank" rel="external">hexo-migrate-cnblogs</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/05/12/把博客迁到github上来了/" data-id="cib610iqy0002yjv4zwsqrt0c" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cnblogs/">cnblogs</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hexo/">hexo</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/migrate/">migrate</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hexoblog" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/11/hexoblog/" class="article-date">
  <time datetime="2015-05-11T08:45:35.000Z" itemprop="datePublished">2015-05-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/11/hexoblog/">hexoblog</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="安装hexo3-0的一些勘误">安装hexo3.0的一些勘误</h2><p>心血来潮在github上搞了一个blog，但是注册完了做完了github推荐的步骤以后发现还离博客很远。正好看到一个知乎的<a href="http://www.zhihu.com/question/20962496" target="_blank" rel="external">帖子</a><br>推荐hexo就下来试试看。没想到还不是一般好用。但是由于hexo已经从2.0升级到3.0的，而网络上的教程基本都是针对2.0的，所以很多步骤都有改变。  </p>
<p>首先要有一个github帐号，并且建立一个同名repository，如帐号是testuser那么repository就是testuser.github.io. 当然实际做的过程中要把testuser换成自己的帐号名<br>可以看<a href="http://blog.csdn.net/renfufei/article/details/37725057" target="_blank" rel="external">教程</a>  </p>
<p>申请完了以后要在本机上安装git nodejs npm</p>
<pre><code>aptitude <span class="keyword">install</span> git nodejs nodejs-legacy npm
</code></pre><p>国内有墙，所以npm特别慢还容易断，推荐用taobao的npm镜像</p>
<pre><code>npm install -g cnpm --<span class="keyword">registry</span>=<span class="keyword">http</span>://<span class="keyword">registry</span>.npm.taobao.org
</code></pre><p>然后就是安装hexo<br>root</p>
<pre><code>cnpm install hexo-cli -g

git clone testuser<span class="class">.github</span><span class="class">.io</span>
cd testuser<span class="class">.github</span><span class="class">.io</span>
</code></pre><p>安装hexo</p>
<pre><code>cnpm <span class="operator"><span class="keyword">install</span> hexo <span class="comment">--save</span></span>
</code></pre><p>安装hexo的生成器</p>
<pre><code>cnpm <span class="operator"><span class="keyword">install</span> hexo-generator-<span class="keyword">index</span> <span class="comment">--save</span>
cnpm <span class="keyword">install</span> hexo-generator-archive <span class="comment">--save</span>
cnpm <span class="keyword">install</span> hexo-generator-category <span class="comment">--save</span>
cnpm <span class="keyword">install</span> hexo-generator-tag <span class="comment">--save</span></span>
</code></pre><p>安装hexo的server</p>
<pre><code>cnpm <span class="operator"><span class="keyword">install</span> hexo-<span class="keyword">server</span> <span class="comment">--save</span></span>
</code></pre><p>安装hexo的git部署工具</p>
<pre><code>cnpm <span class="operator"><span class="keyword">install</span> hexo-deployer-git <span class="comment">--save</span></span>
</code></pre><p>安装hexo的渲染工具和feed sitemap生成器</p>
<pre><code>cnpm <span class="operator"><span class="keyword">install</span> hexo-renderer-marked@<span class="number">0.2</span> <span class="comment">--save</span>
cnpm <span class="keyword">install</span> hexo-renderer-stylus@<span class="number">0.2</span> <span class="comment">--save</span>
cnpm <span class="keyword">install</span> hexo-generator-feed@<span class="number">1</span> <span class="comment">--save</span>
cnpm <span class="keyword">install</span> hexo-generator-sitemap@<span class="number">1</span> <span class="comment">--save</span></span>
</code></pre><p>装完这一切以后<br>就可以</p>
<pre><code>hexo init
cnpm <span class="keyword">install</span>
</code></pre><p>安装依赖包<br>注意报错，有缺的就补上<br>我这里就缺hexo-renderer-ejs</p>
<p>然后</p>
<pre><code>hexo generate
hexo <span class="keyword">server</span>
</code></pre><p>本地服务已经开好了。可以访问localhost:4000查看</p>
<p>将github帐号信息放到_config.yml</p>
<pre><code><span class="attribute">deploy</span>:
  <span class="attribute">type</span>: git
  <span class="attribute">repository</span>: git<span class="variable">@github</span>.<span class="attribute">com</span>:testuser/testuser.github.io.git
  <span class="attribute">branch</span>: master
</code></pre><p>生成网页并部署</p>
<pre><code>hexo <span class="keyword">g</span> -<span class="literal">d</span>
</code></pre><p>其他的都可以看参考网页，基本没有改动过</p>
<h2 id="参考">参考</h2><p><a href="http://zipperary.com/2013/05/28/hexo-guide-1/" target="_blank" rel="external">http://zipperary.com/2013/05/28/hexo-guide-1/</a><br><a href="http://zipperary.com/2013/05/28/hexo-guide-2/" target="_blank" rel="external">http://zipperary.com/2013/05/28/hexo-guide-2/</a><br><a href="http://zipperary.com/2013/05/29/hexo-guide-3/" target="_blank" rel="external">http://zipperary.com/2013/05/29/hexo-guide-3/</a><br><a href="http://zipperary.com/2013/05/30/hexo-guide-4/" target="_blank" rel="external">http://zipperary.com/2013/05/30/hexo-guide-4/</a><br><a href="http://zipperary.com/2013/06/02/hexo-guide-5/" target="_blank" rel="external">http://zipperary.com/2013/06/02/hexo-guide-5/</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/05/11/hexoblog/" data-id="cib610iup000vyjv48ese4rz3" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/blog/">blog</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/github/">github</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hexo/">hexo</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/11/hello-world/" class="article-date">
  <time datetime="2015-05-11T08:07:27.000Z" itemprop="datePublished">2015-05-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/11/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="http://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="http://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="http://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick_Start">Quick Start</h2><h3 id="Create_a_new_post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run_server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate_static_files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy_to_remote_sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/05/11/hello-world/" data-id="cib610ivr0011yjv4it5e8so2" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</section>
        
          <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/RDD/">RDD</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/benchmark/">benchmark</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/blog/">blog</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cnblogs/">cnblogs</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cython/">cython</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/dataframe/">dataframe</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/f2py/">f2py</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/github/">github</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/">hexo</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/migrate/">migrate</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mllib/">mllib</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/numba/">numba</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/numpy/">numpy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pyspark/">pyspark</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rdd/">rdd</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala/">scala</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scheduler/">scheduler</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/">spark</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/task/">task</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/RDD/" style="font-size: 10px;">RDD</a><a href="/tags/benchmark/" style="font-size: 10px;">benchmark</a><a href="/tags/blog/" style="font-size: 12.5px;">blog</a><a href="/tags/cnblogs/" style="font-size: 10px;">cnblogs</a><a href="/tags/cython/" style="font-size: 10px;">cython</a><a href="/tags/dataframe/" style="font-size: 10px;">dataframe</a><a href="/tags/f2py/" style="font-size: 10px;">f2py</a><a href="/tags/github/" style="font-size: 10px;">github</a><a href="/tags/hexo/" style="font-size: 12.5px;">hexo</a><a href="/tags/migrate/" style="font-size: 10px;">migrate</a><a href="/tags/mllib/" style="font-size: 12.5px;">mllib</a><a href="/tags/numba/" style="font-size: 10px;">numba</a><a href="/tags/numpy/" style="font-size: 10px;">numpy</a><a href="/tags/pyspark/" style="font-size: 10px;">pyspark</a><a href="/tags/python/" style="font-size: 17.5px;">python</a><a href="/tags/rdd/" style="font-size: 10px;">rdd</a><a href="/tags/scala/" style="font-size: 15px;">scala</a><a href="/tags/scheduler/" style="font-size: 10px;">scheduler</a><a href="/tags/spark/" style="font-size: 20px;">spark</a><a href="/tags/task/" style="font-size: 10px;">task</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/06/">June 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/04/">April 2015</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/01/">January 2015</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/09/">September 2014</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/06/">June 2014</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/12/">December 2013</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/11/">November 2013</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/10/">October 2013</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/09/">September 2013</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/08/">August 2013</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/04/">April 2013</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/06/">June 2012</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/04/">April 2012</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/03/">March 2012</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/01/">January 2012</a><span class="archive-list-count">4</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2015/06/21/explore-spark-distributed-mllib/">spark的mllib到底是不是分布式的</a>
          </li>
        
          <li>
            <a href="/2015/06/09/spark-task-and-scheduler/">spark 的调度和任务</a>
          </li>
        
          <li>
            <a href="/2015/05/15/explore-spark-RDD-source-code/">expore spark RDD source code</a>
          </li>
        
          <li>
            <a href="/2015/05/15/spark-machine-learning-lib-for-python/">spark machine learning lib for python</a>
          </li>
        
          <li>
            <a href="/2015/05/14/explore-the-source-of-pyspark-dataframe/">pyspark dataframe的实现</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 Yuankun Shi<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/about/index.html" class="mobile-nav-link">About</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>
</body>
</html>