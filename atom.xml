<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[彼格海德的笔记空间]]></title>
  <subtitle><![CDATA[a notebook for python]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://yoursite.com/"/>
  <updated>2015-07-20T03:28:52.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name><![CDATA[Yuankun Shi]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[rust学习笔记(三)]]></title>
    <link href="http://yoursite.com/2015/07/17/learning-rust-part-III/"/>
    <id>http://yoursite.com/2015/07/17/learning-rust-part-III/</id>
    <published>2015-07-17T07:38:28.000Z</published>
    <updated>2015-07-20T03:28:52.000Z</updated>
    <content type="html"><![CDATA[<h2 id="迭代器(Iterator)">迭代器(Iterator)</h2><p>rust带了迭代器，而其似乎特别推崇使用迭代器(这点倒是和python很像)，例如：</p>
<pre><code><span class="keyword">let</span> nums = <span class="built_in">vec!</span>[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>];

<span class="keyword">for</span> i <span class="keyword">in</span> <span class="number">0</span>..nums.len() {
    <span class="built_in">println!</span>(<span class="string">"{}"</span>, nums[i]);
}
</code></pre><p>这种写法远比</p>
<pre><code><span class="keyword">let</span> nums = <span class="built_in">vec!</span>[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>];

<span class="keyword">for</span> num <span class="keyword">in</span> &amp;nums {
    <span class="built_in">println!</span>(<span class="string">"{}"</span>, num);
}
</code></pre><p>要差，原因有二，1,第二种更直观，2,第二种不需要做数组的边界检查。与之相关的概念还有两个，迭代器适配器以及消费者，先讲消费者</p>
<h2 id="消费者(Consumer)">消费者(Consumer)</h2><p>消费者很简单，就是一个collect()，collect消费迭代器产生的数据返回一个集合，例如</p>
<pre><code><span class="keyword">let</span> <span class="variable">one_to_one_hundred =</span> (<span class="number">1</span>..<span class="number">101</span>).collect();
</code></pre><p>但这个代码是有问题的，因为collect不知道返回什么样的集合，例如，在以下例子中</p>
<pre><code><span class="keyword">let</span> x = <span class="string">"hello"</span>.chars<span class="literal">()</span>.rev<span class="literal">()</span>.collect<span class="literal">()</span>;
</code></pre><p>编译器不知道是应该返回char组成的数组Vec<char>还是一个字符串String，为了避免这种情况的发生，就必须用指定一下类型。</char></p>
<pre><code>let x = <span class="string">"hello"</span>.chars().rev().<span class="symbol">collect:</span><span class="symbol">:&lt;Vec&lt;char&gt;&gt;</span>();
let one_to_one_hundred = (<span class="number">1</span>..<span class="number">101</span>).<span class="symbol">collect:</span><span class="symbol">:&lt;Vec&lt;u32&gt;&gt;</span>();
</code></pre><p>也可以这么写</p>
<pre><code>let <span class="symbol">x:</span> <span class="constant">Vec&lt;</span>char&gt; = <span class="string">"hello"</span>.chars().rev().<span class="symbol">collect:</span><span class="symbol">:&lt;Vec&lt;char&gt;&gt;</span>();
let one_to_one_hundred = (<span class="number">1</span>..<span class="number">101</span>).<span class="symbol">collect:</span><span class="symbol">:&lt;Vec&lt;_&gt;&gt;</span>();
</code></pre><p>find也是一种消费者，拿一个匿名函数，返回一个Option，Option里面可能有东西Some(_)，也可能什么都没有(None)，这里看着好像haskell里的Maybe，果然好东西大家都会拿来用。</p>
<pre><code><span class="keyword">let</span> greater_than_forty_two = (<span class="number">0.</span>.<span class="number">100</span>)
                             .find(|x| *x &gt; <span class="number">42</span>);

<span class="keyword">match</span> greater_than_forty_two {
    <span class="type">Some</span>(_) =&gt; println!(<span class="string">"We got some numbers!"</span>),
    <span class="type">None</span> =&gt; println!(<span class="string">"No numbers found :("</span>),
}
</code></pre><p>fold也是一种消费者，需要两个参数，第一个是累加器(accumulator)，第二个是闭包</p>
<pre><code><span class="keyword">let</span> <span class="keyword">sum</span> <span class="subst">=</span> (<span class="number">1.</span><span class="built_in">.</span><span class="number">4</span>)<span class="built_in">.</span>fold(<span class="number">0</span>, <span class="subst">|</span><span class="keyword">sum</span>, x<span class="subst">|</span> <span class="keyword">sum</span> <span class="subst">+</span> x);
</code></pre><p>累加器的初始值是0,闭包的参数是sum和x，闭包内容就是sum+x，sum的初始值是0,x从迭代器里获取值。</p>
<h2 id="迭代器适配器">迭代器适配器</h2><p>最简单的迭代器适配器就是map了，从一个迭代器出发，通过对每个迭代器施加一个函数(闭包)获取另一个迭代器。但是要注意，大多数迭代器适配器都是懒惰的，他们不会主动求值</p>
<pre><code>(<span class="number">1</span>..<span class="number">100</span>).map(|x| x + <span class="number">1</span>);

 <span class="number">18</span>:<span class="number">29</span> warning: unused <span class="literal">result</span> which must be used: <span class="keyword">iterator</span> adaptors are lazy <span class="keyword">and</span> <span class="keyword">do</span> nothing unless consumed, <span class="comment">#[warn(unused_must_use)] on by default</span>
    (<span class="number">1</span>..<span class="number">100</span>).map(|x| x + <span class="number">1</span>);
</code></pre><p>take也是一种适配器</p>
<pre><code>(<span class="number">1</span>..).<span class="function"><span class="title">take</span><span class="params">(<span class="number">5</span>)</span></span>
</code></pre><p>filter也是一种适配器</p>
<pre><code>(<span class="number">1</span>..<span class="number">100</span>).<span class="function"><span class="title">filter</span><span class="params">(|&amp;x| x % <span class="number">2</span> == <span class="number">0</span>)</span></span>
</code></pre><p>值得注意的是，所有的适配器都不会主动求值，包括有副作用的函数，比如</p>
<pre><code>(<span class="number">1.</span>.<span class="number">100</span>).<span class="keyword">map</span>(|x| <span class="built_in">println</span>!(<span class="string">"{}"</span>, x));
</code></pre><p>如果想要他们主动求值，必须套在for里面</p>
<pre><code>for i in (<span class="number">1</span>..<span class="number">100</span>).map(|x| println!(<span class="string">"{}"</span>, x)).take(<span class="number">5</span>){
    println!(<span class="string">"a"</span>)<span class="comment">;</span>
}
<span class="number">1</span>
<span class="literal">a</span>
<span class="number">2</span>
<span class="literal">a</span>
<span class="number">3</span>
<span class="literal">a</span>
<span class="number">4</span>
<span class="literal">a</span>
<span class="number">5</span>
<span class="literal">a</span>
</code></pre><p>这点和haskell很不一样(也有可能我记错了)</p>
<h2 id="并发(Concurrency)">并发(Concurrency)</h2><p>并发这一块暂时没看懂回头再来补</p>
<h2 id="错误处理">错误处理</h2><p>rust有两种错误，一种叫做failure，一种叫做panic。前者可以通过代码处理继续运行，而后者则直接停止程序运行。例如</p>
<pre><code><span class="preprocessor">#[derive(Debug)]</span>
<span class="keyword">enum</span> <span class="title">Version</span>{Version1, Version2}

<span class="preprocessor">#[derive(Debug)]</span>
<span class="keyword">enum</span> <span class="title">ParseError</span>{InvalidHeaderLength, InvalidVersion}

<span class="function"><span class="keyword">fn</span> <span class="title">parse_version</span></span>(header: &amp;[<span class="keyword">u8</span>])-&gt;Result&lt;Version, ParseError&gt;{
    <span class="keyword">if</span> header.len()&lt;<span class="number">1</span>{
        <span class="keyword">return</span> Err(ParseError::InvalidHeaderLength);
    }
    <span class="keyword">match</span> header[<span class="number">0</span>]{
        <span class="number">1</span>=&gt;Ok(Version::Version1),
        <span class="number">2</span>=&gt;Ok(Version::Version2),
        _=&gt;Err(ParseError::InvalidVersion)
    }
}

<span class="function"><span class="keyword">fn</span> <span class="title">main</span></span>(){

    <span class="keyword">let</span> version=parse_version(&amp;[<span class="number">3</span>]);
    <span class="keyword">match</span> version{
        Ok(v)=&gt;{
            <span class="built_in">println!</span>(<span class="string">"working with version: {:?}"</span>,v);
        }
        Err(e)=&gt;{
            <span class="built_in">println!</span>(<span class="string">"error parsing header: {:?}"</span>,e);
        }
    }
    <span class="keyword">let</span> version=parse_version(&amp;[<span class="number">1</span>]);
    <span class="keyword">for</span> i <span class="keyword">in</span> version
    <span class="keyword">match</span> version{
        Ok(v)=&gt;{
            <span class="built_in">println!</span>(<span class="string">"working with version: {:?}"</span>,v);
        }
        Err(e)=&gt;{
            <span class="built_in">println!</span>(<span class="string">"error parsing header: {:?}"</span>,e);
        }
    }
}
</code></pre><p>这里就是普通的failure，对于panic，我们之前已经用到的unwrap产生的就是panic，事实上unwrap的工作就是将failure处理(upgrade)为panic。例如</p>
<pre><code><span class="rule"><span class="attribute">io</span>:<span class="value">:<span class="function">stdin</span>().<span class="function">read_line</span>(&amp;mut buffer).<span class="function">unwrap</span>()</span></span>;
</code></pre><p>unwrap的意思就是如果碰到OK就通过，但碰到Err就报错退出。</p>
<p>let mut buffer = String::new();<br>let input = io::stdin().read_line(&amp;mut buffer)<br>                       .ok()<br>                       .expect(“Failed to read line”);</p>
<p>这里用ok更优美点，ok将Result转化为Option，如果发生None，那打印expect的参数退出。expect做的事情很unwrap差不多。</p>
<p>对于函数的错误还有try!这个宏，但是try!这个宏只能在返回Result类型的函数内使用，try失败就返回一个Result类型了，所以main中不能使用try，因为main的返回值不是Result。</p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="迭代器(Iterator)">迭代器(Iterator)</h2><p>rust带了迭代器，而其似乎特别推崇使用迭代器(这点倒是和python很像)，例如：</p>
<pre><code><span class="keyword">let</span> nums ]]>
    </summary>
    
      <category term="rust" scheme="http://yoursite.com/tags/rust/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[rust学习笔记(二)]]></title>
    <link href="http://yoursite.com/2015/07/17/learning-rust-part-II/"/>
    <id>http://yoursite.com/2015/07/17/learning-rust-part-II/</id>
    <published>2015-07-17T03:04:24.000Z</published>
    <updated>2015-07-20T02:46:41.000Z</updated>
    <content type="html"><![CDATA[<h2 id="多线程">多线程</h2><p>继续学习rust，今天看的是一个怎么利用rust写多线程的例子。这个问题叫做哲学家吃饭，一张圆桌子五个哲学家，每人吃饭都要先拿起左手的筷子再拿右手的筷子，吃完放回去。但如果统一拿左手的筷子，必然会发生线程死锁:每个人都拿了左手的筷子并且等右手的筷子。</p>
<pre><code><span class="keyword">use</span> std::thread;<span class="comment">//引入线程</span>
<span class="keyword">use</span> std::sync::{Mutex, Arc};<span class="comment">//引入锁和原子操作</span>

<span class="keyword">struct</span> Table{
    forks: Vec&lt;Mutex&lt;()&gt;&gt;,<span class="comment">//这是个锁组成的数组，数组的每个元素代表一双筷子</span>
}

<span class="keyword">struct</span> Philosopher{
    name: String,
    left: usize,<span class="comment">//哲学家的左右手代表了锁在数组中的相应位置</span>
    right: usize,
}

<span class="keyword">impl</span> Philosopher{
    <span class="function"><span class="keyword">fn</span> <span class="title">new</span></span>(name: &amp;<span class="keyword">str</span>, left: usize, right: usize)-&gt;Philosopher{
        Philosopher{
            name :name.to_string(),
            left: left,
            right: right,
        }
    }
    <span class="function"><span class="keyword">fn</span> <span class="title">eat</span></span>(&amp;<span class="keyword">self</span>, table: &amp;Table){
        <span class="keyword">let</span> _left = table.forks[<span class="keyword">self</span>.left].lock().unwrap();<span class="comment">//获取锁，_left前的_代表不对起不使用进行报警</span>
        <span class="keyword">let</span> _right=table.forks[<span class="keyword">self</span>.right].lock().unwrap();<span class="comment">//unwrap代表当获取锁失败时无视失败，并且将failure变成panic!</span>
        <span class="built_in">println!</span>(<span class="string">"{} is eating."</span>,<span class="keyword">self</span>.name);
        thread::sleep_ms(<span class="number">1000</span>);
        <span class="built_in">println!</span>(<span class="string">"{} is done eating."</span>, <span class="keyword">self</span>.name);
    }
}

<span class="function"><span class="keyword">fn</span> <span class="title">main</span></span>() {
    <span class="keyword">let</span> table = Arc::new(Table {forks: <span class="built_in">vec!</span>[
            Mutex::new(()),
            Mutex::new(()),
            Mutex::new(()),
            Mutex::new(()),
            Mutex::new(()),
            ]});<span class="comment">//(Table {...})是建立了一个Table对象，由于我们没有给Table绑一个new函数所以需要这种方法新建对象。</span>
            <span class="comment">//Arc(Atomic reference count)代表这个Table是支持原子引用计数的，每在一个线程中分享，就加一，这个线程退出就减一。</span>

    <span class="keyword">let</span> philosophers=<span class="built_in">vec!</span>[
        Philosopher::new(<span class="string">"Baruch Spinoza"</span>,<span class="number">0</span>,<span class="number">1</span>),
        Philosopher::new(<span class="string">"Gilles Deleuze"</span>,<span class="number">1</span>,<span class="number">2</span>),
        Philosopher::new(<span class="string">"Karl Marx"</span>,<span class="number">2</span>,<span class="number">3</span>),
        Philosopher::new(<span class="string">"Friedrich Nietzsche"</span>,<span class="number">3</span>,<span class="number">4</span>),
        Philosopher::new(<span class="string">"Michel Foucault"</span>,<span class="number">0</span>,<span class="number">4</span>),<span class="comment">//防止死锁的小技巧，先获取0再获取4</span>
    ];
    <span class="keyword">let</span> handles: Vec&lt;_&gt;=philosophers.into_iter().map(|p|{<span class="comment">//into_iter将数组转换为迭代器(iterator)</span>
            <span class="keyword">let</span> table = table.clone();<span class="comment">//这里将原始引用复制一份</span>
            thread::spawn(move||{<span class="comment">//起一个线程执行对应的p.eat, move用于维持闭包，获得eat的返回值</span>
                p.eat(&amp;table);
                })}).collect();
    <span class="keyword">for</span> h <span class="keyword">in</span> handles{
        h.join().unwrap();<span class="comment">//等每个线程结束</span>
    }
}
</code></pre><h2 id="FFI">FFI</h2><p>rust还很好的支持了FFI(foreign function interface)，能够在别的语言中调用rust写的库，例如<br>新建一个项目</p>
<pre><code><span class="variable">$ </span>cargo new embed
<span class="variable">$ </span>cd embed
</code></pre><p>里面填这些代码</p>
<pre><code><span class="keyword">use</span> std::thread;

<span class="preprocessor">#[no_mangle]</span><span class="comment">//产生外部能辨认的symbol</span>
<span class="keyword">pub</span> <span class="keyword">extern</span> <span class="function"><span class="keyword">fn</span> <span class="title">process</span></span>(){<span class="comment">//pub 和extern代表将process函数公开，外部可调用</span>
    <span class="keyword">let</span> handles: Vec&lt;_&gt;=(<span class="number">0</span>..<span class="number">10</span>).map(|_|{
        thread::spawn(||{
            <span class="keyword">let</span> <span class="keyword">mut</span> _x=<span class="number">0</span>;
            <span class="keyword">for</span> _ <span class="keyword">in</span> (<span class="number">0</span>..<span class="number">5_000_001</span>){
                _x+=<span class="number">1</span>
            }
        })
    }).collect();
    <span class="keyword">for</span> h <span class="keyword">in</span> handles{
        h.join().ok().expect(<span class="string">"Could not join a thread!"</span>);
    }
}
</code></pre><p>更改配置文件</p>
<pre><code>vi Cargo.toml
[lib]
name = <span class="string">"embed"</span>
<span class="keyword">crate</span>-<span class="keyword">type</span> = [<span class="string">"dylib"</span>]<span class="comment">//标准动态链接库，不然默认是rlib</span>
</code></pre><p>编译</p>
<pre><code>$ cargo build <span class="comment">--release</span>
</code></pre><p>python中调用</p>
<pre><code>from ctypes import cdll
lib = cdll.<span class="function"><span class="title">LoadLibrary</span><span class="params">(<span class="string">"target/release/libembed.so"</span>)</span></span>
lib.<span class="function"><span class="title">process</span><span class="params">()</span></span>
<span class="function"><span class="title">print</span><span class="params">(<span class="string">"done!"</span>)</span></span>
</code></pre><h2 id="测试">测试</h2><p>大致看了一下rust里的测试部分，感觉做的还很贴心方便，和python里的unittest不一样的是，rust直接将测试整合了进来，一共三种测试：和每个function一起的单元测试，整合（integration）测试，文档测试。</p>
<pre><code><span class="variable">$ </span>cargo new adder
<span class="variable">$ </span>cd adder
</code></pre><p>打开src/lib.rs</p>
<pre><code><span class="comment">//! The `adder` crate provides functions that add numbers to other numbers.</span>
<span class="comment">//!</span>
<span class="comment">//! # Examples</span>
<span class="comment">//!</span>
<span class="comment">//! <figure class="highlight erlang-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//<span class="exclamation_mark">!</span> <span class="function_or_atom">assert_eq</span><span class="exclamation_mark">!</span>(<span class="number">4</span>, <span class="function_or_atom">adder</span>::<span class="function_or_atom">add_two</span>(<span class="number">2</span>));</span><br><span class="line">//<span class="exclamation_mark">!</span></span><br></pre></td></tr></table></figure></span>


<span class="comment">/// This function adds two to its argument.</span>
<span class="comment">///</span>
<span class="comment">/// # Examples</span>
<span class="comment">///</span>
<span class="comment">/// <figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/// use adder::add_two;</span></span><br><span class="line"><span class="comment">///</span></span><br><span class="line"><span class="comment">/// assert_eq!(4, add_two(2));</span></span><br><span class="line"><span class="comment">///</span></span><br></pre></td></tr></table></figure></span>

<span class="keyword">pub</span> <span class="function"><span class="keyword">fn</span> <span class="title">add_two</span></span>(a:<span class="keyword">i32</span>) -&gt; <span class="keyword">i32</span>{
    a+<span class="number">2</span>
}

<span class="preprocessor">#[cfg(test)]</span>
<span class="keyword">mod</span> tests {
    <span class="keyword">use</span> super::*;

    <span class="preprocessor">#[test]</span>
    <span class="function"><span class="keyword">fn</span> <span class="title">it_works</span></span>() {  
        <span class="built_in">assert_eq!</span>(<span class="number">4</span>, add_two(<span class="number">2</span>));
    }
}
</code></pre><p>斜杠开头的那几行是文档测试，测试内容用<code>xxxx</code>包围起来，支持markdown。///开头的是函数级别的测试，//!是模块级别的测试。#[cfg(test)]那里是单元测试。可以很方便的写一些辅助函数帮助测试，在非cargo test的时候这一部分代码不会编译。还有一种就是另建一个tests目录，tests/lib.rs，做整合测试。</p>
<pre><code><span class="keyword">extern</span> <span class="keyword">crate</span> adder;

<span class="preprocessor">#[test]</span>
<span class="function"><span class="keyword">fn</span> <span class="title">it_works</span></span>(){
    <span class="built_in">assert_eq!</span>(<span class="number">4</span>, adder::add_two(<span class="number">2</span>));
}
</code></pre><h2 id="条件编译">条件编译</h2><p>rust也支持条件编译，例如</p>
<pre><code>#[<span class="function"><span class="title">cfg</span><span class="params">(foo)</span></span>]

#[<span class="function"><span class="title">cfg</span><span class="params">(bar = <span class="string">"baz"</span>)</span></span>]

#[<span class="function"><span class="title">cfg</span><span class="params">(any(unix, windows)</span></span>)]

#[<span class="function"><span class="title">cfg</span><span class="params">(all(unix, target_pointer_width = <span class="string">"32"</span>)</span></span>)]

#[<span class="function"><span class="title">cfg</span><span class="params">(not(foo)</span></span>)]
</code></pre><p>在函数定义的上一行写下这些字段，能帮助cargo决定这个函数是不是要在当前条件下编译。<br>在Cargo.toml中</p>
<pre><code>[features]
<span class="preprocessor"># no features by default</span>
<span class="default"><span class="keyword">default</span> = []</span>
</code></pre><h2 id="文档">文档</h2><p>刚才已经提到过，需要用///来注释文档，但需要注意的是，文档需要写成这样</p>
<pre><code><span class="dartdoc"><span class="markdown">/// The <span class="code">`Option`</span> type. See [<span class="link_label">the module level documentation</span>](<span class="link_url">../</span>) for more.</span></span>
<span class="keyword">enum</span> Option&lt;T&gt; {
    <span class="dartdoc"><span class="markdown">/// No value</span></span>
    None,
    <span class="dartdoc"><span class="markdown">/// Some value <span class="code">`T`</span></span></span>
    Some(T),
}
</code></pre><p>而不是这样</p>
<pre><code><span class="dartdoc"><span class="markdown">/// The <span class="code">`Option`</span> type. See [<span class="link_label">the module level documentation</span>](<span class="link_url">../</span>) for more.</span></span>
<span class="keyword">enum</span> Option&lt;T&gt; {
    None, <span class="dartdoc"><span class="markdown">/// No value</span></span>
    Some(T), <span class="dartdoc"><span class="markdown">/// Some value <span class="code">`T`</span></span></span>
}
</code></pre><p>使用</p>
<pre><code><span class="title">cargo</span> doc
</code></pre><p>生成文档</p>
<p>大致了解了一下，接下去继续看详细的语法</p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="多线程">多线程</h2><p>继续学习rust，今天看的是一个怎么利用rust写多线程的例子。这个问题叫做哲学家吃饭，一张圆桌子五个哲学家，每人吃饭都要先拿起左手的筷子再拿右手的筷子，吃完放回去。但如果统一拿左手的筷子，必然会发生线程死锁:每个人都拿了左手的筷子]]>
    </summary>
    
      <category term="rust" scheme="http://yoursite.com/tags/rust/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[rust学习笔记(一)]]></title>
    <link href="http://yoursite.com/2015/07/16/learning-rust-part-I/"/>
    <id>http://yoursite.com/2015/07/16/learning-rust-part-I/</id>
    <published>2015-07-16T01:49:38.000Z</published>
    <updated>2015-07-16T06:44:54.000Z</updated>
    <content type="html"><![CDATA[<p>rust今年来发布了1.0版本，很火，一直想学可惜一直太懒，最近有空正好学习一下，写一点笔记。<br>初次关注rust的时候还是1.0版本，等到现在想学了，已经是1.1版本了……所以我是有多懒惰啊。  </p>
<h2 id="安装">安装</h2><pre><code>$ curl -<span class="keyword">sf</span> -L http<span class="variable">s:</span>//static.rust-lang.org/rustup.<span class="keyword">sh</span> | <span class="keyword">sh</span>
</code></pre><h2 id="卸载">卸载</h2><pre><code>$ sudo <span class="regexp">/usr/</span>local<span class="regexp">/lib/</span>rustlib<span class="regexp">/uninstall.sh</span>
</code></pre><p>rust自带一个叫做cargo的项目管理器，可以很方便的新建、编译、运行项目</p>
<h2 id="新建">新建</h2><pre><code><span class="variable">$ </span>cargo new hello_world --bin
<span class="variable">$ </span>cd hello_world
</code></pre><h2 id="编译">编译</h2><pre><code><span class="variable">$ </span>cargo build
</code></pre><h2 id="运行">运行</h2><pre><code>$ cargo <span class="command">run</span>
</code></pre><p>新建一个项目以后，这个项目的目录下会有一个Cargo.toml的文件，对项目内容做了一些必要的描述，例如，版本号，名字。rust和c++/c一样，需要编译运行。所以导入一些依赖库需要在Cargo.toml中指明，例如</p>
<pre><code><span class="title">[dependencies]</span>

<span class="setting">rand=<span class="value"><span class="string">"0.3.0"</span></span></span>
</code></pre><p>cargo会自动去github上寻找这个包在编译时链接。以下贴一个官方的例子简要介绍rust的语法</p>
<pre><code><span class="keyword">extern</span> <span class="keyword">crate</span> rand; <span class="comment">//指明rand是一个外部包，crate在rust中即包</span>

<span class="keyword">use</span> std::io; <span class="comment">//指明需要使用的库</span>
<span class="keyword">use</span> std::cmp::Ordering;
<span class="keyword">use</span> rand::Rng;

<span class="function"><span class="keyword">fn</span> <span class="title">main</span></span>() { <span class="comment">//定义一个main函数，fn是定义函数的关键字</span>
    <span class="built_in">println!</span>(<span class="string">"Guess the number!"</span>);<span class="comment">//println!是宏</span>

    <span class="keyword">let</span> secret_number = rand::thread_rng().gen_range(<span class="number">1</span>, <span class="number">101</span>);<span class="comment">//let是一个绑定，将secret_number绑定到随机数上，rust里默认的变量是不可变的</span>

    <span class="keyword">loop</span> {<span class="comment">//loop代表一个循环</span>
        <span class="built_in">println!</span>(<span class="string">"Please input your guess."</span>);

        <span class="keyword">let</span> <span class="keyword">mut</span> guess = String::new();<span class="comment">//mut代表将guess设定为mutable(可变的)</span>

        io::stdin().read_line(&amp;<span class="keyword">mut</span> guess)<span class="comment">//这里&amp;guess代表的是guess的reference，默认是不可变的，但mut指定为可变（我在想是不是其实就是读了一个字符串得到一个指向这个字符串的地址A，然后直接将A复制到guess这个指针所在的地址，相当于将guess指向了另一个字符串。）</span>
            .ok()<span class="comment">//io会返回io::Result，必须写代码处理，不然编译器会鄙视你，这里ok代表io成功运行什么代码</span>
            .expect(<span class="string">"failed to read line"</span>);<span class="comment">//ok也会返回一个需要处理的值需要expect处理，如果失败就打印这句话并退出</span>

        <span class="keyword">let</span> guess: <span class="keyword">u32</span> = <span class="keyword">match</span> guess.trim().parse() {<span class="comment">//字符转换成数字，并对结果进行模式匹配</span>
            Ok(num) =&gt; num,<span class="comment">//如果成功，就返回数值</span>
            Err(_) =&gt; <span class="keyword">continue</span>,<span class="comment">//如果失败就继续，不出错</span>
        };

        <span class="built_in">println!</span>(<span class="string">"You guessed: {}"</span>, guess);

        <span class="keyword">match</span> guess.cmp(&amp;secret_number) {<span class="comment">//guess和secret_number进行比较，并且进行模式匹配</span>
            Ordering::Less    =&gt; <span class="built_in">println!</span>(<span class="string">"Too small!"</span>),
            Ordering::Greater =&gt; <span class="built_in">println!</span>(<span class="string">"Too big!"</span>),
            Ordering::Equal   =&gt; {
                <span class="built_in">println!</span>(<span class="string">"You win!"</span>);
                <span class="keyword">break</span>;<span class="comment">//退出循环</span>
            }
        }
    }
}
</code></pre><p>这是官方教程的第一个例子。来一个复杂一点的</p>
<pre><code><span class="function"><span class="keyword">fn</span> <span class="title">main</span></span>() {
    <span class="keyword">let</span> <span class="keyword">mut</span> x = <span class="built_in">vec!</span>[<span class="string">"Hello"</span>, <span class="string">"world"</span>];

    <span class="keyword">let</span> y = &amp;x[<span class="number">0</span>];

    x.push(<span class="string">"foo"</span>);
}
</code></pre><p>这段代码会报错</p>
<pre><code>src/main.<span class="string">rs:</span><span class="number">6</span>:<span class="number">5</span>: <span class="number">6</span>:<span class="number">6</span> <span class="string">error:</span> cannot borrow `x` <span class="keyword">as</span> mutable because it is also borrowed <span class="keyword">as</span> immutable
src/main.<span class="string">rs:</span><span class="number">6</span>     x.push(<span class="string">"foo"</span>);
                  ^
src/main.<span class="string">rs:</span><span class="number">4</span>:<span class="number">14</span>: <span class="number">4</span>:<span class="number">15</span> <span class="string">note:</span> previous borrow of `x` occurs here; the immutable borrow prevents subsequent moves or mutable borrows of `x` until the borrow ends
src/main.<span class="string">rs:</span><span class="number">4</span>     let y = &amp;x[<span class="number">0</span>];
                           ^
src/main.<span class="string">rs:</span><span class="number">7</span>:<span class="number">2</span>: <span class="number">7</span>:<span class="number">2</span> <span class="string">note:</span> previous borrow ends here
src/main.<span class="string">rs:</span><span class="number">1</span> fn main() {
...
src/main.<span class="string">rs:</span><span class="number">7</span> }
</code></pre><p>大意就是 x的第一个元素被immutable的引用了一次，所以x就不能被更改，如果我们把他的引用改为mutable呢？</p>
<pre><code>src/main.<span class="string">rs:</span><span class="number">6</span>:<span class="number">5</span>: <span class="number">6</span>:<span class="number">6</span> <span class="string">error:</span> cannot borrow `x` <span class="keyword">as</span> mutable more than once at a time
src/main.<span class="string">rs:</span><span class="number">6</span>     x.push(<span class="string">"foo"</span>);
                  ^
src/main.<span class="string">rs:</span><span class="number">4</span>:<span class="number">19</span>: <span class="number">4</span>:<span class="number">20</span> <span class="string">note:</span> previous borrow of `x` occurs here; the mutable borrow prevents subsequent moves, borrows, or modification of `x` until the borrow ends
src/main.<span class="string">rs:</span><span class="number">4</span>     let y = &amp; mut x[<span class="number">0</span>];
                                ^
src/main.<span class="string">rs:</span><span class="number">7</span>:<span class="number">2</span>: <span class="number">7</span>:<span class="number">2</span> <span class="string">note:</span> previous borrow ends here
src/main.<span class="string">rs:</span><span class="number">1</span> fn main() {
...
src/main.<span class="string">rs:</span><span class="number">7</span> }
</code></pre><p>又报错了，说x不能被多次mutable引用<br>所以rust通过控制引用的可变和不可变以及可变应用次数来达到不需要gc也能自动释放占用内存。要想达到刚才的效果，只能这么做。</p>
<pre><code><span class="function"><span class="keyword">fn</span> <span class="title">main</span></span>() {
    <span class="keyword">let</span> <span class="keyword">mut</span> x = <span class="built_in">vec!</span>[<span class="string">"Hello"</span>, <span class="string">"world"</span>];

    <span class="keyword">let</span> y = x[<span class="number">0</span>].clone();

    x.push(<span class="string">"foo"</span>);
}

或者

<span class="function"><span class="keyword">fn</span> <span class="title">main</span></span>() {
    <span class="keyword">let</span> <span class="keyword">mut</span> x = <span class="built_in">vec!</span>[<span class="string">"Hello"</span>, <span class="string">"world"</span>];

    {
        <span class="keyword">let</span> y = &amp;x[<span class="number">0</span>];
    }

    x.push(<span class="string">"foo"</span>);
}
</code></pre><p>前者给y做了一个x的备份，这样很自然的就将同一内存的操作分开了。后者在一个scope内新建了一个引用，离开这个scope这个引用就消失了，所以也能对x进行可变操作。<br>先写到这里，继续看书明天更新</p>
<h2 id="参考">参考</h2><p><a href="https://doc.rust-lang.org/stable/book/hello-cargo.html" target="_blank" rel="external">https://doc.rust-lang.org/stable/book/hello-cargo.html</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>rust今年来发布了1.0版本，很火，一直想学可惜一直太懒，最近有空正好学习一下，写一点笔记。<br>初次关注rust的时候还是1.0版本，等到现在想学了，已经是1.1版本了……所以我是有多懒惰啊。  </p>
<h2 id="安装">安装</h2><pre><code>$]]>
    </summary>
    
      <category term="rust" scheme="http://yoursite.com/tags/rust/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[spark的mllib到底是不是分布式的]]></title>
    <link href="http://yoursite.com/2015/06/21/explore-spark-distributed-mllib/"/>
    <id>http://yoursite.com/2015/06/21/explore-spark-distributed-mllib/</id>
    <published>2015-06-21T04:26:50.000Z</published>
    <updated>2015-06-21T07:32:43.000Z</updated>
    <content type="html"><![CDATA[<p>端午节，群里有个人问了一个spark关于mllib扩展性的问题，不太确定，继续看代码。正好上周1.4也发布了，也把代码下下来比较了一下。<br>比较以后才知道，原来scala版本的计算和pyspark差了好多，pyspark关于linalg的计算都是直接调用numpy，也就是非并行的。例如linalg.py里随处可见的np.dot</p>
<pre><code>assert <span class="function"><span class="title">len</span><span class="params">(self)</span></span> == other<span class="class">.shape</span>[<span class="number">0</span>], <span class="string">"dimension mismatch"</span>
             return other.<span class="function"><span class="title">transpose</span><span class="params">()</span></span>.<span class="function"><span class="title">dot</span><span class="params">(self.toArray()</span></span>)
         <span class="keyword">else</span>:
             assert <span class="function"><span class="title">len</span><span class="params">(self)</span></span> == _vector_size(other), <span class="string">"dimension mismatch"</span>
             <span class="keyword">if</span> <span class="function"><span class="title">isinstance</span><span class="params">(other, SparseVector)</span></span>:
                 return other.<span class="function"><span class="title">dot</span><span class="params">(self)</span></span>
             elif <span class="function"><span class="title">isinstance</span><span class="params">(other, Vector)</span></span>:
                 return np.<span class="function"><span class="title">dot</span><span class="params">(self.toArray()</span></span>, other.<span class="function"><span class="title">toArray</span><span class="params">()</span></span>)
             <span class="keyword">else</span>:
                 return np.<span class="function"><span class="title">dot</span><span class="params">(self.toArray()</span></span>, other)
</code></pre><p>但scala版本的linalg多了一个distributed的目录，下面就定义了好几种分布式矩阵</p>
<pre><code>:~/spark-<span class="number">1.4</span>.<span class="number">0</span>$ ls mllib/src/<span class="keyword">main</span>/scala/org/apache/spark/mllib/linalg/distributed/*
mllib/src/<span class="keyword">main</span>/scala/org/apache/spark/mllib/linalg/distributed/<span class="constant">Block</span>Matrix.scala
mllib/src/<span class="keyword">main</span>/scala/org/apache/spark/mllib/linalg/distributed/<span class="constant">Coordinate</span>Matrix.scala
mllib/src/<span class="keyword">main</span>/scala/org/apache/spark/mllib/linalg/distributed/<span class="constant">Distributed</span>Matrix.scala
mllib/src/<span class="keyword">main</span>/scala/org/apache/spark/mllib/linalg/distributed/<span class="constant">Indexed</span>RowMatrix.scala
mllib/src/<span class="keyword">main</span>/scala/org/apache/spark/mllib/linalg/distributed/<span class="constant">Row</span>Matrix.scala
</code></pre><p>除此以外还多了SVD，PCA一些常见矩阵运算。当然，在linalg中的BLAS.scala里面，依旧是调用了本地的blas库来计算矩阵乘法</p>
<pre><code>private <span class="function"><span class="keyword">def</span> <span class="title">dot</span><span class="params">(x: DenseVector, y: DenseVector)</span>:</span> Double = {
  val n = x.size
  f2jBLAS.ddot(n, x.values, <span class="number">1</span>, y.values, <span class="number">1</span>)
}     
</code></pre><p>这里仅限于非稀疏矩阵，spark中所有的稀疏矩阵都是自己写的没有调用额外的库</p>
<pre><code><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">dot</span>(</span>x: <span class="type">SparseVector</span>, y: <span class="type">SparseVector</span>): <span class="type">Double</span> = {
     <span class="function"><span class="keyword">val</span> <span class="title">xValues</span> =</span> x.values
     <span class="function"><span class="keyword">val</span> <span class="title">xIndices</span> =</span> x.indices
     <span class="function"><span class="keyword">val</span> <span class="title">yValues</span> =</span> y.values
     <span class="function"><span class="keyword">val</span> <span class="title">yIndices</span> =</span> y.indices
     <span class="function"><span class="keyword">val</span> <span class="title">nnzx</span> =</span> xIndices.size
     <span class="function"><span class="keyword">val</span> <span class="title">nnzy</span> =</span> yIndices.size

     <span class="keyword">var</span> kx = <span class="number">0</span>
     <span class="keyword">var</span> ky = <span class="number">0</span>
     <span class="keyword">var</span> sum = <span class="number">0.0</span>
     <span class="comment">// y catching x</span>
     <span class="keyword">while</span> (kx &lt; nnzx &amp;&amp; ky &lt; nnzy) {
       <span class="function"><span class="keyword">val</span> <span class="title">ix</span> =</span> xIndices(kx)
       <span class="keyword">while</span> (ky &lt; nnzy &amp;&amp; yIndices(ky) &lt; ix) {
         ky += <span class="number">1</span>
       }
       <span class="keyword">if</span> (ky &lt; nnzy &amp;&amp; yIndices(ky) == ix) {
         sum += xValues(kx) * yValues(ky)
         ky += <span class="number">1</span>
       }
       kx += <span class="number">1</span>
     }
     sum
   }
</code></pre><p>矩阵乘法也是一样处理，都是牵涉到稀疏矩阵计算的就用自己写的方法</p>
<pre><code>private def gemm(
       alpha: Double,
       A: DenseMatrix,
       B: DenseMatrix,
       beta: Double,
       C: DenseMatrix): <span class="variable">Unit =</span> {
     val <span class="variable">tAstr =</span> <span class="keyword">if</span> (A.isTransposed) <span class="string">"T"</span> <span class="keyword">else</span> <span class="string">"N"</span>
     val <span class="variable">tBstr =</span> <span class="keyword">if</span> (B.isTransposed) <span class="string">"T"</span> <span class="keyword">else</span> <span class="string">"N"</span>
     val <span class="variable">lda =</span> <span class="keyword">if</span> (!A.isTransposed) A.numRows <span class="keyword">else</span> A.numCols
     val <span class="variable">ldb =</span> <span class="keyword">if</span> (!B.isTransposed) B.numRows <span class="keyword">else</span> B.numCols

     require(A.<span class="variable">numCols =</span>= B.numRows,
       s<span class="string">"The columns of A don't match the rows of B. A: <span class="subst">${A.numCols}</span>, B: <span class="subst">${B.numRows}</span>"</span>)
     require(A.<span class="variable">numRows =</span>= C.numRows,
       s<span class="string">"The rows of C don't match the rows of A. C: <span class="subst">${C.numRows}</span>, A: <span class="subst">${A.numRows}</span>"</span>)
     require(B.<span class="variable">numCols =</span>= C.numCols,
       s<span class="string">"The columns of C don't match the columns of B. C: <span class="subst">${C.numCols}</span>, A: <span class="subst">${B.numCols}</span>"</span>)
     nativeBLAS.dgemm(tAstr, tBstr, A.numRows, B.numCols, A.numCols, alpha, A.values, lda,
       B.values, ldb, beta, C.values, C.numRows)
   }
</code></pre><p>然后，我们回到spark的分布式矩阵，分布式矩阵的好处是直接利用rdd就能将对应的值转化为矩阵，例如这样</p>
<pre><code>val rows = sc.textFile(args(<span class="number">0</span>)).<span class="built_in">map</span> { <span class="built_in">line</span> =&gt;
       val values = <span class="built_in">line</span>.<span class="built_in">split</span>(<span class="string">' '</span>).<span class="built_in">map</span>(_.toDouble)
       Vectors.dense(values)
     }
     val mat = <span class="keyword">new</span> RowMatrix(rows)
</code></pre><p>这样，mat就是一个分布式的RowMatrix了，所做的就是赋值一个rdd。对于做SVD这类计算来说，RowMatrix定义了三种方法LocalARPACK，LocalLAPACK和DistARPACK，根据m，n的大小来自动决定到底用哪个计算</p>
<pre><code><span class="keyword">if</span> (n &lt; <span class="number">100</span> || (k &gt; n / <span class="number">2</span> &amp;&amp; n &lt;= <span class="number">15000</span>)) {
          <span class="regexp">//</span> If n <span class="keyword">is</span> small <span class="keyword">or</span> k <span class="keyword">is</span> large compared <span class="reserved">with</span> n, we better compute the Gramian matrix first
          <span class="regexp">//</span> <span class="keyword">and</span> <span class="keyword">then</span> compute its eigenvalues locally, instead <span class="keyword">of</span> making multiple passes.
          <span class="keyword">if</span> (k &lt; n / <span class="number">3</span>) {
            SVDMode.LocalARPACK
          } <span class="keyword">else</span> {
            SVDMode.LocalLAPACK
          }
        } <span class="keyword">else</span> {
          <span class="regexp">//</span> If k <span class="keyword">is</span> small compared <span class="reserved">with</span> n, we use ARPACK <span class="reserved">with</span> distributed multiplication.
          SVDMode.DistARPACK
        }
</code></pre><p>基本上矩阵尺寸不大的时候就用local的方法来计算了，似乎作者认为ARPACK在计算方阵的时候效率不如lapack。所以当k&gt;n/3的时候选择了lapack。选完了以后就开始计算对角化矩阵</p>
<pre><code><span class="function"><span class="keyword">val</span> (</span>sigmaSquares: <span class="type">BDV</span>[<span class="type">Double</span>], u: <span class="type">BDM</span>[<span class="type">Double</span>]) = computeMode <span class="keyword">match</span> {
       <span class="keyword">case</span> <span class="type">SVDMode</span>.<span class="type">LocalARPACK</span> =&gt;
         require(k &lt; n, s<span class="string">"k must be smaller than n in local-eigs mode but got k=$k and n=$n."</span>)
         <span class="function"><span class="keyword">val</span> <span class="title">G</span> =</span> computeGramianMatrix().toBreeze.asInstanceOf[<span class="type">BDM</span>[<span class="type">Double</span>]]
         <span class="type">EigenValueDecomposition</span>.symmetricEigs(v =&gt; <span class="type">G</span> * v, n, k, tol, maxIter)
       <span class="keyword">case</span> <span class="type">SVDMode</span>.<span class="type">LocalLAPACK</span> =&gt;
         <span class="comment">// breeze (v0.10) svd latent constraint, 7 * n * n + 4 * n &lt; Int.MaxValue</span>
         require(n &lt; <span class="number">17515</span>, s<span class="string">"$n exceeds the breeze svd capability"</span>)
         <span class="function"><span class="keyword">val</span> <span class="title">G</span> =</span> computeGramianMatrix().toBreeze.asInstanceOf[<span class="type">BDM</span>[<span class="type">Double</span>]]
         <span class="function"><span class="keyword">val</span> <span class="title">brzSvd</span>.<span class="title">SVD</span>(</span>uFull: <span class="type">BDM</span>[<span class="type">Double</span>], sigmaSquaresFull: <span class="type">BDV</span>[<span class="type">Double</span>], _) = brzSvd(<span class="type">G</span>)
         (sigmaSquaresFull, uFull)
       <span class="keyword">case</span> <span class="type">SVDMode</span>.<span class="type">DistARPACK</span> =&gt;
         <span class="keyword">if</span> (rows.getStorageLevel == <span class="type">StorageLevel</span>.<span class="type">NONE</span>) {
           logWarning(<span class="string">"The input data is not directly cached, which may hurt performance if its"</span>
             + <span class="string">" parent RDDs are also uncached."</span>)
         }
         require(k &lt; n, s<span class="string">"k must be smaller than n in dist-eigs mode but got k=$k and n=$n."</span>)
         <span class="type">EigenValueDecomposition</span>.symmetricEigs(multiplyGramianMatrixBy, n, k, tol, maxIter)
         }
</code></pre><p>这里不管lapack怎么算（其实两者最后都是调用了brzSvd去计算的），ARPACK是调用了EigenValueDecomposition去计算的，并且要求一个Gramian矩阵</p>
<pre><code><span class="keyword">private</span>[mllib] <span class="function"><span class="keyword">def</span> <span class="title">multiplyGramianMatrixBy</span>(</span>v: <span class="type">BDV</span>[<span class="type">Double</span>]): <span class="type">BDV</span>[<span class="type">Double</span>] = {
     <span class="function"><span class="keyword">val</span> <span class="title">n</span> =</span> numCols().toInt
     <span class="function"><span class="keyword">val</span> <span class="title">vbr</span> =</span> rows.context.broadcast(v)
     rows.treeAggregate(<span class="type">BDV</span>.zeros[<span class="type">Double</span>](n))(
       seqOp = (<span class="type">U</span>, r) =&gt; {
         <span class="function"><span class="keyword">val</span> <span class="title">rBrz</span> =</span> r.toBreeze
         <span class="function"><span class="keyword">val</span> <span class="title">a</span> =</span> rBrz.dot(vbr.value)
         rBrz <span class="keyword">match</span> {
           <span class="comment">// use specialized axpy for better performance</span>
           <span class="keyword">case</span> _: <span class="type">BDV</span>[_] =&gt; brzAxpy(a, rBrz.asInstanceOf[<span class="type">BDV</span>[<span class="type">Double</span>]], <span class="type">U</span>)
           <span class="keyword">case</span> _: <span class="type">BSV</span>[_] =&gt; brzAxpy(a, rBrz.asInstanceOf[<span class="type">BSV</span>[<span class="type">Double</span>]], <span class="type">U</span>)
           <span class="keyword">case</span> _ =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnsupportedOperationException</span>(
             s<span class="string">"Do not support vector operation from type ${rBrz.getClass.getName}."</span>)
         }
         <span class="type">U</span>
       }, combOp = (<span class="type">U1</span>, <span class="type">U2</span>) =&gt; <span class="type">U1</span> += <span class="type">U2</span>)
   }
</code></pre><p>这个计算用到了broadcast，头一次看到将数据分发出去-_-bb。然后就在各自本地机器上计算。给定一个矩阵x，他的列向量的Gramian矩阵其实就是x’<em>x，他的行向量的Gramian矩阵就是x</em>x’。</p>
<pre><code><span class="keyword">private</span>[mllib] <span class="class"><span class="keyword">object</span> <span class="title">EigenValueDecomposition</span> {</span>
  <span class="comment">/**
   * Compute the leading k eigenvalues and eigenvectors on a symmetric square matrix using ARPACK.
   * The caller needs to ensure that the input matrix is real symmetric. This function requires
   * memory for `n*(4*k+4)` doubles.
   *
   * @param mul a function that multiplies the symmetric matrix with a DenseVector.
   * @param n dimension of the square matrix (maximum Int.MaxValue).
   * @param k number of leading eigenvalues required, 0 &lt; k &lt; n.
   * @param tol tolerance of the eigs computation.
   * @param maxIterations the maximum number of Arnoldi update iterations.
   * @return a dense vector of eigenvalues in descending order and a dense matrix of eigenvectors
   *         (columns of the matrix).
   * @note The number of computed eigenvalues might be smaller than k when some Ritz values do not
   *       satisfy the convergence criterion specified by tol (see ARPACK Users Guide, Chapter 4.6
   *       for more details). The maximum number of Arnoldi update iterations is set to 300 in this
   *       function.
   */</span>
  <span class="keyword">private</span>[mllib] <span class="function"><span class="keyword">def</span> <span class="title">symmetricEigs</span>(</span>
      mul: <span class="type">BDV</span>[<span class="type">Double</span>] =&gt; <span class="type">BDV</span>[<span class="type">Double</span>],
      n: <span class="type">Int</span>,
      k: <span class="type">Int</span>,
      tol: <span class="type">Double</span>,
      maxIterations: <span class="type">Int</span>): (<span class="type">BDV</span>[<span class="type">Double</span>], <span class="type">BDM</span>[<span class="type">Double</span>]) = {
    <span class="comment">// TODO: remove this function and use eigs in breeze when switching breeze version</span>
    require(n &gt; k, s<span class="string">"Number of required eigenvalues $k must be smaller than matrix dimension $n"</span>)

    <span class="function"><span class="keyword">val</span> <span class="title">arpack</span> =</span> <span class="type">ARPACK</span>.getInstance()

    <span class="comment">// tolerance used in stopping criterion</span>
    <span class="function"><span class="keyword">val</span> <span class="title">tolW</span> =</span> <span class="keyword">new</span> doubleW(tol)
    <span class="comment">// number of desired eigenvalues, 0 &lt; nev &lt; n</span>
    <span class="function"><span class="keyword">val</span> <span class="title">nev</span> =</span> <span class="keyword">new</span> intW(k)
    <span class="comment">// nev Lanczos vectors are generated in the first iteration</span>
    <span class="comment">// ncv-nev Lanczos vectors are generated in each subsequent iteration</span>
    <span class="comment">// ncv must be smaller than n</span>
    <span class="function"><span class="keyword">val</span> <span class="title">ncv</span> =</span> math.min(<span class="number">2</span> * k, n)

    <span class="comment">// "I" for standard eigenvalue problem, "G" for generalized eigenvalue problem</span>
    <span class="function"><span class="keyword">val</span> <span class="title">bmat</span> =</span> <span class="string">"I"</span>
    <span class="comment">// "LM" : compute the NEV largest (in magnitude) eigenvalues</span>
    <span class="function"><span class="keyword">val</span> <span class="title">which</span> =</span> <span class="string">"LM"</span>

    <span class="keyword">var</span> iparam = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](<span class="number">11</span>)
    <span class="comment">// use exact shift in each iteration</span>
    iparam(<span class="number">0</span>) = <span class="number">1</span>
    <span class="comment">// maximum number of Arnoldi update iterations, or the actual number of iterations on output</span>
    iparam(<span class="number">2</span>) = maxIterations
    <span class="comment">// Mode 1: A*x = lambda*x, A symmetric</span>
    iparam(<span class="number">6</span>) = <span class="number">1</span>

    require(n * ncv.toLong &lt;= <span class="type">Integer</span>.<span class="type">MAX_VALUE</span> &amp;&amp; ncv * (ncv.toLong + <span class="number">8</span>) &lt;= <span class="type">Integer</span>.<span class="type">MAX_VALUE</span>,
      s<span class="string">"k = $k and/or n = $n are too large to compute an eigendecomposition"</span>)

    <span class="keyword">var</span> ido = <span class="keyword">new</span> intW(<span class="number">0</span>)
    <span class="keyword">var</span> info = <span class="keyword">new</span> intW(<span class="number">0</span>)
    <span class="keyword">var</span> resid = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Double</span>](n)
    <span class="keyword">var</span> v = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Double</span>](n * ncv)
    <span class="keyword">var</span> workd = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Double</span>](n * <span class="number">3</span>)
    <span class="keyword">var</span> workl = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Double</span>](ncv * (ncv + <span class="number">8</span>))
    <span class="keyword">var</span> ipntr = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](<span class="number">11</span>)

    <span class="comment">// call ARPACK's reverse communication, first iteration with ido = 0</span>
    arpack.dsaupd(ido, bmat, n, which, nev.`<span class="function"><span class="keyword">val</span>`, <span class="title">tolW</span>, <span class="title">resid</span>, <span class="title">ncv</span>, <span class="title">v</span>, <span class="title">n</span>, <span class="title">iparam</span>, <span class="title">ipntr</span>, <span class="title">workd</span>,
</span>      workl, workl.length, info)

    <span class="function"><span class="keyword">val</span> <span class="title">w</span> =</span> <span class="type">BDV</span>(workd)

    <span class="comment">// ido = 99 : done flag in reverse communication</span>
    <span class="keyword">while</span> (ido.`<span class="function"><span class="keyword">val</span>` <span class="title">!=</span> 99) {</span>
      <span class="keyword">if</span> (ido.`<span class="function"><span class="keyword">val</span>` <span class="title">!=</span> <span class="title">-1</span> <span class="title">&amp;&amp;</span> <span class="title">ido</span>.`<span class="title">val</span>` <span class="title">!=</span> 1) {</span>
        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"ARPACK returns ido = "</span> + ido.`<span class="function"><span class="keyword">val</span>` <span class="title">+</span>
</span>            <span class="string">" This flag is not compatible with Mode 1: A*x = lambda*x, A symmetric."</span>)
      }
      <span class="comment">// multiply working vector with the matrix</span>
      <span class="function"><span class="keyword">val</span> <span class="title">inputOffset</span> =</span> ipntr(<span class="number">0</span>) - <span class="number">1</span>
      <span class="function"><span class="keyword">val</span> <span class="title">outputOffset</span> =</span> ipntr(<span class="number">1</span>) - <span class="number">1</span>
      <span class="function"><span class="keyword">val</span> <span class="title">x</span> =</span> w.slice(inputOffset, inputOffset + n)
      <span class="function"><span class="keyword">val</span> <span class="title">y</span> =</span> w.slice(outputOffset, outputOffset + n)
      y := mul(x)
      <span class="comment">// call ARPACK's reverse communication</span>
      arpack.dsaupd(ido, bmat, n, which, nev.`<span class="function"><span class="keyword">val</span>`, <span class="title">tolW</span>, <span class="title">resid</span>, <span class="title">ncv</span>, <span class="title">v</span>, <span class="title">n</span>, <span class="title">iparam</span>, <span class="title">ipntr</span>,
</span>        workd, workl, workl.length, info)
    }

    <span class="keyword">if</span> (info.`<span class="function"><span class="keyword">val</span>` <span class="title">!=</span> 0) {</span>
      info.`<span class="function"><span class="keyword">val</span>` <span class="title">match</span> {</span>
        <span class="keyword">case</span> <span class="number">1</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"ARPACK returns non-zero info = "</span> + info.`<span class="function"><span class="keyword">val</span>` <span class="title">+</span>
</span>            <span class="string">" Maximum number of iterations taken. (Refer ARPACK user guide for details)"</span>)
        <span class="keyword">case</span> <span class="number">3</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"ARPACK returns non-zero info = "</span> + info.`<span class="function"><span class="keyword">val</span>` <span class="title">+</span>
</span>            <span class="string">" No shifts could be applied. Try to increase NCV. "</span> +
            <span class="string">"(Refer ARPACK user guide for details)"</span>)
        <span class="keyword">case</span> _ =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"ARPACK returns non-zero info = "</span> + info.`<span class="function"><span class="keyword">val</span>` <span class="title">+</span>
</span>            <span class="string">" Please refer ARPACK user guide for error message."</span>)
      }
    }

    <span class="function"><span class="keyword">val</span> <span class="title">d</span> =</span> <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Double</span>](nev.`<span class="function"><span class="keyword">val</span>`)
</span>    <span class="function"><span class="keyword">val</span> <span class="title">select</span> =</span> <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Boolean</span>](ncv)
    <span class="comment">// copy the Ritz vectors</span>
    <span class="function"><span class="keyword">val</span> <span class="title">z</span> =</span> java.util.<span class="type">Arrays</span>.copyOfRange(v, <span class="number">0</span>, nev.`<span class="function"><span class="keyword">val</span>` <span class="title">*</span> <span class="title">n</span>)
</span>
    <span class="comment">// call ARPACK's post-processing for eigenvectors</span>
    arpack.dseupd(<span class="literal">true</span>, <span class="string">"A"</span>, select, d, z, n, <span class="number">0.0</span>, bmat, n, which, nev, tol, resid, ncv, v, n,
      iparam, ipntr, workd, workl, workl.length, info)

    <span class="comment">// number of computed eigenvalues, might be smaller than k</span>
    <span class="function"><span class="keyword">val</span> <span class="title">computed</span> =</span> iparam(<span class="number">4</span>)

    <span class="function"><span class="keyword">val</span> <span class="title">eigenPairs</span> =</span> java.util.<span class="type">Arrays</span>.copyOfRange(d, <span class="number">0</span>, computed).zipWithIndex.map { r =&gt;
      (r._1, java.util.<span class="type">Arrays</span>.copyOfRange(z, r._2 * n, r._2 * n + n))
    }

    <span class="comment">// sort the eigen-pairs in descending order</span>
    <span class="function"><span class="keyword">val</span> <span class="title">sortedEigenPairs</span> =</span> eigenPairs.sortBy(- _._1)

    <span class="comment">// copy eigenvectors in descending order of eigenvalues</span>
    <span class="function"><span class="keyword">val</span> <span class="title">sortedU</span> =</span> <span class="type">BDM</span>.zeros[<span class="type">Double</span>](n, computed)
    sortedEigenPairs.zipWithIndex.foreach { r =&gt;
      <span class="function"><span class="keyword">val</span> <span class="title">b</span> =</span> r._2 * n
      <span class="keyword">var</span> i = <span class="number">0</span>
      <span class="keyword">while</span> (i &lt; n) {
        sortedU.data(b + i) = r._1._2(i)
        i += <span class="number">1</span>
      }
    }

    (<span class="type">BDV</span>[<span class="type">Double</span>](sortedEigenPairs.map(_._1)), sortedU)
  }
}
</code></pre><p>最后来到关键的地方，所以其实spark内部的分布式求矩阵都是用svd的近似，不断调用Gramian方法来传递向量，用迭代的方式来求特征值和特征向量。最后返回的也是一个BDV，也就是breeze.linalg.DenseVector。我个人认为这种方法相比mpi，似乎一路上也没有checkpoint，效率可能还不一定比mpi做的高，毕竟MPI，BLAS，SCALAPACK这些库都是久经考验得工业标准了。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>端午节，群里有个人问了一个spark关于mllib扩展性的问题，不太确定，继续看代码。正好上周1.4也发布了，也把代码下下来比较了一下。<br>比较以后才知道，原来scala版本的计算和pyspark差了好多，pyspark关于linalg的计算都是直接调用numpy，也就]]>
    </summary>
    
      <category term="mllib" scheme="http://yoursite.com/tags/mllib/"/>
    
      <category term="scala" scheme="http://yoursite.com/tags/scala/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[spark 的调度和任务]]></title>
    <link href="http://yoursite.com/2015/06/09/spark-task-and-scheduler/"/>
    <id>http://yoursite.com/2015/06/09/spark-task-and-scheduler/</id>
    <published>2015-06-09T13:27:45.000Z</published>
    <updated>2015-06-09T13:53:54.000Z</updated>
    <content type="html"><![CDATA[<p>一晃快一个月过去了，这个月有点小忙做了很多项目一直都没空闲重新研究spark的代码。今天晚上下决心看完。花了一点时间追了一下代码。基本算是搞清楚了spark的调度。<br>上次说到，spark会把记录了操作的rdd提交交给调度器等待运行。那调度器具体是怎么执行计算的呢？随手提交了一个作业，有如下日志</p>
<pre><code><span class="number">15</span><span class="regexp">/06/</span><span class="number">10</span> <span class="number">03</span>:<span class="number">05</span>:<span class="number">09</span> INFO <span class="string">DAGScheduler:</span> Submitting <span class="number">1</span> missing tasks from Stage <span class="number">1</span> (MapPartitionsRDD[<span class="number">2</span>] at map at Task2a.<span class="string">scala:</span><span class="number">8</span>)
<span class="number">15</span><span class="regexp">/06/</span><span class="number">10</span> <span class="number">03</span>:<span class="number">05</span>:<span class="number">09</span> INFO <span class="string">TaskSchedulerImpl:</span> Adding task set <span class="number">1.0</span> with <span class="number">1</span> tasks
<span class="number">15</span><span class="regexp">/06/</span><span class="number">10</span> <span class="number">03</span>:<span class="number">05</span>:<span class="number">09</span> INFO <span class="string">TaskSetManager:</span> Starting task <span class="number">0.0</span> <span class="keyword">in</span> stage <span class="number">1.0</span> (TID <span class="number">1</span>, localhost, PROCESS_LOCAL, <span class="number">1348</span> bytes)
<span class="number">15</span><span class="regexp">/06/</span><span class="number">10</span> <span class="number">03</span>:<span class="number">05</span>:<span class="number">09</span> INFO <span class="string">Executor:</span> Running task <span class="number">0.0</span> <span class="keyword">in</span> stage <span class="number">1.0</span> (TID <span class="number">1</span>)
<span class="number">15</span><span class="regexp">/06/</span><span class="number">10</span> <span class="number">03</span>:<span class="number">05</span>:<span class="number">09</span> INFO <span class="string">HadoopRDD:</span> Input <span class="string">split:</span> <span class="string">file:</span><span class="regexp">/home/</span>cloudera/<span class="number">1</span>millionTweets.<span class="string">tsv:</span><span class="number">0</span>+<span class="number">28775886</span>
<span class="number">15</span><span class="regexp">/06/</span><span class="number">10</span> <span class="number">03</span>:<span class="number">05</span>:<span class="number">10</span> INFO <span class="string">BlockManager:</span> Removing broadcast <span class="number">1</span>
<span class="number">15</span><span class="regexp">/06/</span><span class="number">10</span> <span class="number">03</span>:<span class="number">05</span>:<span class="number">10</span> INFO <span class="string">BlockManager:</span> Removing block broadcast_1_piece0
<span class="number">15</span><span class="regexp">/06/</span><span class="number">10</span> <span class="number">03</span>:<span class="number">05</span>:<span class="number">10</span> INFO <span class="string">MemoryStore:</span> Block broadcast_1_piece0 of size <span class="number">2039</span> dropped from memory (free <span class="number">280032800</span>)
</code></pre><p>我们可以看到，DAGScheduler将任务提交了以后其实是TaskSetManager执行任务的</p>
<pre><code>logInfo(<span class="string">"Starting <span class="variable">%s</span> (TID <span class="variable">%d</span>, <span class="variable">%s</span>, <span class="variable">%s</span>, <span class="variable">%d</span> bytes)"</span>.<span class="keyword">format</span>(
taskName, taskId, host, taskLocality, serializedTask.limit))
sched.dagScheduler.taskStarted(task, info)
<span class="keyword">return</span> Some(new TaskDescription(taskId = taskId, attemptNumber = attemptNum, execId,
taskName, <span class="keyword">index</span>, serializedTask))
</code></pre><p>这里调用了dagScheduler的taskstarted开始任务的而最后到了Executor执行，也就是这一段</p>
<pre><code><span class="comment">// Run the actual task and measure its runtime.</span>
taskStart = System.<span class="function"><span class="title">currentTimeMillis</span><span class="params">()</span></span>
val value = task.<span class="function"><span class="title">run</span><span class="params">(taskAttemptId = taskId, attemptNumber = attemptNumber)</span></span>
val taskFinish = System.<span class="function"><span class="title">currentTimeMillis</span><span class="params">()</span></span>
</code></pre><p>这里调用的是task.run来执行</p>
<pre><code>final <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(taskAttemptId: Long, attemptNumber: Int)</span>:</span> T = {
  context = new TaskContextImpl(stageId = stageId, partitionId = partitionId,
    taskAttemptId = taskAttemptId, attemptNumber = attemptNumber, runningLocally = false)
  TaskContextHelper.setTaskContext(context)
  context.taskMetrics.setHostname(Utils.localHostName())
  taskThread = Thread.currentThread()
  <span class="keyword">if</span> (_killed) {
    kill(interruptThread = false)
  }
  <span class="keyword">try</span> {
    runTask(context)
  } <span class="keyword">finally</span> {
    context.markTaskCompleted()
    TaskContextHelper.unset()
  }
 }
</code></pre><p>这是一个抽象类的final方法，基本可以看出大致的逻辑，调用自己的runTask执行任务完事。<br>例如，对于shuffle任务有一个</p>
<pre><code><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">ShuffleMapTask</span>(</span>
  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">runTask</span>(</span>context: <span class="type">TaskContext</span>): <span class="type">MapStatus</span> = {
    <span class="comment">// Deserialize the RDD using the broadcast variable.</span>
    <span class="function"><span class="keyword">val</span> <span class="title">ser</span> =</span> <span class="type">SparkEnv</span>.get.closureSerializer.newInstance()
    <span class="function"><span class="keyword">val</span> (</span>rdd, dep) = ser.deserialize[(<span class="type">RDD</span>[_], <span class="type">ShuffleDependency</span>[_, _, _])](
      <span class="type">ByteBuffer</span>.wrap(taskBinary.value), <span class="type">Thread</span>.currentThread.getContextClassLoader)

    metrics = <span class="type">Some</span>(context.taskMetrics)
    <span class="keyword">var</span> writer: <span class="type">ShuffleWriter</span>[<span class="type">Any</span>, <span class="type">Any</span>] = <span class="literal">null</span>
    <span class="keyword">try</span> {
      <span class="function"><span class="keyword">val</span> <span class="title">manager</span> =</span> <span class="type">SparkEnv</span>.get.shuffleManager
      writer = manager.getWriter[<span class="type">Any</span>, <span class="type">Any</span>](dep.shuffleHandle, partitionId, context)
      writer.write(rdd.iterator(partition, context).asInstanceOf[<span class="type">Iterator</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">Any</span>, <span class="type">Any</span>]]])
      <span class="keyword">return</span> writer.stop(success = <span class="literal">true</span>).get
    } <span class="keyword">catch</span> {
      <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;
        <span class="keyword">try</span> {
          <span class="keyword">if</span> (writer != <span class="literal">null</span>) {
            writer.stop(success = <span class="literal">false</span>)
          }
        } <span class="keyword">catch</span> {
          <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;
            log.debug(<span class="string">"Could not stop writer"</span>, e)
        }
        <span class="keyword">throw</span> e
    }
  }
</code></pre><p>每一个Task子类都有自己的衍生的Runtask方法，取决于各自的目的。<br>终于算是找到了核心运算单元了，继续看mllib和dataframe的代码</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>一晃快一个月过去了，这个月有点小忙做了很多项目一直都没空闲重新研究spark的代码。今天晚上下决心看完。花了一点时间追了一下代码。基本算是搞清楚了spark的调度。<br>上次说到，spark会把记录了操作的rdd提交交给调度器等待运行。那调度器具体是怎么执行计算的呢？随手]]>
    </summary>
    
      <category term="scala" scheme="http://yoursite.com/tags/scala/"/>
    
      <category term="scheduler" scheme="http://yoursite.com/tags/scheduler/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="task" scheme="http://yoursite.com/tags/task/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[expore spark RDD source code]]></title>
    <link href="http://yoursite.com/2015/05/15/explore-spark-RDD-source-code/"/>
    <id>http://yoursite.com/2015/05/15/explore-spark-RDD-source-code/</id>
    <published>2015-05-15T08:14:17.000Z</published>
    <updated>2015-05-15T09:26:56.000Z</updated>
    <content type="html"><![CDATA[<p>一鼓作气再而衰三而竭。<br>开始阅读spark部分的代码，好在之前在coursera上看过一点scala虽然习题没做完但是不至于全忘。在spark里面，RDD的lazy evaluation也是通过和pyspark差不多的方法实现的，定义一个RDD类，每次操作都返回一个MapParitionsRDD，在这个MapParitionsRDD里记下前一个RDD和对应的函数。</p>
<pre><code>abstract <span class="class"><span class="keyword">class</span> <span class="title">RDD</span>[<span class="title">T</span>:</span> ClassTag](
    <span class="decorator">@transient private var _sc: SparkContext,</span>
    <span class="decorator">@transient private var deps: Seq[Dependency[_]]</span>
  ) extends Serializable <span class="keyword">with</span> Logging {

  <span class="keyword">if</span> (classOf[RDD[_]].isAssignableFrom(elementClassTag.runtimeClass)) {
    // This <span class="keyword">is</span> a warning instead of an exception <span class="keyword">in</span> order to avoid breaking user programs that
    // might have defined nested RDDs without running jobs <span class="keyword">with</span> them.
    logWarning(<span class="string">"Spark does not support nested RDDs (see SPARK-5063)"</span>)
  }

  private <span class="function"><span class="keyword">def</span> <span class="title">sc</span>:</span> SparkContext = {
    <span class="keyword">if</span> (_sc == null) {
      throw new SparkException(
        <span class="string">"RDD transformations and actions can only be invoked by the driver, not inside of other "</span> +
        <span class="string">"transformations; for example, rdd1.map(x =&gt; rdd2.values.count() * x) is invalid because "</span> +
        <span class="string">"the values transformation and count action cannot be performed inside of the rdd1.map "</span> +
        <span class="string">"transformation. For more information, see SPARK-5063."</span>)
    }
    _sc
  }


  <span class="function"><span class="keyword">def</span> <span class="title">map</span>[<span class="title">U</span>:</span> ClassTag](f: T =&gt; U): RDD[U] = {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.map(cleanF))
  }

  <span class="function"><span class="keyword">def</span> <span class="title">filter</span><span class="params">(f: T =&gt; Boolean)</span>:</span> RDD[T] = {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[T, T](
      this,
      (context, pid, iter) =&gt; iter.filter(cleanF),
      preservesPartitioning = true)
  }

  <span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsWithIndex</span>[<span class="title">U</span>:</span> ClassTag](
      f: (Int, Iterator[T]) =&gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = {
    val func = (context: TaskContext, index: Int, iter: Iterator[T]) =&gt; f(index, iter)
    new MapPartitionsRDD(this, sc.clean(func), preservesPartitioning)
  }
</code></pre><p>而MapPartitionsRDD仅仅记录下了动作和父RDD</p>
<pre><code>private[spark] class <span class="type">MapPartitionsRDD</span>[U: <span class="type">ClassTag</span>, T: <span class="type">ClassTag</span>](
    prev: <span class="type">RDD</span>[T],
    f: (<span class="type">TaskContext</span>, <span class="type">Int</span>, <span class="type">Iterator</span>[T]) =&gt; <span class="type">Iterator</span>[U],  // (<span class="type">TaskContext</span>, partition index, <span class="keyword">iterator</span>)
    preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>)
  extends <span class="type">RDD</span>[U](prev) {

  override val partitioner = <span class="keyword">if</span> (preservesPartitioning) firstParent[T].partitioner <span class="keyword">else</span> <span class="type">None</span>

  override def getPartitions: <span class="type">Array</span>[<span class="type">Partition</span>] = firstParent[T].partitions

  override def compute(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>) =
    f(context, split.index, firstParent[T].<span class="keyword">iterator</span>(split, context))
}
</code></pre><p>而只有collect和reduce这类函数是例外的，</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">collect</span><span class="params">()</span>:</span> Array[T] = {
    val results = sc.runJob(this, (iter: Iterator[T]) =&gt; iter.toArray)
    Array.concat(results: _*)
  }

<span class="function"><span class="keyword">def</span> <span class="title">reduce</span><span class="params">(f: <span class="params">(T, T)</span> =&gt; T)</span>:</span> T = {
    val cleanF = sc.clean(f)
    val reducePartition: Iterator[T] =&gt; Option[T] = iter =&gt; {
      <span class="keyword">if</span> (iter.hasNext) {
        Some(iter.reduceLeft(cleanF))
      } <span class="keyword">else</span> {
        <span class="keyword">None</span>
      }
    }
    var jobResult: Option[T] = <span class="keyword">None</span>
    val mergeResult = (index: Int, taskResult: Option[T]) =&gt; {
      <span class="keyword">if</span> (taskResult.isDefined) {
        jobResult = jobResult match {
          case Some(value) =&gt; Some(f(value, taskResult.get))
          case <span class="keyword">None</span> =&gt; taskResult
        }
      }
    }
    sc.runJob(this, reducePartition, mergeResult)
    // Get the final result out of our Option, <span class="keyword">or</span> throw an exception <span class="keyword">if</span> the RDD was empty
    jobResult.getOrElse(throw new UnsupportedOperationException(<span class="string">"empty collection"</span>))
  }
</code></pre><p>看代码可以知道collect这个动作做的时候其实是调用sc.runJob了，不再累积RDD类。而从SparkContext里的runJob里可以看到，scala shell其实是通过调用DAGScheduler.runjob让spark干活的</p>
<pre><code>def runJob[T, U: ClassTag](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) =&gt; U,
      partitions: Seq[<span class="typename">Int</span>],
      allowLocal: <span class="typename">Boolean</span>,
      resultHandler: (<span class="typename">Int</span>, U) =&gt; <span class="typename">Unit</span>) {
    <span class="keyword">if</span> (stopped) {
      <span class="keyword">throw</span> new IllegalStateException(<span class="string">"SparkContext has been shutdown"</span>)
    }
    <span class="variable"><span class="keyword">val</span> callSite</span> = getCallSite
    <span class="variable"><span class="keyword">val</span> cleanedFunc</span> = clean(func)
    logInfo(<span class="string">"Starting job: "</span> + callSite.shortForm)
    <span class="keyword">if</span> (conf.getBoolean(<span class="string">"spark.logLineage"</span>, <span class="literal">false</span>)) {
      logInfo(<span class="string">"RDD's recursive dependencies:\n"</span> + rdd.toDebugString)
    }
    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, allowLocal,
      resultHandler, localProperties.<span class="keyword">get</span>)
    progressBar.foreach(_.finishAll())
    rdd.doCheckpoint()
  }
</code></pre><p>对于dag调度器，他的runJob是用submitJob来提交的</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">runJob</span>[</span><span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](
      rdd: <span class="type">RDD</span>[<span class="type">T</span>],
      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,
      partitions: <span class="type">Seq</span>[<span class="type">Int</span>],
      callSite: <span class="type">CallSite</span>,
      allowLocal: <span class="type">Boolean</span>,
      resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>,
      properties: <span class="type">Properties</span>): <span class="type">Unit</span> = {
    <span class="function"><span class="keyword">val</span> <span class="title">start</span> =</span> <span class="type">System</span>.nanoTime
    <span class="function"><span class="keyword">val</span> <span class="title">waiter</span> =</span> submitJob(rdd, func, partitions, callSite, allowLocal, resultHandler, properties)
    waiter.awaitResult() <span class="keyword">match</span> {
      <span class="keyword">case</span> <span class="type">JobSucceeded</span> =&gt; {
        logInfo(<span class="string">"Job %d finished: %s, took %f s"</span>.format
          (waiter.jobId, callSite.shortForm, (<span class="type">System</span>.nanoTime - start) / <span class="number">1e9</span>))
      }
      <span class="keyword">case</span> <span class="type">JobFailed</span>(exception: <span class="type">Exception</span>) =&gt;
        logInfo(<span class="string">"Job %d failed: %s, took %f s"</span>.format
          (waiter.jobId, callSite.shortForm, (<span class="type">System</span>.nanoTime - start) / <span class="number">1e9</span>))
        <span class="keyword">throw</span> exception
    }
  }

  <span class="function"><span class="keyword">def</span> <span class="title">submitJob</span>[</span><span class="type">T</span>, <span class="type">U</span>](
      rdd: <span class="type">RDD</span>[<span class="type">T</span>],
      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,
      partitions: <span class="type">Seq</span>[<span class="type">Int</span>],
      callSite: <span class="type">CallSite</span>,
      allowLocal: <span class="type">Boolean</span>,
      resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>,
      properties: <span class="type">Properties</span>): <span class="type">JobWaiter</span>[<span class="type">U</span>] = {
    <span class="comment">// Check to make sure we are not launching a task on a partition that does not exist.</span>
    <span class="function"><span class="keyword">val</span> <span class="title">maxPartitions</span> =</span> rdd.partitions.length
    partitions.find(p =&gt; p &gt;= maxPartitions || p &lt; <span class="number">0</span>).foreach { p =&gt;
      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(
        <span class="string">"Attempting to access a non-existent partition: "</span> + p + <span class="string">". "</span> +
          <span class="string">"Total number of partitions: "</span> + maxPartitions)
    }

    <span class="function"><span class="keyword">val</span> <span class="title">jobId</span> =</span> nextJobId.getAndIncrement()
    <span class="keyword">if</span> (partitions.size == <span class="number">0</span>) {
      <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">JobWaiter</span>[<span class="type">U</span>](<span class="keyword">this</span>, jobId, <span class="number">0</span>, resultHandler)
    }

    assert(partitions.size &gt; <span class="number">0</span>)
    <span class="function"><span class="keyword">val</span> <span class="title">func2</span> =</span> func.asInstanceOf[(<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _]
    <span class="function"><span class="keyword">val</span> <span class="title">waiter</span> =</span> <span class="keyword">new</span> <span class="type">JobWaiter</span>(<span class="keyword">this</span>, jobId, partitions.size, resultHandler)
    eventProcessLoop.post(<span class="type">JobSubmitted</span>(
      jobId, rdd, func2, partitions.toArray, allowLocal, callSite, waiter, properties))
    waiter
  }
</code></pre><p>做了一系列安全性检查，起了一个jobwaiter等结果，然后继续调用JobSubmitted比把它提交到DAGSchedulerEventProcessLoop类里去，</p>
<pre><code><span class="keyword">private</span>[scheduler] <span class="class"><span class="keyword">class</span> <span class="title">DAGSchedulerEventProcessLoop</span>(</span>dagScheduler: <span class="type">DAGScheduler</span>)
  <span class="keyword">extends</span> <span class="type">EventLoop</span>[<span class="type">DAGSchedulerEvent</span>](<span class="string">"dag-scheduler-event-loop"</span>) <span class="keyword">with</span> <span class="type">Logging</span> {

  <span class="comment">/**
   * The main event loop of the DAG scheduler.
   */</span>
  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onReceive</span>(</span>event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = event <span class="keyword">match</span> {
    <span class="keyword">case</span> <span class="type">JobSubmitted</span>(jobId, rdd, func, partitions, allowLocal, callSite, listener, properties) =&gt;
      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, allowLocal, callSite,
        listener, properties)

...

  <span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">handleJobSubmitted</span>(</span>jobId: <span class="type">Int</span>,
      finalRDD: <span class="type">RDD</span>[_],
      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,
      partitions: <span class="type">Array</span>[<span class="type">Int</span>],
      allowLocal: <span class="type">Boolean</span>,
      callSite: <span class="type">CallSite</span>,
      listener: <span class="type">JobListener</span>,
      properties: <span class="type">Properties</span>) {
    <span class="keyword">var</span> finalStage: <span class="type">Stage</span> = <span class="literal">null</span>
    <span class="keyword">try</span> {
      <span class="comment">// New stage creation may throw an exception if, for example, jobs are run on a</span>
      <span class="comment">// HadoopRDD whose underlying HDFS files have been deleted.</span>
      finalStage = newStage(finalRDD, partitions.size, <span class="type">None</span>, jobId, callSite)
    } <span class="keyword">catch</span> {
      <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;
        logWarning(<span class="string">"Creating new stage failed due to exception - job: "</span> + jobId, e)
        listener.jobFailed(e)
        <span class="keyword">return</span>
    }
    <span class="keyword">if</span> (finalStage != <span class="literal">null</span>) {
      <span class="function"><span class="keyword">val</span> <span class="title">job</span> =</span> <span class="keyword">new</span> <span class="type">ActiveJob</span>(jobId, finalStage, func, partitions, callSite, listener, properties)
      clearCacheLocs()
      logInfo(<span class="string">"Got job %s (%s) with %d output partitions (allowLocal=%s)"</span>.format(
        job.jobId, callSite.shortForm, partitions.length, allowLocal))
      logInfo(<span class="string">"Final stage: "</span> + finalStage + <span class="string">"("</span> + finalStage.name + <span class="string">")"</span>)
      logInfo(<span class="string">"Parents of final stage: "</span> + finalStage.parents)
      logInfo(<span class="string">"Missing parents: "</span> + getMissingParentStages(finalStage))
      <span class="function"><span class="keyword">val</span> <span class="title">shouldRunLocally</span> =</span>
        localExecutionEnabled &amp;&amp; allowLocal &amp;&amp; finalStage.parents.isEmpty &amp;&amp; partitions.length == <span class="number">1</span>
      <span class="function"><span class="keyword">val</span> <span class="title">jobSubmissionTime</span> =</span> clock.getTimeMillis()
      <span class="keyword">if</span> (shouldRunLocally) {
        <span class="comment">// Compute very short actions like first() or take() with no parent stages locally.</span>
        listenerBus.post(
          <span class="type">SparkListenerJobStart</span>(job.jobId, jobSubmissionTime, <span class="type">Seq</span>.empty, properties))
        runLocally(job)
      } <span class="keyword">else</span> {
        jobIdToActiveJob(jobId) = job
        activeJobs += job
        finalStage.resultOfJob = <span class="type">Some</span>(job)
        <span class="function"><span class="keyword">val</span> <span class="title">stageIds</span> =</span> jobIdToStageIds(jobId).toArray
        <span class="function"><span class="keyword">val</span> <span class="title">stageInfos</span> =</span> stageIds.flatMap(id =&gt; stageIdToStage.get(id).map(_.latestInfo))
        listenerBus.post(
          <span class="type">SparkListenerJobStart</span>(job.jobId, jobSubmissionTime, stageInfos, properties))
        submitStage(finalStage)
      }
    }
    submitWaitingStages()
  }
</code></pre><p>包装了一大堆东西，然后扔到listenerBus里准备运行（这次是真的要运行了吧？）</p>
<pre><code><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">LiveListenerBus</span>
</span>  <span class="keyword">extends</span> <span class="type">AsynchronousListenerBus</span>[<span class="type">SparkListener</span>, <span class="type">SparkListenerEvent</span>](<span class="string">"SparkListenerBus"</span>)
  <span class="keyword">with</span> <span class="type">SparkListenerBus</span> {

  <span class="keyword">private</span> <span class="function"><span class="keyword">val</span> <span class="title">logDroppedEvent</span> =</span> <span class="keyword">new</span> <span class="type">AtomicBoolean</span>(<span class="literal">false</span>)

  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onDropEvent</span>(</span>event: <span class="type">SparkListenerEvent</span>): <span class="type">Unit</span> = {
    <span class="keyword">if</span> (logDroppedEvent.compareAndSet(<span class="literal">false</span>, <span class="literal">true</span>)) {
      <span class="comment">// Only log the following message once to avoid duplicated annoying logs.</span>
      logError(<span class="string">"Dropping SparkListenerEvent because no remaining room in event queue. "</span> +
        <span class="string">"This likely means one of the SparkListeners is too slow and cannot keep up with "</span> +
        <span class="string">"the rate at which tasks are being started by the scheduler."</span>)
    }
  }

}

<span class="keyword">private</span>[spark] <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">AsynchronousListenerBus</span>[</span><span class="type">L</span> &lt;: <span class="type">AnyRef</span>, <span class="type">E</span>](name: <span class="type">String</span>)
  <span class="keyword">extends</span> <span class="type">ListenerBus</span>[<span class="type">L</span>, <span class="type">E</span>] {
  <span class="function"><span class="keyword">def</span> <span class="title">post</span>(</span>event: <span class="type">E</span>) {
    <span class="keyword">if</span> (stopped.get) {
      <span class="comment">// Drop further events to make `listenerThread` exit ASAP</span>
      logError(s<span class="string">"$name has already stopped! Dropping event $event"</span>)
      <span class="keyword">return</span>
    }
    <span class="function"><span class="keyword">val</span> <span class="title">eventAdded</span> =</span> eventQueue.offer(event)
    <span class="keyword">if</span> (eventAdded) {
      eventLock.release()
    } <span class="keyword">else</span> {
      onDropEvent(event)
    }
  }
</code></pre><p>扔完以后开始submitStage把最后状态提交开始运行<br>所以跳到最后就是把作业提交完事，胸闷，还是没有看到RDD是怎么在不同节点间交互的代码。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>一鼓作气再而衰三而竭。<br>开始阅读spark部分的代码，好在之前在coursera上看过一点scala虽然习题没做完但是不至于全忘。在spark里面，RDD的lazy evaluation也是通过和pyspark差不多的方法实现的，定义一个RDD类，每次操作都返回一个M]]>
    </summary>
    
      <category term="RDD" scheme="http://yoursite.com/tags/RDD/"/>
    
      <category term="scala" scheme="http://yoursite.com/tags/scala/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[spark machine learning lib for python]]></title>
    <link href="http://yoursite.com/2015/05/15/spark-machine-learning-lib-for-python/"/>
    <id>http://yoursite.com/2015/05/15/spark-machine-learning-lib-for-python/</id>
    <published>2015-05-15T05:29:26.000Z</published>
    <updated>2015-05-15T05:46:33.000Z</updated>
    <content type="html"><![CDATA[<p>今天看了一下pyspark的mllib这个库。作为一个大数据计算平台，spark这点比hadoop向前走了一大步。提供了种类繁多的数据计算的模型，从随机数生成到线性代数到回归聚类。<br>粗粗的看了一下，pyspark在做这个mllib的时候实现有两种，一种是spark原生型的api调用，比如刚才提到的回归聚类，都是直接输入rdd然后不断迭代返回结果，比如</p>
<pre><code><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegressionWithSGD</span><span class="params">(object)</span>:</span>

    <span class="decorator">@classmethod</span>
    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(cls, data, iterations=<span class="number">100</span>, step=<span class="number">1.0</span>, miniBatchFraction=<span class="number">1.0</span>,
              initialWeights=None, regParam=<span class="number">0.01</span>, regType=<span class="string">"l2"</span>, intercept=False)</span>:</span>
        <span class="string">"""
        Train a logistic regression model on the given data.

        :param data:              The training data, an RDD of LabeledPoint.
        :param iterations:        The number of iterations (default: 100).
        :param step:              The step parameter used in SGD
                                  (default: 1.0).
        :param miniBatchFraction: Fraction of data to be used for each SGD
                                  iteration.
        :param initialWeights:    The initial weights (default: None).
        :param regParam:          The regularizer parameter (default: 0.01).
        :param regType:           The type of regularizer used for training
                                  our model.

                                  :Allowed values:
                                     - "l1" for using L1 regularization
                                     - "l2" for using L2 regularization
                                     - None for no regularization

                                     (default: "l2")

        :param intercept:         Boolean parameter which indicates the use
                                  or not of the augmented representation for
                                  training data (i.e. whether bias features
                                  are activated or not).
        """</span>
        <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(rdd, i)</span>:</span>
            <span class="keyword">return</span> callMLlibFunc(<span class="string">"trainLogisticRegressionModelWithSGD"</span>, rdd, int(iterations),
                                 float(step), float(miniBatchFraction), i, float(regParam), regType,
                                 bool(intercept))

        <span class="keyword">return</span> _regression_train_wrapper(train, LogisticRegressionModel, data, initialWeights)


<span class="function"><span class="keyword">def</span> <span class="title">callMLlibFunc</span><span class="params">(name, *args)</span>:</span>
    <span class="string">""" Call API in PythonMLLibAPI """</span>
    sc = SparkContext._active_spark_context
    api = getattr(sc._jvm.PythonMLLibAPI(), name)
    <span class="keyword">return</span> callJavaFunc(sc, api, *args)
</code></pre><p>第一类api全都把工作丢给了spark去做，也就是其实是跨节点并行计算。<br>第二类api就比较奇怪了，linalg里面全是这类函数，如</p>
<pre><code><span class="class"><span class="keyword">class</span> <span class="title">DenseVector</span><span class="params">(Vector)</span>:</span>
    <span class="string">"""
    A dense vector represented by a value array. We use numpy array for
    storage and arithmetics will be delegated to the underlying numpy
    array.

    def dot(self, other):
        """</span>
        Compute the dot product of two Vectors. We support
        (Numpy array, list, SparseVector, <span class="keyword">or</span> SciPy sparse)
        <span class="keyword">and</span> a target NumPy array that <span class="keyword">is</span> either <span class="number">1</span>- <span class="keyword">or</span> <span class="number">2</span>-dimensional.
        Equivalent to calling numpy.dot of the two vectors.
        <span class="keyword">if</span> type(other) == np.ndarray:
            <span class="keyword">if</span> other.ndim &gt; <span class="number">1</span>:
                <span class="keyword">assert</span> len(self) == other.shape[<span class="number">0</span>], <span class="string">"dimension mismatch"</span>
            <span class="keyword">return</span> np.dot(self.array, other)
        <span class="keyword">elif</span> _have_scipy <span class="keyword">and</span> scipy.sparse.issparse(other):
            <span class="keyword">assert</span> len(self) == other.shape[<span class="number">0</span>], <span class="string">"dimension mismatch"</span>
            <span class="keyword">return</span> other.transpose().dot(self.toArray())
        <span class="keyword">else</span>:
            <span class="keyword">assert</span> len(self) == _vector_size(other), <span class="string">"dimension mismatch"</span>
            <span class="keyword">if</span> isinstance(other, SparseVector):
                <span class="keyword">return</span> other.dot(self)
            <span class="keyword">elif</span> isinstance(other, Vector):
                <span class="keyword">return</span> np.dot(self.toArray(), other.toArray())
            <span class="keyword">else</span>:
                <span class="keyword">return</span> np.dot(self.toArray(), other)
</code></pre><p>居然全部都是丢给numpy去做，虽然numpy做这类计算的效率很高不假但这样就没有spark的跨节点并行的优势了，在计算的时候要注意。另外听说spark的运行效率首先于带宽，根据我有限的hpc经验来看，网络的延迟影响可能也会很大。而spark所在的网络最多也就是万兆网络，在做矩阵并行计算这类对网络延迟依赖很大的计算的时候可能会很慢，开发人员会不会因此考虑将不少矩阵计算本地化呢？不得而知，我很好奇下一个版本如果他们给出矩阵求逆以及对焦化或者svd是不是依然也使用本地计算。如果这个问题不解决可能对spark要称霸大数据平台有点影响。</p>
<p>好了，pyspark的代码看的差不多了，接下去就是用scala写的spark了，cross fingers</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>今天看了一下pyspark的mllib这个库。作为一个大数据计算平台，spark这点比hadoop向前走了一大步。提供了种类繁多的数据计算的模型，从随机数生成到线性代数到回归聚类。<br>粗粗的看了一下，pyspark在做这个mllib的时候实现有两种，一种是spark原生]]>
    </summary>
    
      <category term="mllib" scheme="http://yoursite.com/tags/mllib/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[pyspark dataframe的实现]]></title>
    <link href="http://yoursite.com/2015/05/14/explore-the-source-of-pyspark-dataframe/"/>
    <id>http://yoursite.com/2015/05/14/explore-the-source-of-pyspark-dataframe/</id>
    <published>2015-05-14T07:24:15.000Z</published>
    <updated>2015-05-14T07:56:38.000Z</updated>
    <content type="html"><![CDATA[<p>dataframe是spark 1.3今年新推出的东西，但其实早期叫做SchemaRDD，这还可以在源代码里看到。不过按照SPARK east summit 2015上的说法，dataframe速度很快，这得益于全部用内核实现计算。<br>随便摘一段出来就能发现，Dataframe这个类里的大多数计算都是交由self._jdf实现的。</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">count</span><span class="params">(self)</span>:</span>
    <span class="string">"""Returns the number of rows in this :class:`DataFrame`.

    &gt;&gt;&gt; df.count()
    2L
    """</span>
    <span class="keyword">return</span> self._jdf.count()

<span class="function"><span class="keyword">def</span> <span class="title">collect</span><span class="params">(self)</span>:</span>
    <span class="string">"""Returns all the records as a list of :class:`Row`.

    &gt;&gt;&gt; df.collect()
    [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]
    """</span>
    <span class="keyword">with</span> SCCallSiteSync(self._sc) <span class="keyword">as</span> css:
        port = self._sc._jvm.PythonRDD.collectAndServe(self._jdf.javaToPython().rdd())
    rs = list(_load_from_socket(port, BatchedSerializer(PickleSerializer())))
    cls = _create_cls(self.schema)
    <span class="keyword">return</span> [cls(r) <span class="keyword">for</span> r <span class="keyword">in</span> rs]

<span class="function"><span class="keyword">def</span> <span class="title">limit</span><span class="params">(self, num)</span>:</span>
    <span class="string">"""Limits the result count to the number specified.

    &gt;&gt;&gt; df.limit(1).collect()
    [Row(age=2, name=u'Alice')]
    &gt;&gt;&gt; df.limit(0).collect()
    []
    """</span>
    jdf = self._jdf.limit(num)
    <span class="keyword">return</span> DataFrame(jdf, self.sql_ctx)
</code></pre><p>而self._jdf来自类初始化传入的值，通常如果用SQLContext创建DataFrame的话都来自这段代码</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">createDataFrame</span><span class="params">(self, data, schema=None, samplingRatio=None)</span>:</span>
   ...
   <span class="keyword">if</span> isinstance(schema, (list, tuple)):
       first = rdd.first()
       <span class="keyword">if</span> <span class="keyword">not</span> isinstance(first, (list, tuple)):
           <span class="keyword">raise</span> ValueError(<span class="string">"each row in `rdd` should be list or tuple, "</span>
                            <span class="string">"but got %r"</span> % type(first))
       row_cls = Row(*schema)
       schema = self._inferSchema(rdd.map(<span class="keyword">lambda</span> r: row_cls(*r)), samplingRatio)

   <span class="comment"># take the first few rows to verify schema</span>
   rows = rdd.take(<span class="number">10</span>)
   <span class="comment"># Row() cannot been deserialized by Pyrolite</span>
   <span class="keyword">if</span> rows <span class="keyword">and</span> isinstance(rows[<span class="number">0</span>], tuple) <span class="keyword">and</span> rows[<span class="number">0</span>].__class__.__name__ == <span class="string">'Row'</span>:
       rdd = rdd.map(tuple)
       rows = rdd.take(<span class="number">10</span>)

   <span class="keyword">for</span> row <span class="keyword">in</span> rows:
       _verify_type(row, schema)

   <span class="comment"># convert python objects to sql data</span>
   converter = _python_to_sql_converter(schema)
   rdd = rdd.map(converter)

   jrdd = self._jvm.SerDeUtil.toJavaArray(rdd._to_java_object_rdd())
   df = self._ssql_ctx.applySchemaToPythonRDD(jrdd.rdd(), schema.json())
   <span class="keyword">return</span> DataFrame(df, self)
</code></pre><p>而这里的self._ssql_ctx来自于同一个类里的</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">_ssql_ctx</span><span class="params">(self)</span>:</span>
    <span class="string">"""Accessor for the JVM Spark SQL context.

    Subclasses can override this property to provide their own
    JVM Contexts.
    """</span>
    <span class="keyword">if</span> self._scala_SQLContext <span class="keyword">is</span> <span class="keyword">None</span>:
        self._scala_SQLContext = self._jvm.SQLContext(self._jsc.sc())
    <span class="keyword">return</span> self._scala_SQLContext
</code></pre><p>是对scala_SQLContext的一个封装。包括sql语句的执行也是直接调用_ssql_ctx</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">sql</span><span class="params">(self, sqlQuery)</span>:</span>
    <span class="string">"""Returns a :class:`DataFrame` representing the result of the given query.

    &gt;&gt;&gt; sqlContext.registerDataFrameAsTable(df, "table1")
    &gt;&gt;&gt; df2 = sqlContext.sql("SELECT field1 AS f1, field2 as f2 from table1")
    &gt;&gt;&gt; df2.collect()
    [Row(f1=1, f2=u'row1'), Row(f1=2, f2=u'row2'), Row(f1=3, f2=u'row3')]
    """</span>
    <span class="keyword">return</span> DataFrame(self._ssql_ctx.sql(sqlQuery), self)
</code></pre><p>而原先的SchemaRDD则不用了，不过看1.2.2的代码似乎也是调用_ssql_ctx来实现计算的，不能明白为什么1.3和1.2能性能差那么多。这还有待于阅读spark的代码来解答。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>dataframe是spark 1.3今年新推出的东西，但其实早期叫做SchemaRDD，这还可以在源代码里看到。不过按照SPARK east summit 2015上的说法，dataframe速度很快，这得益于全部用内核实现计算。<br>随便摘一段出来就能发现，Dataf]]>
    </summary>
    
      <category term="dataframe" scheme="http://yoursite.com/tags/dataframe/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[spark rdd, pipeline, lazyevaluation]]></title>
    <link href="http://yoursite.com/2015/05/14/spark-rdd-pipeline-lazyevaluation/"/>
    <id>http://yoursite.com/2015/05/14/spark-rdd-pipeline-lazyevaluation/</id>
    <published>2015-05-14T02:28:14.000Z</published>
    <updated>2015-05-14T04:44:45.000Z</updated>
    <content type="html"><![CDATA[<p>一直以来写代码不求甚解，感觉这样不好，从今天开始起读各数据框架的源代码，学习学习再学习  </p>
<p>今天看的是pyspark里lazy evaluation的处理，python和scala不同不是函数式的。那这是怎么办到的呢？  </p>
<p>首先所有的数据集在spark内部都叫做rdd，这在pyspark里也有定义：  </p>
<pre><code><span class="keyword">class</span> RDD(object):

    <span class="string">""</span>"
    A Resilient Distributed Dataset (RDD), the basic abstraction <span class="keyword">in</span> Spark.
    Represents <span class="keyword">an</span> immutable, partitioned collection of elements that can be
    operated <span class="keyword">on</span> <span class="keyword">in</span> parallel.
</code></pre><p>RDD内部实现了很多函数，有map，filter这类一个集合对一个集合的映射，也有collect，reduce这种一个集合到一个值的映射。</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">map</span><span class="params">(self, f, preservesPartitioning=False)</span>:</span>
    <span class="string">"""
    Return a new RDD by applying a function to each element of this RDD.

    &gt;&gt;&gt; rdd = sc.parallelize(["b", "a", "c"])
    &gt;&gt;&gt; sorted(rdd.map(lambda x: (x, 1)).collect())
    [('a', 1), ('b', 1), ('c', 1)]
    """</span>
    <span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(_, iterator)</span>:</span>
        <span class="keyword">return</span> imap(f, iterator)
    <span class="keyword">return</span> self.mapPartitionsWithIndex(func, preservesPartitioning)

<span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsWithIndex</span><span class="params">(self, f, preservesPartitioning=False)</span>:</span>
    <span class="string">"""
    Return a new RDD by applying a function to each partition of this RDD,
    while tracking the index of the original partition.

    &gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4], 4)
    &gt;&gt;&gt; def f(splitIndex, iterator): yield splitIndex
    &gt;&gt;&gt; rdd.mapPartitionsWithIndex(f).sum()
    6
    """</span>
    <span class="keyword">return</span> PipelinedRDD(self, f, preservesPartitioning)
</code></pre><p>对于map filter这类函数来说，他们每次操作都是产生一个叫做PipelinedRDD的对象，那这个PipelinedRDD又是干什么的呢？</p>
<pre><code><span class="class"><span class="keyword">class</span> <span class="title">PipelinedRDD</span><span class="params">(RDD)</span>:</span>

    <span class="string">"""
    Pipelined maps:

    &gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4])
    &gt;&gt;&gt; rdd.map(lambda x: 2 * x).cache().map(lambda x: 2 * x).collect()
    [4, 8, 12, 16]
    &gt;&gt;&gt; rdd.map(lambda x: 2 * x).map(lambda x: 2 * x).collect()
    [4, 8, 12, 16]

    Pipelined reduces:
    &gt;&gt;&gt; from operator import add
    &gt;&gt;&gt; rdd.map(lambda x: 2 * x).reduce(add)
    20
    &gt;&gt;&gt; rdd.flatMap(lambda x: [x, x]).reduce(add)
    20
    """</span>

    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, prev, func, preservesPartitioning=False)</span>:</span>
        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(prev, PipelinedRDD) <span class="keyword">or</span> <span class="keyword">not</span> prev._is_pipelinable():
            <span class="comment"># This transformation is the first in its stage:</span>
            self.func = func
            self.preservesPartitioning = preservesPartitioning
            self._prev_jrdd = prev._jrdd
            self._prev_jrdd_deserializer = prev._jrdd_deserializer
        <span class="keyword">else</span>:
            prev_func = prev.func

            <span class="function"><span class="keyword">def</span> <span class="title">pipeline_func</span><span class="params">(split, iterator)</span>:</span>
                <span class="keyword">return</span> func(split, prev_func(split, iterator))
            self.func = pipeline_func
            self.preservesPartitioning = \
                prev.preservesPartitioning <span class="keyword">and</span> preservesPartitioning
            self._prev_jrdd = prev._prev_jrdd  <span class="comment"># maintain the pipeline</span>
            self._prev_jrdd_deserializer = prev._prev_jrdd_deserializer
        self.is_cached = <span class="keyword">False</span>
        self.is_checkpointed = <span class="keyword">False</span>
        self.ctx = prev.ctx
        self.prev = prev
        self._jrdd_val = <span class="keyword">None</span>
        self._id = <span class="keyword">None</span>
        self._jrdd_deserializer = self.ctx.serializer
        self._bypass_serializer = <span class="keyword">False</span>
        self.partitioner = prev.partitioner <span class="keyword">if</span> self.preservesPartitioning <span class="keyword">else</span> <span class="keyword">None</span>
        self._broadcast = <span class="keyword">None</span>
</code></pre><p>我们可以看到，PipelinedRDD只是记录下当前操作但不执行所以每做一次rdd操作，只是记录下了对应的映射关系，数据集还是在原始状态。只有当使用到了reduce这类函数时才会被执行计算。</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">mean</span><span class="params">(self)</span>:</span>
    <span class="string">"""
    Compute the mean of this RDD's elements.

    &gt;&gt;&gt; sc.parallelize([1, 2, 3]).mean()
    2.0
    """</span>
    <span class="keyword">return</span> self.stats().mean()

<span class="function"><span class="keyword">def</span> <span class="title">stats</span><span class="params">(self)</span>:</span>
    <span class="string">"""
    Return a L{StatCounter} object that captures the mean, variance
    and count of the RDD's elements in one operation.
    """</span>
    <span class="function"><span class="keyword">def</span> <span class="title">redFunc</span><span class="params">(left_counter, right_counter)</span>:</span>
        <span class="keyword">return</span> left_counter.mergeStats(right_counter)

    <span class="keyword">return</span> self.mapPartitions(<span class="keyword">lambda</span> i: [StatCounter(i)]).reduce(redFunc)
</code></pre><p>这里，也是回到了PipelinedRDD，但是这次就不只保存待执行的函数了，而是通过jrdd执行</p>
<pre><code>@property
def _jrdd(self):
    <span class="keyword">if</span> self._jrdd_val:
        return self._jrdd_val
    <span class="keyword">if</span> self._bypass_serializer:
        self._jrdd_deserializer = <span class="function"><span class="title">NoOpSerializer</span><span class="params">()</span></span>

    <span class="keyword">if</span> self<span class="class">.ctx</span><span class="class">.profiler_collector</span>:
        profiler = self<span class="class">.ctx</span><span class="class">.profiler_collector</span><span class="class">.new_profiler</span>(self.ctx)
    <span class="keyword">else</span>:
        profiler = None

    command = (self<span class="class">.func</span>, profiler, self._prev_jrdd_deserializer,
               self._jrdd_deserializer)
    pickled_cmd, bvars, env, includes = _prepare_for_python_RDD(self<span class="class">.ctx</span>, command, self)
    python_rdd = self<span class="class">.ctx</span>._jvm.<span class="function"><span class="title">PythonRDD</span><span class="params">(self._prev_jrdd.rdd()</span></span>,
                                         <span class="function"><span class="title">bytearray</span><span class="params">(pickled_cmd)</span></span>,
                                         env, includes, self<span class="class">.preservesPartitioning</span>,
                                         self<span class="class">.ctx</span><span class="class">.pythonExec</span>,
                                         bvars, self<span class="class">.ctx</span>._javaAccumulator)
    self._jrdd_val = python_rdd.<span class="function"><span class="title">asJavaRDD</span><span class="params">()</span></span>

    <span class="keyword">if</span> profiler:
        self._id = self._jrdd_val.<span class="function"><span class="title">id</span><span class="params">()</span></span>
        self<span class="class">.ctx</span><span class="class">.profiler_collector</span><span class="class">.add_profiler</span>(self._id, profiler)
    return self._jrdd_val
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[<p>一直以来写代码不求甚解，感觉这样不好，从今天开始起读各数据框架的源代码，学习学习再学习  </p>
<p>今天看的是pyspark里lazy evaluation的处理，python和scala不同不是函数式的。那这是怎么办到的呢？  </p>
<p>首先所有的数据集在sp]]>
    </summary>
    
      <category term="pyspark" scheme="http://yoursite.com/tags/pyspark/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="rdd" scheme="http://yoursite.com/tags/rdd/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[comparison_between_high_performance_packages_for_python]]></title>
    <link href="http://yoursite.com/2015/05/12/comparison-between-high-performance-packages-for-python/"/>
    <id>http://yoursite.com/2015/05/12/comparison-between-high-performance-packages-for-python/</id>
    <published>2015-05-12T10:01:30.000Z</published>
    <updated>2015-05-13T09:54:22.000Z</updated>
    <content type="html"><![CDATA[<p>一直以来大家提到python的性能总是一个字慢，然后顺带提一下GIL表示自己也知道。但其实python有不少模块可以显著提高性能，而且GIL也并不总是影响性能。这里就做了一个很简单的测试。二维矩阵求和。<br>鉴于运行效率，以及内存连续性考虑，这里的矩阵都是用numpy提供的array而没有使用list。numpy的数组和list相比更接近于c fortran的数组。</p>
<pre><code>Include/listobject.h
typedef struct {
    PyObject_VAR_HEAD
    /* Vector <span class="keyword">of</span> pointers <span class="keyword">to</span> <span class="type">list</span> elements.  <span class="type">list</span>[<span class="number">0</span>] <span class="keyword">is</span> ob_item[<span class="number">0</span>], etc. */
    PyObject **ob_item;

    /* ob_item <span class="keyword">contains</span> <span class="constant">space</span> <span class="keyword">for</span> 'allocated' elements.  The <span class="type">number</span>
     * currently <span class="keyword">in</span> use <span class="keyword">is</span> ob_size.
     * Invariants:
     *     <span class="number">0</span> &lt;= ob_size &lt;= allocated
     *     len(<span class="type">list</span>) == ob_size
     *     ob_item == NULL implies ob_size == allocated == <span class="number">0</span>
     * <span class="type">list</span>.sort() temporarily sets allocated <span class="keyword">to</span> -<span class="number">1</span> <span class="keyword">to</span> detect mutations.
     *
     * Items must normally <span class="keyword">not</span> be NULL, except during construction when
     * <span class="keyword">the</span> <span class="type">list</span> <span class="keyword">is</span> <span class="keyword">not</span> yet visible outside <span class="keyword">the</span> function <span class="keyword">that</span> builds <span class="keyword">it</span>.
     */
    Py_ssize_t allocated;
} PyListObject;
</code></pre><p>拿一个包含浮点数的list来说，list里面的东西其实是PyFloatObject</p>
<pre><code>Include/floatobject.h
<span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> {</span>
    PyObject_HEAD
    <span class="built_in">double</span> ob_fval;
} PyFloatObject;
</code></pre><p>也就是一个PyObject_HEAD的头加上具体的值，看源代码可以发现PyObject_HEAD的定义是这样子的</p>
<pre><code>Include/<span class="keyword">object</span>.h
<span class="preprocessor">#<span class="keyword">define</span> PyObject_HEAD                   \</span>
    _PyObject_HEAD_EXTRA                \
    Py_ssize_t ob_refcnt;               \
    <span class="keyword">struct</span> _typeobject *ob_type;
</code></pre><p>一堆零零碎碎的东西。本身list存object的时候已经不能保证一个list内的对象是内存连续了，对象内部也掺杂着其他信息，这样子的内存排列方式对于计算其实没有多少帮助。<br>例如在做浮点计算的时候要用到的sse/avx指令集就要求2/4个double必须连续排列。<br>而对于numpy来说，事情就简单的多了</p>
<pre><code>numpy/core/include/numpy/ndarraytypes.h
typedef struct tagPyArrayObject_fields {
    PyObject_HEAD
    /<span class="keyword">*</span> Pointer to the raw data buffer <span class="keyword">*</span>/
    char <span class="keyword">*</span>data;
    /<span class="keyword">*</span> The number of dimensions, also called 'ndim' <span class="keyword">*</span>/
    int nd;
    /<span class="keyword">*</span> The size in each dimension, also called 'shape' <span class="keyword">*</span>/
    npy_intp <span class="keyword">*</span>dimensions;
    /<span class="keyword">*</span>
     <span class="keyword">*</span> Number of bytes to jump to get to the
     <span class="keyword">*</span> next element in each dimension
     <span class="keyword">*</span>/
    npy_intp <span class="keyword">*</span>strides;
    /<span class="keyword">*</span>
     <span class="keyword">*</span> This object is decref'd upon
     <span class="keyword">*</span> deletion of array. Except in the
     <span class="keyword">*</span> case of UPDATEIFCOPY which has
     <span class="keyword">*</span> special handling.
     <span class="keyword">*</span>
     <span class="keyword">*</span> For views it points to the original
     <span class="keyword">*</span> array, collapsed so no chains of
     <span class="keyword">*</span> views occur.
     <span class="keyword">*</span>
     <span class="keyword">*</span> For creation from buffer object it
     <span class="keyword">*</span> points to an object that shold be
     <span class="keyword">*</span> decref'd on deletion
     <span class="keyword">*</span>
     <span class="keyword">*</span> For UPDATEIFCOPY flag this is an
     <span class="keyword">*</span> array to-be-updated upon deletion
     <span class="keyword">*</span> of this one
     <span class="keyword">*</span>/
    PyObject <span class="keyword">*</span>base;
    /<span class="keyword">*</span> Pointer to type structure <span class="keyword">*</span>/
    PyArray_Descr <span class="keyword">*</span>descr;
    /<span class="keyword">*</span> Flags describing array -- see below <span class="keyword">*</span>/
    int flags;
    /<span class="keyword">*</span> For weak references <span class="keyword">*</span>/
    PyObject <span class="keyword">*</span>weakreflist;
} PyArrayObject_fields;
</code></pre><p>除了ndim，size这些零碎以外，核心部分的数据都是连续存放在一个char *data里的，这就能保证了浮点数据的连续存放。在做浮点运算的时候也能利用上指令集了。<br>好了说完这些我们回过头来看python的这些加速库就能知道孰优孰劣了</p>
<p>首先是比较的条件，7000×7000的矩阵，全都是随机数的求和。虚拟机<br>当然是最差的cpython</p>
<pre><code>def sum2d(arr):
    M, N = arr.shape
    <span class="literal">result</span> = <span class="number">0</span>.<span class="number">0</span>
    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="type">range</span>(M):
        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="type">range</span>(N):
            <span class="literal">result</span> += arr[i, j]
    <span class="keyword">return</span> <span class="literal">result</span>
</code></pre><p>时间是9.8秒</p>
<p>然后试试看f2py</p>
<pre><code><span class="function"><span class="keyword">subroutine</span></span> sum2dfort(x, y, n, m)
    <span class="type">implicit</span> <span class="type">none</span>
    <span class="type">integer</span>(<span class="keyword">kind</span> = <span class="number">4</span>) :: n, m
    <span class="type">real</span>(<span class="keyword">kind</span> = <span class="number">8</span>), <span class="type">dimension</span>(n, m), <span class="type">intent</span>(<span class="type">in</span>) :: x
    <span class="type">real</span>(<span class="keyword">kind</span> = <span class="number">8</span>), <span class="type">intent</span>(<span class="type">out</span>) :: y
    <span class="type">integer</span>(<span class="keyword">kind</span> = <span class="number">4</span>):: i, j
    y = <span class="number">0.0</span>
    <span class="keyword">do</span> j = <span class="number">1</span>, m
        <span class="keyword">do</span> i = <span class="number">1</span>, n
                y = y + x(i, j)
        <span class="keyword">end</span> <span class="keyword">do</span>
    <span class="keyword">end</span> <span class="keyword">do</span>
    <span class="keyword">return</span>
<span class="keyword">end</span> <span class="function"><span class="keyword">subroutine</span></span> sum2dfort
</code></pre><p>时间是5.83秒，快了很多哟。不过这里还是要吐嘈一下f2py，为啥矩阵一大就要用swap了，其他几个方法都没有这么大的内存需求量</p>
<p>然后是cython</p>
<pre><code>def sum2dcyt(arr):
    cdef <span class="type">int</span> m=arr.shape[<span class="number">0</span>]
    cdef <span class="type">int</span> n=arr.shape[<span class="number">1</span>]
    cdef double <span class="literal">result</span>=<span class="number">0</span>.<span class="number">0</span>
    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="type">range</span>(m):
        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="type">range</span>(n):
            <span class="literal">result</span> += arr[i,j]
    <span class="keyword">return</span> <span class="literal">result</span>
</code></pre><p>一定是我写的方式不对，为什么只有9秒？</p>
<p>最后是numba</p>
<pre><code><span class="keyword">from</span> numba <span class="keyword">import</span> jit
@jit
def sum2djit(arr):
    M,N = arr.shape
    <span class="literal">result</span> = <span class="number">0</span>.<span class="number">0</span>
    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="type">range</span>(M):
        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="type">range</span>(N):
            <span class="literal">result</span> += arr[i, j]
    <span class="keyword">return</span> <span class="literal">result</span>
</code></pre><p>居然到了惊人的0.22秒</p>
<p>当然还有一个numpy自带的求和是0.04秒</p>
<p>最后来一个<img src="/images/compare_python_performance.png" alt="汇总"></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>一直以来大家提到python的性能总是一个字慢，然后顺带提一下GIL表示自己也知道。但其实python有不少模块可以显著提高性能，而且GIL也并不总是影响性能。这里就做了一个很简单的测试。二维矩阵求和。<br>鉴于运行效率，以及内存连续性考虑，这里的矩阵都是用numpy提供]]>
    </summary>
    
      <category term="benchmark" scheme="http://yoursite.com/tags/benchmark/"/>
    
      <category term="cython" scheme="http://yoursite.com/tags/cython/"/>
    
      <category term="f2py" scheme="http://yoursite.com/tags/f2py/"/>
    
      <category term="numba" scheme="http://yoursite.com/tags/numba/"/>
    
      <category term="numpy" scheme="http://yoursite.com/tags/numpy/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[把博客迁到github上来了]]></title>
    <link href="http://yoursite.com/2015/05/12/%E6%8A%8A%E5%8D%9A%E5%AE%A2%E8%BF%81%E5%88%B0github%E4%B8%8A%E6%9D%A5%E4%BA%86/"/>
    <id>http://yoursite.com/2015/05/12/把博客迁到github上来了/</id>
    <published>2015-05-12T01:52:53.000Z</published>
    <updated>2015-05-12T02:03:08.000Z</updated>
    <content type="html"><![CDATA[<p>昨天建站搞了一下午，晚上又忙了一会，终于把博客从cnblogs全迁移到github上来了。这里要赞一下hexo的高效。<br>本来没打算迁移的，结果在学习hexo的过程中发现了一篇<a href="http://www.huangyunkun.com/2014/03/17/migrate_from_cnblogs_to_hexo/" target="_blank" rel="external">文章</a>就想试一下。<br>结果很沮丧的发现作者的<a href="https://github.com/htynkn/hexo-migrator-cnblogs/" target="_blank" rel="external">github</a>去年9月份以后就不更新了。<br>没办法，fork一个下来自己改，javascript一点都不懂，只好console.log一个一个点追过来。<br>最后发现cnblogs改版了以后有些标签换掉了，而且hexo从2升级到3以后hexo.util已经没有这个包了。<br>最后下了独立出来的hexo-fs替换了事。<br>没仔细测试过，不过我这里的cnblogs文章都读出来了。<br>最后欢迎大家使用我更新过的<a href="https://github.com/shiyuankun/hexo-migrator-cnblogs" target="_blank" rel="external">hexo-migrate-cnblogs</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>昨天建站搞了一下午，晚上又忙了一会，终于把博客从cnblogs全迁移到github上来了。这里要赞一下hexo的高效。<br>本来没打算迁移的，结果在学习hexo的过程中发现了一篇<a href="http://www.huangyunkun.com/2014/03/17/]]>
    </summary>
    
      <category term="cnblogs" scheme="http://yoursite.com/tags/cnblogs/"/>
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
      <category term="migrate" scheme="http://yoursite.com/tags/migrate/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[hexoblog]]></title>
    <link href="http://yoursite.com/2015/05/11/hexoblog/"/>
    <id>http://yoursite.com/2015/05/11/hexoblog/</id>
    <published>2015-05-11T08:45:35.000Z</published>
    <updated>2015-05-11T09:17:12.000Z</updated>
    <content type="html"><![CDATA[<h2 id="安装hexo3-0的一些勘误">安装hexo3.0的一些勘误</h2><p>心血来潮在github上搞了一个blog，但是注册完了做完了github推荐的步骤以后发现还离博客很远。正好看到一个知乎的<a href="http://www.zhihu.com/question/20962496" target="_blank" rel="external">帖子</a><br>推荐hexo就下来试试看。没想到还不是一般好用。但是由于hexo已经从2.0升级到3.0的，而网络上的教程基本都是针对2.0的，所以很多步骤都有改变。  </p>
<p>首先要有一个github帐号，并且建立一个同名repository，如帐号是testuser那么repository就是testuser.github.io. 当然实际做的过程中要把testuser换成自己的帐号名<br>可以看<a href="http://blog.csdn.net/renfufei/article/details/37725057" target="_blank" rel="external">教程</a>  </p>
<p>申请完了以后要在本机上安装git nodejs npm</p>
<pre><code>aptitude <span class="keyword">install</span> git nodejs nodejs-legacy npm
</code></pre><p>国内有墙，所以npm特别慢还容易断，推荐用taobao的npm镜像</p>
<pre><code>npm install -g cnpm --<span class="keyword">registry</span>=<span class="keyword">http</span>://<span class="keyword">registry</span>.npm.taobao.org
</code></pre><p>然后就是安装hexo<br>root</p>
<pre><code>cnpm install hexo-cli -g

git clone testuser<span class="class">.github</span><span class="class">.io</span>
cd testuser<span class="class">.github</span><span class="class">.io</span>
</code></pre><p>安装hexo</p>
<pre><code>cnpm <span class="operator"><span class="keyword">install</span> hexo <span class="comment">--save</span></span>
</code></pre><p>安装hexo的生成器</p>
<pre><code>cnpm <span class="operator"><span class="keyword">install</span> hexo-generator-<span class="keyword">index</span> <span class="comment">--save</span>
cnpm <span class="keyword">install</span> hexo-generator-archive <span class="comment">--save</span>
cnpm <span class="keyword">install</span> hexo-generator-category <span class="comment">--save</span>
cnpm <span class="keyword">install</span> hexo-generator-tag <span class="comment">--save</span></span>
</code></pre><p>安装hexo的server</p>
<pre><code>cnpm <span class="operator"><span class="keyword">install</span> hexo-<span class="keyword">server</span> <span class="comment">--save</span></span>
</code></pre><p>安装hexo的git部署工具</p>
<pre><code>cnpm <span class="operator"><span class="keyword">install</span> hexo-deployer-git <span class="comment">--save</span></span>
</code></pre><p>安装hexo的渲染工具和feed sitemap生成器</p>
<pre><code>cnpm <span class="operator"><span class="keyword">install</span> hexo-renderer-marked@<span class="number">0.2</span> <span class="comment">--save</span>
cnpm <span class="keyword">install</span> hexo-renderer-stylus@<span class="number">0.2</span> <span class="comment">--save</span>
cnpm <span class="keyword">install</span> hexo-generator-feed@<span class="number">1</span> <span class="comment">--save</span>
cnpm <span class="keyword">install</span> hexo-generator-sitemap@<span class="number">1</span> <span class="comment">--save</span></span>
</code></pre><p>装完这一切以后<br>就可以</p>
<pre><code>hexo init
cnpm <span class="keyword">install</span>
</code></pre><p>安装依赖包<br>注意报错，有缺的就补上<br>我这里就缺hexo-renderer-ejs</p>
<p>然后</p>
<pre><code>hexo generate
hexo <span class="keyword">server</span>
</code></pre><p>本地服务已经开好了。可以访问localhost:4000查看</p>
<p>将github帐号信息放到_config.yml</p>
<pre><code><span class="attribute">deploy</span>:
  <span class="attribute">type</span>: git
  <span class="attribute">repository</span>: git<span class="variable">@github</span>.<span class="attribute">com</span>:testuser/testuser.github.io.git
  <span class="attribute">branch</span>: master
</code></pre><p>生成网页并部署</p>
<pre><code>hexo <span class="keyword">g</span> -<span class="literal">d</span>
</code></pre><p>其他的都可以看参考网页，基本没有改动过</p>
<h2 id="参考">参考</h2><p><a href="http://zipperary.com/2013/05/28/hexo-guide-1/" target="_blank" rel="external">http://zipperary.com/2013/05/28/hexo-guide-1/</a><br><a href="http://zipperary.com/2013/05/28/hexo-guide-2/" target="_blank" rel="external">http://zipperary.com/2013/05/28/hexo-guide-2/</a><br><a href="http://zipperary.com/2013/05/29/hexo-guide-3/" target="_blank" rel="external">http://zipperary.com/2013/05/29/hexo-guide-3/</a><br><a href="http://zipperary.com/2013/05/30/hexo-guide-4/" target="_blank" rel="external">http://zipperary.com/2013/05/30/hexo-guide-4/</a><br><a href="http://zipperary.com/2013/06/02/hexo-guide-5/" target="_blank" rel="external">http://zipperary.com/2013/06/02/hexo-guide-5/</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="安装hexo3-0的一些勘误">安装hexo3.0的一些勘误</h2><p>心血来潮在github上搞了一个blog，但是注册完了做完了github推荐的步骤以后发现还离博客很远。正好看到一个知乎的<a href="http://www.zhihu.com/que]]>
    </summary>
    
      <category term="blog" scheme="http://yoursite.com/tags/blog/"/>
    
      <category term="github" scheme="http://yoursite.com/tags/github/"/>
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hello World]]></title>
    <link href="http://yoursite.com/2015/05/11/hello-world/"/>
    <id>http://yoursite.com/2015/05/11/hello-world/</id>
    <published>2015-05-11T08:07:27.000Z</published>
    <updated>2015-05-11T08:07:27.000Z</updated>
    <content type="html"><![CDATA[<p>Welcome to <a href="http://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="http://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="http://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick_Start">Quick Start</h2><h3 id="Create_a_new_post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run_server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate_static_files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy_to_remote_sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Welcome to <a href="http://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="http://hexo.io]]>
    </summary>
    
  </entry>
  
  <entry>
    <title><![CDATA[first blog]]></title>
    <link href="http://yoursite.com/2015/05/11/first-blog/"/>
    <id>http://yoursite.com/2015/05/11/first-blog/</id>
    <published>2015-05-11T07:22:16.000Z</published>
    <updated>2015-05-11T08:43:43.000Z</updated>
    <content type="html"><![CDATA[<h2 id="这是第一篇_markdown-mode_学习笔记">这是第一篇 markdown-mode 学习笔记</h2><p>用来测试markdown的效果<br>emacs中如果要配置markdown-mode<br>如果是emacs-24可以使用  </p>
<pre><code><span class="constant">M</span>-x <span class="keyword">package</span>-install
markdown-<span class="literal">mode</span>
</code></pre><p>来安装<br>debian/emacs-23也可以</p>
<pre><code>aptitude <span class="keyword">install</span> emacs-good-el
</code></pre><p>随后在.emacs或者.emacs.d/init.el中添加这些  </p>
<pre><code><span class="list">(<span class="keyword">autoload</span> <span class="quoted">'markdown-mode</span> <span class="string">"markdown-mode"</span>
   <span class="string">"Major mode for editing Markdown files"</span> <span class="literal">t</span>)</span>
<span class="list">(<span class="keyword">add-to-list</span> <span class="quoted">'auto-mode-alist</span> <span class="quoted">'(<span class="string">"\\.text\\'"</span> . markdown-mode)</span>)</span>
<span class="list">(<span class="keyword">add-to-list</span> <span class="quoted">'auto-mode-alist</span> <span class="quoted">'(<span class="string">"\\.markdown\\'"</span> . markdown-mode)</span>)</span>
<span class="list">(<span class="keyword">add-to-list</span> <span class="quoted">'auto-mode-alist</span> <span class="quoted">'(<span class="string">"\\.md\\'"</span> . markdown-mode)</span>)</span>
</code></pre><p>在Markdown模式中，可以使用  </p>
<h2 id="C-c_C-a_超链接">C-c C-a 超链接</h2><p>跟l是增加一个<a href="ilink">inline link</a> (如C-c C-a l, 下同)<br>跟L是增加一个<a href="rlink">reference link</a><br>跟u是增加一个bare url<br>跟f是增加一个[^1]footnote<br>跟w是增加一个[[wiki link]]  </p>
<h2 id="C-c_C-i_图片">C-c C-i 图片</h2><p>跟i是增加一个<img src="aa.jpg" alt="inline image"><br>跟I增加一个![reference image][aa.jpg]  </p>
<h2 id="C-c_C-s_样式">C-c C-s 样式</h2><p>跟e标记<em>italic</em><br>跟b标记</p>
<blockquote>
<p>blockquote</p>
</blockquote>
<p>跟p标记</p>
<pre><code>preformatted <span class="tag">code</span> blocks
</code></pre><h2 id="C-c_C-t_标题">C-c C-t 标题</h2><p>跟h标记自动样式标题<br>跟H自动选择标题的级别但使用下划线<br>1-6 !可以选择标题级别<br>C-c C-c 命令<br>跟m代表编译<br>跟p代表预览<br>跟e代表导出<br>跟v代表导出并用浏览器查看<br>跟c代表检查错误<br>跟n重新编号<br>跟]完成所有标题和水平规则  </p>
<h2 id="参考文献">参考文献</h2><p><a href="http://jblevins.org/projects/markdown-mode/" target="_blank" rel="external">http://jblevins.org/projects/markdown-mode/</a><br><a href="http://lutaf.com/markdown-simple-usage.htm" target="_blank" rel="external">http://lutaf.com/markdown-simple-usage.htm</a><br><a href="http://zipperary.com/2013/05/22/introduction-to-markdown/" target="_blank" rel="external">http://zipperary.com/2013/05/22/introduction-to-markdown/</a></p>
<p>[^1]: foot note</p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="这是第一篇_markdown-mode_学习笔记">这是第一篇 markdown-mode 学习笔记</h2><p>用来测试markdown的效果<br>emacs中如果要配置markdown-mode<br>如果是emacs-24可以使用  </p>
<pre>]]>
    </summary>
    
      <category term="blog" scheme="http://yoursite.com/tags/blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[在docker里部署网络服务]]></title>
    <link href="http://yoursite.com/2015/05/08/4488876/"/>
    <id>http://yoursite.com/2015/05/08/4488876/</id>
    <published>2015-05-08T13:17:00.000Z</published>
    <updated>2015-05-11T14:27:15.000Z</updated>
    <content type="html"><![CDATA[<p>之前试着玩玩docker有一阵子了，今天算是头一回正式在docker里部署网络服务。</p>
<p>本来想和lxc差不多的东西那自然是手到擒来，没想到还是改了很多。</p>
<p>第一个遇到的问题是，远程连到docker宿主机干活的时候突然断网了。一下傻掉了，以前都是连内网，从来不断的。这次连了一个反向隧道，居然断了，傻眼了。</p>
<p>再连回去，发现docker ps里还有进程，好办，docker attach这个进程，就又进去了。</p>
<p>如果没有找到进程，就只要docker commit存一下，然后再docker run拉起来继续<br><a id="more"></a></p>
<p>如果连id号也忘了，那还可以docker ps -a看一下最近用过的容器。</p>
<p>第二个遇到的问题是，部署了tornado以后起服务发现cannot bind address，想了一下，猜估计是容器里命令太少没法自动决定ip地址，手工指定了一个，可是问题又来了，怎么把内部的端口和外部的端口打通呢？</p>
<p>有几种办法，</p>
<p>第一种是讲docker的网络非容器花，—net=host将网络置为和主机一样的环境</p>
<p>第二种就是docker run -p outerip:outerport:innerport将外部的ip，端口映射到内部的端口，当然也可以用-v将外部的目录映射到内部的目录</p>
<p>&nbsp;</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>之前试着玩玩docker有一阵子了，今天算是头一回正式在docker里部署网络服务。</p>
<p>本来想和lxc差不多的东西那自然是手到擒来，没想到还是改了很多。</p>
<p>第一个遇到的问题是，远程连到docker宿主机干活的时候突然断网了。一下傻掉了，以前都是连内网，从来不断的。这次连了一个反向隧道，居然断了，傻眼了。</p>
<p>再连回去，发现docker ps里还有进程，好办，docker attach这个进程，就又进去了。</p>
<p>如果没有找到进程，就只要docker commit存一下，然后再docker run拉起来继续<br>]]>
    
    </summary>
    
  </entry>
  
  <entry>
    <title><![CDATA[初学python里的yield send next]]></title>
    <link href="http://yoursite.com/2015/04/19/4439945/"/>
    <id>http://yoursite.com/2015/04/19/4439945/</id>
    <published>2015-04-19T13:09:00.000Z</published>
    <updated>2015-05-11T14:27:15.000Z</updated>
    <content type="html"><![CDATA[<p>今天看书的时候突然看到这个想起来一直没有怎么使用过send和next试了一下</p>
<p>发现了一个诡异的问题</p>
<div class="cnblogs_code"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_primes</span><span class="params">(start)</span>:</span></span><br><span class="line">    <span class="keyword">while</span> <span class="number">1</span> :</span><br><span class="line">        <span class="keyword">if</span> is_prime(start) :</span><br><span class="line">            start = <span class="keyword">yield</span> start</span><br><span class="line">        start += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_prime</span><span class="params">(number)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> number &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">if</span> number == <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">if</span> number % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">for</span> current <span class="keyword">in</span> range(<span class="number">3</span>, int(math.sqrt(number) + <span class="number">1</span>), <span class="number">2</span>):</span><br><span class="line">            <span class="keyword">if</span> number % current == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span> :</span></span><br><span class="line">    genet = get_primes(<span class="number">100</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> genet : </span><br><span class="line">        send(i)</span><br><span class="line">        <span class="keyword">print</span> i</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span> :</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><br><br></div>

<p>告诉我start的值为None</p>
<p>然后跑去翻资料，发现自己理解错误的地方，send和next其实有一个类似的功能，就是拿一个yield的值，不同的是send是传一个进去，而next传一个None</p>
<p>所以第一次跑的时候，start=100不是质数，直接过，101是质数，get_primes执行到yield停住，等带main的循环执行next，101拿出来，再用send传101进get_primes，start赋值为101，然后start加1为102，发现不是质数再加1。yield停住。而这时候main里的循环又执行到了next（倒数第一步是send），所以next传递进去的是None，这时候start就被赋值为None了。<br><a id="more"></a></p>
<p>&nbsp;</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>今天看书的时候突然看到这个想起来一直没有怎么使用过send和next试了一下</p>
<p>发现了一个诡异的问题</p>
<div class="cnblogs_code"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_primes</span><span class="params">(start)</span>:</span></span><br><span class="line">    <span class="keyword">while</span> <span class="number">1</span> :</span><br><span class="line">        <span class="keyword">if</span> is_prime(start) :</span><br><span class="line">            start = <span class="keyword">yield</span> start</span><br><span class="line">        start += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_prime</span><span class="params">(number)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> number &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">if</span> number == <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">if</span> number % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">for</span> current <span class="keyword">in</span> range(<span class="number">3</span>, int(math.sqrt(number) + <span class="number">1</span>), <span class="number">2</span>):</span><br><span class="line">            <span class="keyword">if</span> number % current == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span> :</span></span><br><span class="line">    genet = get_primes(<span class="number">100</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> genet : </span><br><span class="line">        send(i)</span><br><span class="line">        <span class="keyword">print</span> i</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span> :</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><br><br></div>

<p>告诉我start的值为None</p>
<p>然后跑去翻资料，发现自己理解错误的地方，send和next其实有一个类似的功能，就是拿一个yield的值，不同的是send是传一个进去，而next传一个None</p>
<p>所以第一次跑的时候，start=100不是质数，直接过，101是质数，get_primes执行到yield停住，等带main的循环执行next，101拿出来，再用send传101进get_primes，start赋值为101，然后start加1为102，发现不是质数再加1。yield停住。而这时候main里的循环又执行到了next（倒数第一步是send），所以next传递进去的是None，这时候start就被赋值为None了。<br>]]>
    
    </summary>
    
  </entry>
  
  <entry>
    <title><![CDATA[opencl初体验]]></title>
    <link href="http://yoursite.com/2015/04/10/4415746/"/>
    <id>http://yoursite.com/2015/04/10/4415746/</id>
    <published>2015-04-10T13:46:00.000Z</published>
    <updated>2015-05-11T14:27:15.000Z</updated>
    <content type="html"><![CDATA[<p>总结一下，opencl的步骤差不多是这些</p>
<p>先要获取平台的id clGetPlatformIDs(nPlatforms, platform_id, &amp;num_of_platforms)</p>
<p>然后获取设备id clGetDeviceIDs(platform_id[1], CL_DEVICE_TYPE_GPU, 1, %device_id &amp;num_of_devices)</p>
<p>////这里要注意的是，如果有多个设备（如cpu和gpu）platform_id必须使用数组形式传入</p>
<p>然后是创建上下文clCreateContext（properties, 1, &amp;device_id, NULL, NULL, &amp;err)<br><a id="more"></a></p>
<p>创建命令队列clCreateCommandQueue(context, device_id, 0, &amp;err)</p>
<p>创建设备缓存clCreateBuffer(context, CL_MEM_READ_WRITE, sizeof(float) * DATA_SIZE, NULL, NULL);</p>
<p>复制数据clEnqueueWriteBuffer(command_queue, input, CL_TRUE, 0, sizeof(float)*DATA_SIZE, inputData, 0, NULL, NULL)</p>
<p>然后是根据源代码产生program代码 clCreateProgramWithSource(context, 1, (const char **)&amp;ProgramSource, NULL, &amp;err)</p>
<p>然后编译program clBuildProgram(program, 0, NULL, NULL, NULL, NULL)</p>
<p>最后产生kernel clCreateKernel(program, “test”, &amp;err);</p>
<p>设定kernel的参数 clSetKernelArg(kernel, 0, sizeof(cl_mem), &amp;input)</p>
<p>将kernel推入命令队列 clEnqueueNDRangeKernel(command_queue, kernel, 1, NULL, &amp;global, NULL, 0, NULL, NULL)</p>
<p>完成计算clFinish(command_queue)</p>
<p>读取设备缓存clEnqueueReadBuffer(command_queue, input, CL_TRUE, 0, sizeof(float) * DATA_SIZE, inputData, 0, NULL, NULL)</p>
<p>最后是清理工作</p>
<p>clReleaseMemObject(input)</p>
<p>clReleaseProgram(program)</p>
<p>clReleaseKernel(kernel)</p>
<p>clReleaseCommandQueue(command_queue)</p>
<p>clReleaseContext(context)</p>
<p>大致是这个流程，详细每个命令的参数怎么设定还要看文档</p>
<p><a href="https://www.khronos.org/registry/cl/sdk/1.1/docs/man/xhtml/" target="_blank" rel="external">https://www.khronos.org/registry/cl/sdk/1.1/docs/man/xhtml/</a></p>
<p>出了任何报错最好还是查一下头文件，很有帮助</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>总结一下，opencl的步骤差不多是这些</p>
<p>先要获取平台的id clGetPlatformIDs(nPlatforms, platform_id, &amp;num_of_platforms)</p>
<p>然后获取设备id clGetDeviceIDs(platform_id[1], CL_DEVICE_TYPE_GPU, 1, %device_id &amp;num_of_devices)</p>
<p>////这里要注意的是，如果有多个设备（如cpu和gpu）platform_id必须使用数组形式传入</p>
<p>然后是创建上下文clCreateContext（properties, 1, &amp;device_id, NULL, NULL, &amp;err)<br>]]>
    
    </summary>
    
  </entry>
  
  <entry>
    <title><![CDATA[cuda计算的分块]]></title>
    <link href="http://yoursite.com/2015/04/03/4390056/"/>
    <id>http://yoursite.com/2015/04/03/4390056/</id>
    <published>2015-04-03T06:50:00.000Z</published>
    <updated>2015-05-11T14:27:15.000Z</updated>
    <content type="html"><![CDATA[<p>gpu的架构分为streaming multiprocessors</p>
<p>每个streaming&nbsp;multiprocessors（SM）又能分步骤执行很多threads，单个SM内部能同时执行的threads叫做warp。一个warp能同时操作16个单精度浮点数/8个双精度（tesla），或者32个单精度浮点数/16个双精度浮点数（feimi）。</p>
<p>单个SM内部有local&nbsp;memory和16kb大小的share&nbsp;memory，后者是在做计算的时候要尽量利用好的东西。</p>
<p>&nbsp;</p>
<p>根据gpu的架构，做cuda计算的时候基本上是这么一个流程<br><a id="more"></a></p>
<p>先把目标矩阵分块，8<em>8或者16</em>16等，具体多大要看所用gpu的配置</p>
<p>分块完以后，gpu会把每个块调度到每个SM上去执行。SM执行的时候按照warp大小起线程，直到运算完成。</p>
<p>&nbsp;</p>
<p>资源限制：</p>
<p>每个GPU必须有16个以上的block（对应16个SM）。而每个SM最多只能有8个block（对应8个flag位）。</p>
<p>&nbsp;</p>
<p>算法举例</p>
<p>拿矩阵乘法C=A*B举例，</p>
<p>在不使用share&nbsp;memory的时候，每计算C中的一个值就需要2<em>N</em>N的数值。所以不使用BLOCK算法的时候，一个N<em>N的矩阵算一次就需要读2</em>N^4的数据，起了N^2的threads。每次要读2<em>N的数，这个操作数就是2N。带宽显然不够。这是非常慢的。在使用share memory的时候，每计算BLOCK_SIZE</em>BLOCK_SIZE中的值就需要读2<em>BLOCK_SIZE</em>BLOCK_SIZE的数据，当BLOCK_SIZE是16的时候，那就是2k，16kb的share memory能允许8个block。</p>
<p>当BLOCK_SIZE是32的时候，大小是8k，就是2个block。</p>
<p>那回顾一下刚才的数据，我们能知道，大小是32<em>32的block在16kbshare&nbsp;memory的时候对SM的利用率不高，（只能放2个block），而大小是16</em>16的block在16kb&nbsp;sharememory的时候对SM的利用率高（到了8个block，到顶了）。</p>
<p>为什么不用2个block而是8个block呢？</p>
<p>因为在从local&nbsp;memory读到share&nbsp;memory的是要时间的，gpu可以在这一个warp读取share&nbsp;memory的时候切换到别的warp&nbsp;让他们也读share&nbsp;memory。还记得吗？一个warp是16个线程，SM能同时保存8个BLOCK的状态。对于32*32这个大小的block，对于SM的调度没有利用好，所有线程全都卡在读取内存上了。</p>
<p>对于16<em>16的block，每次要读2</em>256=512个数，这256个数的操作是256<em>16</em>2=8192个浮点操作。这时带宽就足够了。</p>
<p>&nbsp;</p>
<p>对于feimi架构来说，share&nbsp;memory翻了3倍成了48kb</p>
<p>BLOCK_SIZE等于16的时候，就是24个block，BLOCK_SIZE=32的时候就是6个block。gpu读取share&nbsp;memory大概要20个cycle。一个32*32的block有1024个线程，一个warp 32个线程。这样就至少32个cycle过去了，足够前面的线程读完。</p>
<p>而对于BLOCK_SIZE等于16的时候，24个block对于一个SM来说太多了（同时只能调度8个）.</p>
<p>&nbsp;</p>
<p>ps:</p>
<p>在调试矩阵乘法的时候掉进坑里了&hellip;&hellip;给每个矩阵赋值10.0<em>i+j，两个矩阵互相乘。当矩阵比较小的时候100</em>100及以下，一点问题都没有。大到1000*1000的开始出现计算错误。找了大半天都没找出个所以然来。突然发现结果比较长，一数位数，都快7-8位了，原来是过了单精度浮点数的有效位数了。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>gpu的架构分为streaming multiprocessors</p>
<p>每个streaming&nbsp;multiprocessors（SM）又能分步骤执行很多threads，单个SM内部能同时执行的threads叫做warp。一个warp能同时操作16个单精度浮点数/8个双精度（tesla），或者32个单精度浮点数/16个双精度浮点数（feimi）。</p>
<p>单个SM内部有local&nbsp;memory和16kb大小的share&nbsp;memory，后者是在做计算的时候要尽量利用好的东西。</p>
<p>&nbsp;</p>
<p>根据gpu的架构，做cuda计算的时候基本上是这么一个流程<br>]]>
    
    </summary>
    
  </entry>
  
  <entry>
    <title><![CDATA[尽信书不如无书]]></title>
    <link href="http://yoursite.com/2015/01/27/4253426/"/>
    <id>http://yoursite.com/2015/01/27/4253426/</id>
    <published>2015-01-27T08:56:00.000Z</published>
    <updated>2015-05-11T14:27:15.000Z</updated>
    <content type="html"><![CDATA[<p>最近在看dive into python，看到介绍了另外一种字典类UserDict，挖好玩，试试看。请鄙视我这个不读书星人，之前都是用到什么stackoverflow上搜什么&hellip;&hellip;</p>
<p>开始使用到代码里去。发现在某个地方报了个错</p>
<p>for&nbsp; i in a:</p>
<p>blablabla</p>
<p>KeyError: 0<br><a id="more"></a></p>
<p>虽然遍历的方式可以用for k,v in a.items()来完成，但是这个0值是哪里来的？再把字典打出来也没有发现根源</p>
<p>好奇</p>
<p>然后放狗搜，居然userdict已经从2.6版本开始就deprecated!这是为什么，感觉碰到幽灵了。</p>
<p>换成从dict类继承同样的代码一点问题都没有。</p>
<p>&hellip;&hellip;浪费了我好多时间，悲哀，人果然还是要多读书啊！</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>最近在看dive into python，看到介绍了另外一种字典类UserDict，挖好玩，试试看。请鄙视我这个不读书星人，之前都是用到什么stackoverflow上搜什么&hellip;&hellip;</p>
<p>开始使用到代码里去。发现在某个地方报了个错</p>
<p>for&nbsp; i in a:</p>
<p>blablabla</p>
<p>KeyError: 0<br>]]>
    
    </summary>
    
  </entry>
  
  <entry>
    <title><![CDATA[docker on centos]]></title>
    <link href="http://yoursite.com/2015/01/09/4212796/"/>
    <id>http://yoursite.com/2015/01/09/4212796/</id>
    <published>2015-01-09T03:03:00.000Z</published>
    <updated>2015-05-11T14:27:15.000Z</updated>
    <content type="html"><![CDATA[<p>docker最好在centos7上安装，centos6.5上似乎麻烦不少</p>
<p>这里直接在centos7上安装，要提前装一下epel的repo</p>
<p>yum install docker&nbsp;安装就行</p>
<p>chkconfig docker on</p>
<p>service docker start<br><a id="more"></a></p>
<p>把docker服务设置为每次开机启动，然后启动docker服务</p>
<p>然后就能docker pull centos抓一个镜像下来</p>
<p>和lxc略有不一样的是，lxc&nbsp;和普通虚拟机一样有create命令，能够创建一个容器，还要自己手动配一下网络，主机名等等</p>
<p>docker直接把这些都封装起来了，你能看到的只有id以及repo名，而repo名是不在容器内体现的。</p>
<p>所以docker没有create命令，直接docker run &lt;image name&gt; &lt;command name&gt;就能运行</p>
<p>一旦对容器状态进行改变以后，比如装了一个包啊之类的，就需要docker&nbsp;push来对状态改变进行保存到另一个repo里面去（或者原repo）</p>
<p>看来，docker风靡不是没有原因的，类git的操作方法应该很容易被码农接受，并且避免了环境依赖，使得部署测试变得非常方便。</p>
<p>以后应该会大规模流行。</p>
<p>而lxc可能更适合定制化需求的paas平台吧。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>docker最好在centos7上安装，centos6.5上似乎麻烦不少</p>
<p>这里直接在centos7上安装，要提前装一下epel的repo</p>
<p>yum install docker&nbsp;安装就行</p>
<p>chkconfig docker on</p>
<p>service docker start<br>]]>
    
    </summary>
    
  </entry>
  
</feed>