<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>彼格海德的笔记空间</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="python">
<meta property="og:type" content="website">
<meta property="og:title" content="彼格海德的笔记空间">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="彼格海德的笔记空间">
<meta property="og:description" content="python">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="彼格海德的笔记空间">
<meta name="twitter:description" content="python">
  
    <link rel="alternative" href="/atom.xml" title="彼格海德的笔记空间" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-62861406-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  
<!-- Baidu Statistics -->
<script type="text/javascript">
    var _hmt = _hmt || [];
(function() {
 var hm = document.createElement("script");
 hm.src = "//hm.baidu.com/hm.js?77faa6f8da10ec42fb1c01f2947de873";
 var s = document.getElementsByTagName("script")[0]; 
 s.parentNode.insertBefore(hm, s);
 })();
</script>
<!-- End Baidu Statistics -->


</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">彼格海德的笔记空间</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">a notebook for python</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/about/index.html">About</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="q" value="site:http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-learning-rust-part-IV" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/07/20/learning-rust-part-IV/" class="article-date">
  <time datetime="2015-07-20T06:25:13.000Z" itemprop="datePublished">2015-07-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/20/learning-rust-part-IV/">rust学习笔记(四)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="基本语法">基本语法</h1><h2 id="变量绑定">变量绑定</h2><p>let 用于绑定变量，rust会自动推断类型名字，也可以自己指定类型，例如：</p>
<pre><code><span class="tag">let</span> <span class="rule"><span class="attribute">x</span>:<span class="value"> i32 = <span class="number">5</span></span></span>;
</code></pre><p>但是要注意的是所有变量初始化以后都是immutable的，需要添加mut才能改变变量的值。</p>
<pre><code><span class="keyword">let</span> mut <span class="variable">x=</span><span class="number">5</span>;
<span class="variable">x=</span><span class="number">10</span>;
</code></pre><p>不初始化的变量能过编译(会warning)，不能执行(报错)。</p>
<h2 id="函数">函数</h2><p>fn用于定义函数，基本格式如下：</p>
<pre><code><span class="function"><span class="keyword">fn</span> <span class="title">foo</span></span>(x: <span class="keyword">i32</span>, y: <span class="keyword">i32</span>)-&gt;<span class="keyword">i32</span>{
}
</code></pre><p>参数后的类型指定了参数的类型，函数定义后的-&gt;代表了返回类型，函数的最后一行的结果决定了他返回什么值，所以这个返回值要符合返回值类型，并且不需要分号(;)结尾。<br>rust也支持return返回值，但不这么推荐。</p>
<h2 id="原始类型">原始类型</h2><p>rust里的primitive类型有这几个，boolean, char, numeric(i8, i16, i32, i64, u8, u16, u32, u64, isize, usize, f32, f64)</p>
<h2 id="数组">数组</h2><pre><code><span class="keyword">let</span> <span class="variable">a=</span>[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>];
<span class="keyword">let</span> mut <span class="variable">m=</span>[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>];
</code></pre><p>数组的类型是[T;N], T代表通用类型，N是数组长度，编译时决定。</p>
<p>利用这个也能初始化数组，比如</p>
<p>let a=[0;20]</p>
<p>就是20个0组成的数组。</p>
<h2 id="切片">切片</h2><p>数组能切片，比如</p>
<pre><code><span class="keyword">let</span> <span class="variable">a=</span>[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>];
<span class="keyword">let</span> <span class="variable">middle=</span>&amp;a[<span class="number">1</span>..<span class="number">4</span>];
<span class="keyword">let</span> <span class="variable">complete=</span>&amp;a[..];
</code></pre><h2 id="字符串">字符串</h2><p>字符串类型 str</p>
<h2 id="tuple">tuple</h2><p>rust里的tuple和python的一样</p>
<pre><code><span class="keyword">let</span> <span class="variable">x=</span>(<span class="number">1</span>,<span class="string">"hello"</span>);
</code></pre><p>定义只有一个值的tuple也需要加一个,</p>
<p>取值可以用下标</p>
<pre><code><span class="keyword">let</span> <span class="variable">a=</span> x.<span class="number">0</span>;
<span class="keyword">let</span> <span class="variable">b=</span> x.<span class="number">1</span>;
</code></pre><h2 id="函数-1">函数</h2><p>函数也有类型</p>
<pre><code><span class="function"><span class="keyword">fn</span> <span class="title">foo</span></span>(x: <span class="keyword">i32</span>) -&gt; <span class="keyword">i32</span> {x}

<span class="keyword">let</span> x: <span class="function"><span class="keyword">fn</span></span>(<span class="keyword">i32</span>) -&gt; <span class="keyword">i32</span> = foo;
</code></pre><p>这种情况下x就是一个函数指针指向一个需要i32参数返回一个i32值的函数</p>
<h1 id="注释">注释</h1><p>注释之前已经说过了，这里就不说了。</p>
<h1 id="if">if</h1><p>rust里的if可以当做传统方法写</p>
<pre><code><span class="keyword">let</span> <span class="variable">x =</span> <span class="number">5</span>;

<span class="keyword">if</span> <span class="variable">x =</span>= <span class="number">5</span> {
    println!(<span class="string">"x is five!"</span>);
} <span class="keyword">else</span> {
    println!(<span class="string">"x is not five :("</span>);
}
</code></pre><p>也可以用函数式的方法写，推荐这么写：</p>
<pre><code><span class="keyword">let</span> <span class="variable">x =</span> <span class="number">5</span>;

<span class="keyword">let</span> <span class="variable">y =</span> <span class="keyword">if</span> <span class="variable">x =</span>= <span class="number">5</span> { <span class="number">10</span> } <span class="keyword">else</span> { <span class="number">15</span> };
</code></pre><p>当然这种写法就必须保证if有else 分支，并且每个分支的返回类型相同。</p>
<h2 id="for循环">for循环</h2><p>rust里的for循环也和python有点像:</p>
<pre><code><span class="keyword">for</span> <span class="keyword">var</span> <span class="keyword">in</span> expression{
    code
}
</code></pre><p>这个expression应该是一个迭代器(iterator)</p>
<h2 id="while循环">while循环</h2><p>rust也有while循环</p>
<pre><code><span class="keyword">let</span> mut x = <span class="number">5</span>; <span class="comment">// mut x: i32</span>
<span class="keyword">let</span> mut <span class="keyword">done</span> = <span class="keyword">false</span>; <span class="comment">// mut done: bool</span>

<span class="keyword">while</span> !<span class="keyword">done</span> {
    x += x - <span class="number">3</span>;

    println!(<span class="string">"{}"</span>, x);

    <span class="keyword">if</span> x % <span class="number">5</span> == <span class="number">0</span> {
        <span class="keyword">done</span> = <span class="keyword">true</span>;
    }
}
</code></pre><p>在循环里也可以用continue和break，用法同python</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/07/20/learning-rust-part-IV/" data-id="cicczjs5b000vwhv4boa1iv54" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/rust/">rust</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-learning-rust-part-III" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/07/17/learning-rust-part-III/" class="article-date">
  <time datetime="2015-07-17T07:38:28.000Z" itemprop="datePublished">2015-07-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/17/learning-rust-part-III/">rust学习笔记(三)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="迭代器(Iterator)">迭代器(Iterator)</h2><p>rust带了迭代器，而其似乎特别推崇使用迭代器(这点倒是和python很像)，例如：</p>
<pre><code><span class="keyword">let</span> nums = <span class="built_in">vec!</span>[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>];

<span class="keyword">for</span> i <span class="keyword">in</span> <span class="number">0</span>..nums.len() {
    <span class="built_in">println!</span>(<span class="string">"{}"</span>, nums[i]);
}
</code></pre><p>这种写法远比</p>
<pre><code><span class="keyword">let</span> nums = <span class="built_in">vec!</span>[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>];

<span class="keyword">for</span> num <span class="keyword">in</span> &amp;nums {
    <span class="built_in">println!</span>(<span class="string">"{}"</span>, num);
}
</code></pre><p>要差，原因有二，1,第二种更直观，2,第二种不需要做数组的边界检查。与之相关的概念还有两个，迭代器适配器以及消费者，先讲消费者</p>
<h2 id="消费者(Consumer)">消费者(Consumer)</h2><p>消费者很简单，就是一个collect()，collect消费迭代器产生的数据返回一个集合，例如</p>
<pre><code><span class="keyword">let</span> <span class="variable">one_to_one_hundred =</span> (<span class="number">1</span>..<span class="number">101</span>).collect();
</code></pre><p>但这个代码是有问题的，因为collect不知道返回什么样的集合，例如，在以下例子中</p>
<pre><code><span class="keyword">let</span> x = <span class="string">"hello"</span>.chars<span class="literal">()</span>.rev<span class="literal">()</span>.collect<span class="literal">()</span>;
</code></pre><p>编译器不知道是应该返回char组成的数组Vec<char>还是一个字符串String，为了避免这种情况的发生，就必须用指定一下类型。</char></p>
<pre><code>let x = <span class="string">"hello"</span>.chars().rev().<span class="symbol">collect:</span><span class="symbol">:&lt;Vec&lt;char&gt;&gt;</span>();
let one_to_one_hundred = (<span class="number">1</span>..<span class="number">101</span>).<span class="symbol">collect:</span><span class="symbol">:&lt;Vec&lt;u32&gt;&gt;</span>();
</code></pre><p>也可以这么写</p>
<pre><code>let <span class="symbol">x:</span> <span class="constant">Vec&lt;</span>char&gt; = <span class="string">"hello"</span>.chars().rev().<span class="symbol">collect:</span><span class="symbol">:&lt;Vec&lt;char&gt;&gt;</span>();
let one_to_one_hundred = (<span class="number">1</span>..<span class="number">101</span>).<span class="symbol">collect:</span><span class="symbol">:&lt;Vec&lt;_&gt;&gt;</span>();
</code></pre><p>find也是一种消费者，拿一个匿名函数，返回一个Option，Option里面可能有东西Some(_)，也可能什么都没有(None)，这里看着好像haskell里的Maybe，果然好东西大家都会拿来用。</p>
<pre><code><span class="keyword">let</span> greater_than_forty_two = (<span class="number">0.</span>.<span class="number">100</span>)
                             .find(|x| *x &gt; <span class="number">42</span>);

<span class="keyword">match</span> greater_than_forty_two {
    <span class="type">Some</span>(_) =&gt; println!(<span class="string">"We got some numbers!"</span>),
    <span class="type">None</span> =&gt; println!(<span class="string">"No numbers found :("</span>),
}
</code></pre><p>fold也是一种消费者，需要两个参数，第一个是累加器(accumulator)，第二个是闭包</p>
<pre><code><span class="keyword">let</span> <span class="keyword">sum</span> <span class="subst">=</span> (<span class="number">1.</span><span class="built_in">.</span><span class="number">4</span>)<span class="built_in">.</span>fold(<span class="number">0</span>, <span class="subst">|</span><span class="keyword">sum</span>, x<span class="subst">|</span> <span class="keyword">sum</span> <span class="subst">+</span> x);
</code></pre><p>累加器的初始值是0,闭包的参数是sum和x，闭包内容就是sum+x，sum的初始值是0,x从迭代器里获取值。</p>
<h2 id="迭代器适配器">迭代器适配器</h2><p>最简单的迭代器适配器就是map了，从一个迭代器出发，通过对每个迭代器施加一个函数(闭包)获取另一个迭代器。但是要注意，大多数迭代器适配器都是懒惰的，他们不会主动求值</p>
<pre><code>(<span class="number">1</span>..<span class="number">100</span>).map(|x| x + <span class="number">1</span>);

 <span class="number">18</span>:<span class="number">29</span> warning: unused <span class="literal">result</span> which must be used: <span class="keyword">iterator</span> adaptors are lazy <span class="keyword">and</span> <span class="keyword">do</span> nothing unless consumed, <span class="comment">#[warn(unused_must_use)] on by default</span>
    (<span class="number">1</span>..<span class="number">100</span>).map(|x| x + <span class="number">1</span>);
</code></pre><p>take也是一种适配器</p>
<pre><code>(<span class="number">1</span>..).<span class="function"><span class="title">take</span><span class="params">(<span class="number">5</span>)</span></span>
</code></pre><p>filter也是一种适配器</p>
<pre><code>(<span class="number">1</span>..<span class="number">100</span>).<span class="function"><span class="title">filter</span><span class="params">(|&amp;x| x % <span class="number">2</span> == <span class="number">0</span>)</span></span>
</code></pre><p>值得注意的是，所有的适配器都不会主动求值，包括有副作用的函数，比如</p>
<pre><code>(<span class="number">1.</span>.<span class="number">100</span>).<span class="keyword">map</span>(|x| <span class="built_in">println</span>!(<span class="string">"{}"</span>, x));
</code></pre><p>如果想要他们主动求值，必须套在for里面</p>
<pre><code>for i in (<span class="number">1</span>..<span class="number">100</span>).map(|x| println!(<span class="string">"{}"</span>, x)).take(<span class="number">5</span>){
    println!(<span class="string">"a"</span>)<span class="comment">;</span>
}
<span class="number">1</span>
<span class="literal">a</span>
<span class="number">2</span>
<span class="literal">a</span>
<span class="number">3</span>
<span class="literal">a</span>
<span class="number">4</span>
<span class="literal">a</span>
<span class="number">5</span>
<span class="literal">a</span>
</code></pre><p>这点和haskell很不一样(也有可能我记错了)</p>
<h2 id="并发(Concurrency)">并发(Concurrency)</h2><p>并发这一块暂时没看懂回头再来补</p>
<h2 id="错误处理">错误处理</h2><p>rust有两种错误，一种叫做failure，一种叫做panic。前者可以通过代码处理继续运行，而后者则直接停止程序运行。例如</p>
<pre><code><span class="preprocessor">#[derive(Debug)]</span>
<span class="keyword">enum</span> <span class="title">Version</span>{Version1, Version2}

<span class="preprocessor">#[derive(Debug)]</span>
<span class="keyword">enum</span> <span class="title">ParseError</span>{InvalidHeaderLength, InvalidVersion}

<span class="function"><span class="keyword">fn</span> <span class="title">parse_version</span></span>(header: &amp;[<span class="keyword">u8</span>])-&gt;Result&lt;Version, ParseError&gt;{
    <span class="keyword">if</span> header.len()&lt;<span class="number">1</span>{
        <span class="keyword">return</span> Err(ParseError::InvalidHeaderLength);
    }
    <span class="keyword">match</span> header[<span class="number">0</span>]{
        <span class="number">1</span>=&gt;Ok(Version::Version1),
        <span class="number">2</span>=&gt;Ok(Version::Version2),
        _=&gt;Err(ParseError::InvalidVersion)
    }
}

<span class="function"><span class="keyword">fn</span> <span class="title">main</span></span>(){

    <span class="keyword">let</span> version=parse_version(&amp;[<span class="number">3</span>]);
    <span class="keyword">match</span> version{
        Ok(v)=&gt;{
            <span class="built_in">println!</span>(<span class="string">"working with version: {:?}"</span>,v);
        }
        Err(e)=&gt;{
            <span class="built_in">println!</span>(<span class="string">"error parsing header: {:?}"</span>,e);
        }
    }
    <span class="keyword">let</span> version=parse_version(&amp;[<span class="number">1</span>]);
    <span class="keyword">for</span> i <span class="keyword">in</span> version
    <span class="keyword">match</span> version{
        Ok(v)=&gt;{
            <span class="built_in">println!</span>(<span class="string">"working with version: {:?}"</span>,v);
        }
        Err(e)=&gt;{
            <span class="built_in">println!</span>(<span class="string">"error parsing header: {:?}"</span>,e);
        }
    }
}
</code></pre><p>这里就是普通的failure，对于panic，我们之前已经用到的unwrap产生的就是panic，事实上unwrap的工作就是将failure处理(upgrade)为panic。例如</p>
<pre><code><span class="rule"><span class="attribute">io</span>:<span class="value">:<span class="function">stdin</span>().<span class="function">read_line</span>(&amp;mut buffer).<span class="function">unwrap</span>()</span></span>;
</code></pre><p>unwrap的意思就是如果碰到OK就通过，但碰到Err就报错退出。</p>
<p>let mut buffer = String::new();<br>let input = io::stdin().read_line(&amp;mut buffer)<br>                       .ok()<br>                       .expect(“Failed to read line”);</p>
<p>这里用ok更优美点，ok将Result转化为Option，如果发生None，那打印expect的参数退出。expect做的事情很unwrap差不多。</p>
<p>对于函数的错误还有try!这个宏，但是try!这个宏只能在返回Result类型的函数内使用，try失败就返回一个Result类型了，所以main中不能使用try，因为main的返回值不是Result。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/07/17/learning-rust-part-III/" data-id="cicczjs5d000ywhv4adypyoux" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/rust/">rust</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-learning-rust-part-II" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/07/17/learning-rust-part-II/" class="article-date">
  <time datetime="2015-07-17T03:04:24.000Z" itemprop="datePublished">2015-07-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/17/learning-rust-part-II/">rust学习笔记(二)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="多线程">多线程</h2><p>继续学习rust，今天看的是一个怎么利用rust写多线程的例子。这个问题叫做哲学家吃饭，一张圆桌子五个哲学家，每人吃饭都要先拿起左手的筷子再拿右手的筷子，吃完放回去。但如果统一拿左手的筷子，必然会发生线程死锁:每个人都拿了左手的筷子并且等右手的筷子。</p>
<pre><code><span class="keyword">use</span> std::thread;<span class="comment">//引入线程</span>
<span class="keyword">use</span> std::sync::{Mutex, Arc};<span class="comment">//引入锁和原子操作</span>

<span class="keyword">struct</span> Table{
    forks: Vec&lt;Mutex&lt;()&gt;&gt;,<span class="comment">//这是个锁组成的数组，数组的每个元素代表一双筷子</span>
}

<span class="keyword">struct</span> Philosopher{
    name: String,
    left: usize,<span class="comment">//哲学家的左右手代表了锁在数组中的相应位置</span>
    right: usize,
}

<span class="keyword">impl</span> Philosopher{
    <span class="function"><span class="keyword">fn</span> <span class="title">new</span></span>(name: &amp;<span class="keyword">str</span>, left: usize, right: usize)-&gt;Philosopher{
        Philosopher{
            name :name.to_string(),
            left: left,
            right: right,
        }
    }
    <span class="function"><span class="keyword">fn</span> <span class="title">eat</span></span>(&amp;<span class="keyword">self</span>, table: &amp;Table){
        <span class="keyword">let</span> _left = table.forks[<span class="keyword">self</span>.left].lock().unwrap();<span class="comment">//获取锁，_left前的_代表不对起不使用进行报警</span>
        <span class="keyword">let</span> _right=table.forks[<span class="keyword">self</span>.right].lock().unwrap();<span class="comment">//unwrap代表当获取锁失败时无视失败，并且将failure变成panic!</span>
        <span class="built_in">println!</span>(<span class="string">"{} is eating."</span>,<span class="keyword">self</span>.name);
        thread::sleep_ms(<span class="number">1000</span>);
        <span class="built_in">println!</span>(<span class="string">"{} is done eating."</span>, <span class="keyword">self</span>.name);
    }
}

<span class="function"><span class="keyword">fn</span> <span class="title">main</span></span>() {
    <span class="keyword">let</span> table = Arc::new(Table {forks: <span class="built_in">vec!</span>[
            Mutex::new(()),
            Mutex::new(()),
            Mutex::new(()),
            Mutex::new(()),
            Mutex::new(()),
            ]});<span class="comment">//(Table {...})是建立了一个Table对象，由于我们没有给Table绑一个new函数所以需要这种方法新建对象。</span>
            <span class="comment">//Arc(Atomic reference count)代表这个Table是支持原子引用计数的，每在一个线程中分享，就加一，这个线程退出就减一。</span>

    <span class="keyword">let</span> philosophers=<span class="built_in">vec!</span>[
        Philosopher::new(<span class="string">"Baruch Spinoza"</span>,<span class="number">0</span>,<span class="number">1</span>),
        Philosopher::new(<span class="string">"Gilles Deleuze"</span>,<span class="number">1</span>,<span class="number">2</span>),
        Philosopher::new(<span class="string">"Karl Marx"</span>,<span class="number">2</span>,<span class="number">3</span>),
        Philosopher::new(<span class="string">"Friedrich Nietzsche"</span>,<span class="number">3</span>,<span class="number">4</span>),
        Philosopher::new(<span class="string">"Michel Foucault"</span>,<span class="number">0</span>,<span class="number">4</span>),<span class="comment">//防止死锁的小技巧，先获取0再获取4</span>
    ];
    <span class="keyword">let</span> handles: Vec&lt;_&gt;=philosophers.into_iter().map(|p|{<span class="comment">//into_iter将数组转换为迭代器(iterator)</span>
            <span class="keyword">let</span> table = table.clone();<span class="comment">//这里将原始引用复制一份</span>
            thread::spawn(move||{<span class="comment">//起一个线程执行对应的p.eat, move用于维持闭包，获得eat的返回值</span>
                p.eat(&amp;table);
                })}).collect();
    <span class="keyword">for</span> h <span class="keyword">in</span> handles{
        h.join().unwrap();<span class="comment">//等每个线程结束</span>
    }
}
</code></pre><h2 id="FFI">FFI</h2><p>rust还很好的支持了FFI(foreign function interface)，能够在别的语言中调用rust写的库，例如<br>新建一个项目</p>
<pre><code><span class="variable">$ </span>cargo new embed
<span class="variable">$ </span>cd embed
</code></pre><p>里面填这些代码</p>
<pre><code><span class="keyword">use</span> std::thread;

<span class="preprocessor">#[no_mangle]</span><span class="comment">//产生外部能辨认的symbol</span>
<span class="keyword">pub</span> <span class="keyword">extern</span> <span class="function"><span class="keyword">fn</span> <span class="title">process</span></span>(){<span class="comment">//pub 和extern代表将process函数公开，外部可调用</span>
    <span class="keyword">let</span> handles: Vec&lt;_&gt;=(<span class="number">0</span>..<span class="number">10</span>).map(|_|{
        thread::spawn(||{
            <span class="keyword">let</span> <span class="keyword">mut</span> _x=<span class="number">0</span>;
            <span class="keyword">for</span> _ <span class="keyword">in</span> (<span class="number">0</span>..<span class="number">5_000_001</span>){
                _x+=<span class="number">1</span>
            }
        })
    }).collect();
    <span class="keyword">for</span> h <span class="keyword">in</span> handles{
        h.join().ok().expect(<span class="string">"Could not join a thread!"</span>);
    }
}
</code></pre><p>更改配置文件</p>
<pre><code>vi Cargo.toml
[lib]
name = <span class="string">"embed"</span>
<span class="keyword">crate</span>-<span class="keyword">type</span> = [<span class="string">"dylib"</span>]<span class="comment">//标准动态链接库，不然默认是rlib</span>
</code></pre><p>编译</p>
<pre><code>$ cargo build <span class="comment">--release</span>
</code></pre><p>python中调用</p>
<pre><code>from ctypes import cdll
lib = cdll.<span class="function"><span class="title">LoadLibrary</span><span class="params">(<span class="string">"target/release/libembed.so"</span>)</span></span>
lib.<span class="function"><span class="title">process</span><span class="params">()</span></span>
<span class="function"><span class="title">print</span><span class="params">(<span class="string">"done!"</span>)</span></span>
</code></pre><h2 id="测试">测试</h2><p>大致看了一下rust里的测试部分，感觉做的还很贴心方便，和python里的unittest不一样的是，rust直接将测试整合了进来，一共三种测试：和每个function一起的单元测试，整合（integration）测试，文档测试。</p>
<pre><code><span class="variable">$ </span>cargo new adder
<span class="variable">$ </span>cd adder
</code></pre><p>打开src/lib.rs</p>
<pre><code><span class="comment">//! The `adder` crate provides functions that add numbers to other numbers.</span>
<span class="comment">//!</span>
<span class="comment">//! # Examples</span>
<span class="comment">//!</span>
<span class="comment">//! <figure class="highlight erlang-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//<span class="exclamation_mark">!</span> <span class="function_or_atom">assert_eq</span><span class="exclamation_mark">!</span>(<span class="number">4</span>, <span class="function_or_atom">adder</span>::<span class="function_or_atom">add_two</span>(<span class="number">2</span>));</span><br><span class="line">//<span class="exclamation_mark">!</span></span><br></pre></td></tr></table></figure></span>


<span class="comment">/// This function adds two to its argument.</span>
<span class="comment">///</span>
<span class="comment">/// # Examples</span>
<span class="comment">///</span>
<span class="comment">/// <figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/// use adder::add_two;</span></span><br><span class="line"><span class="comment">///</span></span><br><span class="line"><span class="comment">/// assert_eq!(4, add_two(2));</span></span><br><span class="line"><span class="comment">///</span></span><br></pre></td></tr></table></figure></span>

<span class="keyword">pub</span> <span class="function"><span class="keyword">fn</span> <span class="title">add_two</span></span>(a:<span class="keyword">i32</span>) -&gt; <span class="keyword">i32</span>{
    a+<span class="number">2</span>
}

<span class="preprocessor">#[cfg(test)]</span>
<span class="keyword">mod</span> tests {
    <span class="keyword">use</span> super::*;

    <span class="preprocessor">#[test]</span>
    <span class="function"><span class="keyword">fn</span> <span class="title">it_works</span></span>() {  
        <span class="built_in">assert_eq!</span>(<span class="number">4</span>, add_two(<span class="number">2</span>));
    }
}
</code></pre><p>斜杠开头的那几行是文档测试，测试内容用<code>xxxx</code>包围起来，支持markdown。///开头的是函数级别的测试，//!是模块级别的测试。#[cfg(test)]那里是单元测试。可以很方便的写一些辅助函数帮助测试，在非cargo test的时候这一部分代码不会编译。还有一种就是另建一个tests目录，tests/lib.rs，做整合测试。</p>
<pre><code><span class="keyword">extern</span> <span class="keyword">crate</span> adder;

<span class="preprocessor">#[test]</span>
<span class="function"><span class="keyword">fn</span> <span class="title">it_works</span></span>(){
    <span class="built_in">assert_eq!</span>(<span class="number">4</span>, adder::add_two(<span class="number">2</span>));
}
</code></pre><h2 id="条件编译">条件编译</h2><p>rust也支持条件编译，例如</p>
<pre><code>#[<span class="function"><span class="title">cfg</span><span class="params">(foo)</span></span>]

#[<span class="function"><span class="title">cfg</span><span class="params">(bar = <span class="string">"baz"</span>)</span></span>]

#[<span class="function"><span class="title">cfg</span><span class="params">(any(unix, windows)</span></span>)]

#[<span class="function"><span class="title">cfg</span><span class="params">(all(unix, target_pointer_width = <span class="string">"32"</span>)</span></span>)]

#[<span class="function"><span class="title">cfg</span><span class="params">(not(foo)</span></span>)]
</code></pre><p>在函数定义的上一行写下这些字段，能帮助cargo决定这个函数是不是要在当前条件下编译。<br>在Cargo.toml中</p>
<pre><code>[features]
<span class="preprocessor"># no features by default</span>
<span class="default"><span class="keyword">default</span> = []</span>
</code></pre><h2 id="文档">文档</h2><p>刚才已经提到过，需要用///来注释文档，但需要注意的是，文档需要写成这样</p>
<pre><code><span class="dartdoc"><span class="markdown">/// The <span class="code">`Option`</span> type. See [<span class="link_label">the module level documentation</span>](<span class="link_url">../</span>) for more.</span></span>
<span class="keyword">enum</span> Option&lt;T&gt; {
    <span class="dartdoc"><span class="markdown">/// No value</span></span>
    None,
    <span class="dartdoc"><span class="markdown">/// Some value <span class="code">`T`</span></span></span>
    Some(T),
}
</code></pre><p>而不是这样</p>
<pre><code><span class="dartdoc"><span class="markdown">/// The <span class="code">`Option`</span> type. See [<span class="link_label">the module level documentation</span>](<span class="link_url">../</span>) for more.</span></span>
<span class="keyword">enum</span> Option&lt;T&gt; {
    None, <span class="dartdoc"><span class="markdown">/// No value</span></span>
    Some(T), <span class="dartdoc"><span class="markdown">/// Some value <span class="code">`T`</span></span></span>
}
</code></pre><p>使用</p>
<pre><code><span class="title">cargo</span> doc
</code></pre><p>生成文档</p>
<p>大致了解了一下，接下去继续看详细的语法</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/07/17/learning-rust-part-II/" data-id="cicczjs5e0010whv4qn169om5" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/rust/">rust</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-learning-rust-part-I" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/07/16/learning-rust-part-I/" class="article-date">
  <time datetime="2015-07-16T01:49:38.000Z" itemprop="datePublished">2015-07-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/16/learning-rust-part-I/">rust学习笔记(一)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>rust今年来发布了1.0版本，很火，一直想学可惜一直太懒，最近有空正好学习一下，写一点笔记。<br>初次关注rust的时候还是1.0版本，等到现在想学了，已经是1.1版本了……所以我是有多懒惰啊。  </p>
<h2 id="安装">安装</h2><pre><code>$ curl -<span class="keyword">sf</span> -L http<span class="variable">s:</span>//static.rust-lang.org/rustup.<span class="keyword">sh</span> | <span class="keyword">sh</span>
</code></pre><h2 id="卸载">卸载</h2><pre><code>$ sudo <span class="regexp">/usr/</span>local<span class="regexp">/lib/</span>rustlib<span class="regexp">/uninstall.sh</span>
</code></pre><p>rust自带一个叫做cargo的项目管理器，可以很方便的新建、编译、运行项目</p>
<h2 id="新建">新建</h2><pre><code><span class="variable">$ </span>cargo new hello_world --bin
<span class="variable">$ </span>cd hello_world
</code></pre><h2 id="编译">编译</h2><pre><code><span class="variable">$ </span>cargo build
</code></pre><h2 id="运行">运行</h2><pre><code>$ cargo <span class="command">run</span>
</code></pre><p>新建一个项目以后，这个项目的目录下会有一个Cargo.toml的文件，对项目内容做了一些必要的描述，例如，版本号，名字。rust和c++/c一样，需要编译运行。所以导入一些依赖库需要在Cargo.toml中指明，例如</p>
<pre><code><span class="title">[dependencies]</span>

<span class="setting">rand=<span class="value"><span class="string">"0.3.0"</span></span></span>
</code></pre><p>cargo会自动去github上寻找这个包在编译时链接。以下贴一个官方的例子简要介绍rust的语法</p>
<pre><code><span class="keyword">extern</span> <span class="keyword">crate</span> rand; <span class="comment">//指明rand是一个外部包，crate在rust中即包</span>

<span class="keyword">use</span> std::io; <span class="comment">//指明需要使用的库</span>
<span class="keyword">use</span> std::cmp::Ordering;
<span class="keyword">use</span> rand::Rng;

<span class="function"><span class="keyword">fn</span> <span class="title">main</span></span>() { <span class="comment">//定义一个main函数，fn是定义函数的关键字</span>
    <span class="built_in">println!</span>(<span class="string">"Guess the number!"</span>);<span class="comment">//println!是宏</span>

    <span class="keyword">let</span> secret_number = rand::thread_rng().gen_range(<span class="number">1</span>, <span class="number">101</span>);<span class="comment">//let是一个绑定，将secret_number绑定到随机数上，rust里默认的变量是不可变的</span>

    <span class="keyword">loop</span> {<span class="comment">//loop代表一个循环</span>
        <span class="built_in">println!</span>(<span class="string">"Please input your guess."</span>);

        <span class="keyword">let</span> <span class="keyword">mut</span> guess = String::new();<span class="comment">//mut代表将guess设定为mutable(可变的)</span>

        io::stdin().read_line(&amp;<span class="keyword">mut</span> guess)<span class="comment">//这里&amp;guess代表的是guess的reference，默认是不可变的，但mut指定为可变（我在想是不是其实就是读了一个字符串得到一个指向这个字符串的地址A，然后直接将A复制到guess这个指针所在的地址，相当于将guess指向了另一个字符串。）</span>
            .ok()<span class="comment">//io会返回io::Result，必须写代码处理，不然编译器会鄙视你，这里ok代表io成功运行什么代码</span>
            .expect(<span class="string">"failed to read line"</span>);<span class="comment">//ok也会返回一个需要处理的值需要expect处理，如果失败就打印这句话并退出</span>

        <span class="keyword">let</span> guess: <span class="keyword">u32</span> = <span class="keyword">match</span> guess.trim().parse() {<span class="comment">//字符转换成数字，并对结果进行模式匹配</span>
            Ok(num) =&gt; num,<span class="comment">//如果成功，就返回数值</span>
            Err(_) =&gt; <span class="keyword">continue</span>,<span class="comment">//如果失败就继续，不出错</span>
        };

        <span class="built_in">println!</span>(<span class="string">"You guessed: {}"</span>, guess);

        <span class="keyword">match</span> guess.cmp(&amp;secret_number) {<span class="comment">//guess和secret_number进行比较，并且进行模式匹配</span>
            Ordering::Less    =&gt; <span class="built_in">println!</span>(<span class="string">"Too small!"</span>),
            Ordering::Greater =&gt; <span class="built_in">println!</span>(<span class="string">"Too big!"</span>),
            Ordering::Equal   =&gt; {
                <span class="built_in">println!</span>(<span class="string">"You win!"</span>);
                <span class="keyword">break</span>;<span class="comment">//退出循环</span>
            }
        }
    }
}
</code></pre><p>这是官方教程的第一个例子。来一个复杂一点的</p>
<pre><code><span class="function"><span class="keyword">fn</span> <span class="title">main</span></span>() {
    <span class="keyword">let</span> <span class="keyword">mut</span> x = <span class="built_in">vec!</span>[<span class="string">"Hello"</span>, <span class="string">"world"</span>];

    <span class="keyword">let</span> y = &amp;x[<span class="number">0</span>];

    x.push(<span class="string">"foo"</span>);
}
</code></pre><p>这段代码会报错</p>
<pre><code>src/main.<span class="string">rs:</span><span class="number">6</span>:<span class="number">5</span>: <span class="number">6</span>:<span class="number">6</span> <span class="string">error:</span> cannot borrow `x` <span class="keyword">as</span> mutable because it is also borrowed <span class="keyword">as</span> immutable
src/main.<span class="string">rs:</span><span class="number">6</span>     x.push(<span class="string">"foo"</span>);
                  ^
src/main.<span class="string">rs:</span><span class="number">4</span>:<span class="number">14</span>: <span class="number">4</span>:<span class="number">15</span> <span class="string">note:</span> previous borrow of `x` occurs here; the immutable borrow prevents subsequent moves or mutable borrows of `x` until the borrow ends
src/main.<span class="string">rs:</span><span class="number">4</span>     let y = &amp;x[<span class="number">0</span>];
                           ^
src/main.<span class="string">rs:</span><span class="number">7</span>:<span class="number">2</span>: <span class="number">7</span>:<span class="number">2</span> <span class="string">note:</span> previous borrow ends here
src/main.<span class="string">rs:</span><span class="number">1</span> fn main() {
...
src/main.<span class="string">rs:</span><span class="number">7</span> }
</code></pre><p>大意就是 x的第一个元素被immutable的引用了一次，所以x就不能被更改，如果我们把他的引用改为mutable呢？</p>
<pre><code>src/main.<span class="string">rs:</span><span class="number">6</span>:<span class="number">5</span>: <span class="number">6</span>:<span class="number">6</span> <span class="string">error:</span> cannot borrow `x` <span class="keyword">as</span> mutable more than once at a time
src/main.<span class="string">rs:</span><span class="number">6</span>     x.push(<span class="string">"foo"</span>);
                  ^
src/main.<span class="string">rs:</span><span class="number">4</span>:<span class="number">19</span>: <span class="number">4</span>:<span class="number">20</span> <span class="string">note:</span> previous borrow of `x` occurs here; the mutable borrow prevents subsequent moves, borrows, or modification of `x` until the borrow ends
src/main.<span class="string">rs:</span><span class="number">4</span>     let y = &amp; mut x[<span class="number">0</span>];
                                ^
src/main.<span class="string">rs:</span><span class="number">7</span>:<span class="number">2</span>: <span class="number">7</span>:<span class="number">2</span> <span class="string">note:</span> previous borrow ends here
src/main.<span class="string">rs:</span><span class="number">1</span> fn main() {
...
src/main.<span class="string">rs:</span><span class="number">7</span> }
</code></pre><p>又报错了，说x不能被多次mutable引用<br>所以rust通过控制引用的可变和不可变以及可变应用次数来达到不需要gc也能自动释放占用内存。要想达到刚才的效果，只能这么做。</p>
<pre><code><span class="function"><span class="keyword">fn</span> <span class="title">main</span></span>() {
    <span class="keyword">let</span> <span class="keyword">mut</span> x = <span class="built_in">vec!</span>[<span class="string">"Hello"</span>, <span class="string">"world"</span>];

    <span class="keyword">let</span> y = x[<span class="number">0</span>].clone();

    x.push(<span class="string">"foo"</span>);
}

或者

<span class="function"><span class="keyword">fn</span> <span class="title">main</span></span>() {
    <span class="keyword">let</span> <span class="keyword">mut</span> x = <span class="built_in">vec!</span>[<span class="string">"Hello"</span>, <span class="string">"world"</span>];

    {
        <span class="keyword">let</span> y = &amp;x[<span class="number">0</span>];
    }

    x.push(<span class="string">"foo"</span>);
}
</code></pre><p>前者给y做了一个x的备份，这样很自然的就将同一内存的操作分开了。后者在一个scope内新建了一个引用，离开这个scope这个引用就消失了，所以也能对x进行可变操作。<br>先写到这里，继续看书明天更新</p>
<h2 id="参考">参考</h2><p><a href="https://doc.rust-lang.org/stable/book/hello-cargo.html" target="_blank" rel="external">https://doc.rust-lang.org/stable/book/hello-cargo.html</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/07/16/learning-rust-part-I/" data-id="cicczjs5h0012whv4nw12x8sj" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/rust/">rust</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-explore-spark-distributed-mllib" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/06/21/explore-spark-distributed-mllib/" class="article-date">
  <time datetime="2015-06-21T04:26:50.000Z" itemprop="datePublished">2015-06-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/06/21/explore-spark-distributed-mllib/">spark的mllib到底是不是分布式的</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>端午节，群里有个人问了一个spark关于mllib扩展性的问题，不太确定，继续看代码。正好上周1.4也发布了，也把代码下下来比较了一下。<br>比较以后才知道，原来scala版本的计算和pyspark差了好多，pyspark关于linalg的计算都是直接调用numpy，也就是非并行的。例如linalg.py里随处可见的np.dot</p>
<pre><code>assert <span class="function"><span class="title">len</span><span class="params">(self)</span></span> == other<span class="class">.shape</span>[<span class="number">0</span>], <span class="string">"dimension mismatch"</span>
             return other.<span class="function"><span class="title">transpose</span><span class="params">()</span></span>.<span class="function"><span class="title">dot</span><span class="params">(self.toArray()</span></span>)
         <span class="keyword">else</span>:
             assert <span class="function"><span class="title">len</span><span class="params">(self)</span></span> == _vector_size(other), <span class="string">"dimension mismatch"</span>
             <span class="keyword">if</span> <span class="function"><span class="title">isinstance</span><span class="params">(other, SparseVector)</span></span>:
                 return other.<span class="function"><span class="title">dot</span><span class="params">(self)</span></span>
             elif <span class="function"><span class="title">isinstance</span><span class="params">(other, Vector)</span></span>:
                 return np.<span class="function"><span class="title">dot</span><span class="params">(self.toArray()</span></span>, other.<span class="function"><span class="title">toArray</span><span class="params">()</span></span>)
             <span class="keyword">else</span>:
                 return np.<span class="function"><span class="title">dot</span><span class="params">(self.toArray()</span></span>, other)
</code></pre><p>但scala版本的linalg多了一个distributed的目录，下面就定义了好几种分布式矩阵</p>
<pre><code>:~/spark-<span class="number">1.4</span>.<span class="number">0</span>$ ls mllib/src/<span class="keyword">main</span>/scala/org/apache/spark/mllib/linalg/distributed/*
mllib/src/<span class="keyword">main</span>/scala/org/apache/spark/mllib/linalg/distributed/<span class="constant">Block</span>Matrix.scala
mllib/src/<span class="keyword">main</span>/scala/org/apache/spark/mllib/linalg/distributed/<span class="constant">Coordinate</span>Matrix.scala
mllib/src/<span class="keyword">main</span>/scala/org/apache/spark/mllib/linalg/distributed/<span class="constant">Distributed</span>Matrix.scala
mllib/src/<span class="keyword">main</span>/scala/org/apache/spark/mllib/linalg/distributed/<span class="constant">Indexed</span>RowMatrix.scala
mllib/src/<span class="keyword">main</span>/scala/org/apache/spark/mllib/linalg/distributed/<span class="constant">Row</span>Matrix.scala
</code></pre><p>除此以外还多了SVD，PCA一些常见矩阵运算。当然，在linalg中的BLAS.scala里面，依旧是调用了本地的blas库来计算矩阵乘法</p>
<pre><code>private <span class="function"><span class="keyword">def</span> <span class="title">dot</span><span class="params">(x: DenseVector, y: DenseVector)</span>:</span> Double = {
  val n = x.size
  f2jBLAS.ddot(n, x.values, <span class="number">1</span>, y.values, <span class="number">1</span>)
}     
</code></pre><p>这里仅限于非稀疏矩阵，spark中所有的稀疏矩阵都是自己写的没有调用额外的库</p>
<pre><code><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">dot</span>(</span>x: <span class="type">SparseVector</span>, y: <span class="type">SparseVector</span>): <span class="type">Double</span> = {
     <span class="function"><span class="keyword">val</span> <span class="title">xValues</span> =</span> x.values
     <span class="function"><span class="keyword">val</span> <span class="title">xIndices</span> =</span> x.indices
     <span class="function"><span class="keyword">val</span> <span class="title">yValues</span> =</span> y.values
     <span class="function"><span class="keyword">val</span> <span class="title">yIndices</span> =</span> y.indices
     <span class="function"><span class="keyword">val</span> <span class="title">nnzx</span> =</span> xIndices.size
     <span class="function"><span class="keyword">val</span> <span class="title">nnzy</span> =</span> yIndices.size

     <span class="keyword">var</span> kx = <span class="number">0</span>
     <span class="keyword">var</span> ky = <span class="number">0</span>
     <span class="keyword">var</span> sum = <span class="number">0.0</span>
     <span class="comment">// y catching x</span>
     <span class="keyword">while</span> (kx &lt; nnzx &amp;&amp; ky &lt; nnzy) {
       <span class="function"><span class="keyword">val</span> <span class="title">ix</span> =</span> xIndices(kx)
       <span class="keyword">while</span> (ky &lt; nnzy &amp;&amp; yIndices(ky) &lt; ix) {
         ky += <span class="number">1</span>
       }
       <span class="keyword">if</span> (ky &lt; nnzy &amp;&amp; yIndices(ky) == ix) {
         sum += xValues(kx) * yValues(ky)
         ky += <span class="number">1</span>
       }
       kx += <span class="number">1</span>
     }
     sum
   }
</code></pre><p>矩阵乘法也是一样处理，都是牵涉到稀疏矩阵计算的就用自己写的方法</p>
<pre><code>private def gemm(
       alpha: Double,
       A: DenseMatrix,
       B: DenseMatrix,
       beta: Double,
       C: DenseMatrix): <span class="variable">Unit =</span> {
     val <span class="variable">tAstr =</span> <span class="keyword">if</span> (A.isTransposed) <span class="string">"T"</span> <span class="keyword">else</span> <span class="string">"N"</span>
     val <span class="variable">tBstr =</span> <span class="keyword">if</span> (B.isTransposed) <span class="string">"T"</span> <span class="keyword">else</span> <span class="string">"N"</span>
     val <span class="variable">lda =</span> <span class="keyword">if</span> (!A.isTransposed) A.numRows <span class="keyword">else</span> A.numCols
     val <span class="variable">ldb =</span> <span class="keyword">if</span> (!B.isTransposed) B.numRows <span class="keyword">else</span> B.numCols

     require(A.<span class="variable">numCols =</span>= B.numRows,
       s<span class="string">"The columns of A don't match the rows of B. A: <span class="subst">${A.numCols}</span>, B: <span class="subst">${B.numRows}</span>"</span>)
     require(A.<span class="variable">numRows =</span>= C.numRows,
       s<span class="string">"The rows of C don't match the rows of A. C: <span class="subst">${C.numRows}</span>, A: <span class="subst">${A.numRows}</span>"</span>)
     require(B.<span class="variable">numCols =</span>= C.numCols,
       s<span class="string">"The columns of C don't match the columns of B. C: <span class="subst">${C.numCols}</span>, A: <span class="subst">${B.numCols}</span>"</span>)
     nativeBLAS.dgemm(tAstr, tBstr, A.numRows, B.numCols, A.numCols, alpha, A.values, lda,
       B.values, ldb, beta, C.values, C.numRows)
   }
</code></pre><p>然后，我们回到spark的分布式矩阵，分布式矩阵的好处是直接利用rdd就能将对应的值转化为矩阵，例如这样</p>
<pre><code>val rows = sc.textFile(args(<span class="number">0</span>)).<span class="built_in">map</span> { <span class="built_in">line</span> =&gt;
       val values = <span class="built_in">line</span>.<span class="built_in">split</span>(<span class="string">' '</span>).<span class="built_in">map</span>(_.toDouble)
       Vectors.dense(values)
     }
     val mat = <span class="keyword">new</span> RowMatrix(rows)
</code></pre><p>这样，mat就是一个分布式的RowMatrix了，所做的就是赋值一个rdd。对于做SVD这类计算来说，RowMatrix定义了三种方法LocalARPACK，LocalLAPACK和DistARPACK，根据m，n的大小来自动决定到底用哪个计算</p>
<pre><code><span class="keyword">if</span> (n &lt; <span class="number">100</span> || (k &gt; n / <span class="number">2</span> &amp;&amp; n &lt;= <span class="number">15000</span>)) {
          <span class="regexp">//</span> If n <span class="keyword">is</span> small <span class="keyword">or</span> k <span class="keyword">is</span> large compared <span class="reserved">with</span> n, we better compute the Gramian matrix first
          <span class="regexp">//</span> <span class="keyword">and</span> <span class="keyword">then</span> compute its eigenvalues locally, instead <span class="keyword">of</span> making multiple passes.
          <span class="keyword">if</span> (k &lt; n / <span class="number">3</span>) {
            SVDMode.LocalARPACK
          } <span class="keyword">else</span> {
            SVDMode.LocalLAPACK
          }
        } <span class="keyword">else</span> {
          <span class="regexp">//</span> If k <span class="keyword">is</span> small compared <span class="reserved">with</span> n, we use ARPACK <span class="reserved">with</span> distributed multiplication.
          SVDMode.DistARPACK
        }
</code></pre><p>基本上矩阵尺寸不大的时候就用local的方法来计算了，似乎作者认为ARPACK在计算方阵的时候效率不如lapack。所以当k&gt;n/3的时候选择了lapack。选完了以后就开始计算对角化矩阵</p>
<pre><code><span class="function"><span class="keyword">val</span> (</span>sigmaSquares: <span class="type">BDV</span>[<span class="type">Double</span>], u: <span class="type">BDM</span>[<span class="type">Double</span>]) = computeMode <span class="keyword">match</span> {
       <span class="keyword">case</span> <span class="type">SVDMode</span>.<span class="type">LocalARPACK</span> =&gt;
         require(k &lt; n, s<span class="string">"k must be smaller than n in local-eigs mode but got k=$k and n=$n."</span>)
         <span class="function"><span class="keyword">val</span> <span class="title">G</span> =</span> computeGramianMatrix().toBreeze.asInstanceOf[<span class="type">BDM</span>[<span class="type">Double</span>]]
         <span class="type">EigenValueDecomposition</span>.symmetricEigs(v =&gt; <span class="type">G</span> * v, n, k, tol, maxIter)
       <span class="keyword">case</span> <span class="type">SVDMode</span>.<span class="type">LocalLAPACK</span> =&gt;
         <span class="comment">// breeze (v0.10) svd latent constraint, 7 * n * n + 4 * n &lt; Int.MaxValue</span>
         require(n &lt; <span class="number">17515</span>, s<span class="string">"$n exceeds the breeze svd capability"</span>)
         <span class="function"><span class="keyword">val</span> <span class="title">G</span> =</span> computeGramianMatrix().toBreeze.asInstanceOf[<span class="type">BDM</span>[<span class="type">Double</span>]]
         <span class="function"><span class="keyword">val</span> <span class="title">brzSvd</span>.<span class="title">SVD</span>(</span>uFull: <span class="type">BDM</span>[<span class="type">Double</span>], sigmaSquaresFull: <span class="type">BDV</span>[<span class="type">Double</span>], _) = brzSvd(<span class="type">G</span>)
         (sigmaSquaresFull, uFull)
       <span class="keyword">case</span> <span class="type">SVDMode</span>.<span class="type">DistARPACK</span> =&gt;
         <span class="keyword">if</span> (rows.getStorageLevel == <span class="type">StorageLevel</span>.<span class="type">NONE</span>) {
           logWarning(<span class="string">"The input data is not directly cached, which may hurt performance if its"</span>
             + <span class="string">" parent RDDs are also uncached."</span>)
         }
         require(k &lt; n, s<span class="string">"k must be smaller than n in dist-eigs mode but got k=$k and n=$n."</span>)
         <span class="type">EigenValueDecomposition</span>.symmetricEigs(multiplyGramianMatrixBy, n, k, tol, maxIter)
         }
</code></pre><p>这里不管lapack怎么算（其实两者最后都是调用了brzSvd去计算的），ARPACK是调用了EigenValueDecomposition去计算的，并且要求一个Gramian矩阵</p>
<pre><code><span class="keyword">private</span>[mllib] <span class="function"><span class="keyword">def</span> <span class="title">multiplyGramianMatrixBy</span>(</span>v: <span class="type">BDV</span>[<span class="type">Double</span>]): <span class="type">BDV</span>[<span class="type">Double</span>] = {
     <span class="function"><span class="keyword">val</span> <span class="title">n</span> =</span> numCols().toInt
     <span class="function"><span class="keyword">val</span> <span class="title">vbr</span> =</span> rows.context.broadcast(v)
     rows.treeAggregate(<span class="type">BDV</span>.zeros[<span class="type">Double</span>](n))(
       seqOp = (<span class="type">U</span>, r) =&gt; {
         <span class="function"><span class="keyword">val</span> <span class="title">rBrz</span> =</span> r.toBreeze
         <span class="function"><span class="keyword">val</span> <span class="title">a</span> =</span> rBrz.dot(vbr.value)
         rBrz <span class="keyword">match</span> {
           <span class="comment">// use specialized axpy for better performance</span>
           <span class="keyword">case</span> _: <span class="type">BDV</span>[_] =&gt; brzAxpy(a, rBrz.asInstanceOf[<span class="type">BDV</span>[<span class="type">Double</span>]], <span class="type">U</span>)
           <span class="keyword">case</span> _: <span class="type">BSV</span>[_] =&gt; brzAxpy(a, rBrz.asInstanceOf[<span class="type">BSV</span>[<span class="type">Double</span>]], <span class="type">U</span>)
           <span class="keyword">case</span> _ =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnsupportedOperationException</span>(
             s<span class="string">"Do not support vector operation from type ${rBrz.getClass.getName}."</span>)
         }
         <span class="type">U</span>
       }, combOp = (<span class="type">U1</span>, <span class="type">U2</span>) =&gt; <span class="type">U1</span> += <span class="type">U2</span>)
   }
</code></pre><p>这个计算用到了broadcast，头一次看到将数据分发出去-_-bb。然后就在各自本地机器上计算。给定一个矩阵x，他的列向量的Gramian矩阵其实就是x’<em>x，他的行向量的Gramian矩阵就是x</em>x’。</p>
<pre><code><span class="keyword">private</span>[mllib] <span class="class"><span class="keyword">object</span> <span class="title">EigenValueDecomposition</span> {</span>
  <span class="comment">/**
   * Compute the leading k eigenvalues and eigenvectors on a symmetric square matrix using ARPACK.
   * The caller needs to ensure that the input matrix is real symmetric. This function requires
   * memory for `n*(4*k+4)` doubles.
   *
   * @param mul a function that multiplies the symmetric matrix with a DenseVector.
   * @param n dimension of the square matrix (maximum Int.MaxValue).
   * @param k number of leading eigenvalues required, 0 &lt; k &lt; n.
   * @param tol tolerance of the eigs computation.
   * @param maxIterations the maximum number of Arnoldi update iterations.
   * @return a dense vector of eigenvalues in descending order and a dense matrix of eigenvectors
   *         (columns of the matrix).
   * @note The number of computed eigenvalues might be smaller than k when some Ritz values do not
   *       satisfy the convergence criterion specified by tol (see ARPACK Users Guide, Chapter 4.6
   *       for more details). The maximum number of Arnoldi update iterations is set to 300 in this
   *       function.
   */</span>
  <span class="keyword">private</span>[mllib] <span class="function"><span class="keyword">def</span> <span class="title">symmetricEigs</span>(</span>
      mul: <span class="type">BDV</span>[<span class="type">Double</span>] =&gt; <span class="type">BDV</span>[<span class="type">Double</span>],
      n: <span class="type">Int</span>,
      k: <span class="type">Int</span>,
      tol: <span class="type">Double</span>,
      maxIterations: <span class="type">Int</span>): (<span class="type">BDV</span>[<span class="type">Double</span>], <span class="type">BDM</span>[<span class="type">Double</span>]) = {
    <span class="comment">// TODO: remove this function and use eigs in breeze when switching breeze version</span>
    require(n &gt; k, s<span class="string">"Number of required eigenvalues $k must be smaller than matrix dimension $n"</span>)

    <span class="function"><span class="keyword">val</span> <span class="title">arpack</span> =</span> <span class="type">ARPACK</span>.getInstance()

    <span class="comment">// tolerance used in stopping criterion</span>
    <span class="function"><span class="keyword">val</span> <span class="title">tolW</span> =</span> <span class="keyword">new</span> doubleW(tol)
    <span class="comment">// number of desired eigenvalues, 0 &lt; nev &lt; n</span>
    <span class="function"><span class="keyword">val</span> <span class="title">nev</span> =</span> <span class="keyword">new</span> intW(k)
    <span class="comment">// nev Lanczos vectors are generated in the first iteration</span>
    <span class="comment">// ncv-nev Lanczos vectors are generated in each subsequent iteration</span>
    <span class="comment">// ncv must be smaller than n</span>
    <span class="function"><span class="keyword">val</span> <span class="title">ncv</span> =</span> math.min(<span class="number">2</span> * k, n)

    <span class="comment">// "I" for standard eigenvalue problem, "G" for generalized eigenvalue problem</span>
    <span class="function"><span class="keyword">val</span> <span class="title">bmat</span> =</span> <span class="string">"I"</span>
    <span class="comment">// "LM" : compute the NEV largest (in magnitude) eigenvalues</span>
    <span class="function"><span class="keyword">val</span> <span class="title">which</span> =</span> <span class="string">"LM"</span>

    <span class="keyword">var</span> iparam = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](<span class="number">11</span>)
    <span class="comment">// use exact shift in each iteration</span>
    iparam(<span class="number">0</span>) = <span class="number">1</span>
    <span class="comment">// maximum number of Arnoldi update iterations, or the actual number of iterations on output</span>
    iparam(<span class="number">2</span>) = maxIterations
    <span class="comment">// Mode 1: A*x = lambda*x, A symmetric</span>
    iparam(<span class="number">6</span>) = <span class="number">1</span>

    require(n * ncv.toLong &lt;= <span class="type">Integer</span>.<span class="type">MAX_VALUE</span> &amp;&amp; ncv * (ncv.toLong + <span class="number">8</span>) &lt;= <span class="type">Integer</span>.<span class="type">MAX_VALUE</span>,
      s<span class="string">"k = $k and/or n = $n are too large to compute an eigendecomposition"</span>)

    <span class="keyword">var</span> ido = <span class="keyword">new</span> intW(<span class="number">0</span>)
    <span class="keyword">var</span> info = <span class="keyword">new</span> intW(<span class="number">0</span>)
    <span class="keyword">var</span> resid = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Double</span>](n)
    <span class="keyword">var</span> v = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Double</span>](n * ncv)
    <span class="keyword">var</span> workd = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Double</span>](n * <span class="number">3</span>)
    <span class="keyword">var</span> workl = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Double</span>](ncv * (ncv + <span class="number">8</span>))
    <span class="keyword">var</span> ipntr = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](<span class="number">11</span>)

    <span class="comment">// call ARPACK's reverse communication, first iteration with ido = 0</span>
    arpack.dsaupd(ido, bmat, n, which, nev.`<span class="function"><span class="keyword">val</span>`, <span class="title">tolW</span>, <span class="title">resid</span>, <span class="title">ncv</span>, <span class="title">v</span>, <span class="title">n</span>, <span class="title">iparam</span>, <span class="title">ipntr</span>, <span class="title">workd</span>,
</span>      workl, workl.length, info)

    <span class="function"><span class="keyword">val</span> <span class="title">w</span> =</span> <span class="type">BDV</span>(workd)

    <span class="comment">// ido = 99 : done flag in reverse communication</span>
    <span class="keyword">while</span> (ido.`<span class="function"><span class="keyword">val</span>` <span class="title">!=</span> 99) {</span>
      <span class="keyword">if</span> (ido.`<span class="function"><span class="keyword">val</span>` <span class="title">!=</span> <span class="title">-1</span> <span class="title">&amp;&amp;</span> <span class="title">ido</span>.`<span class="title">val</span>` <span class="title">!=</span> 1) {</span>
        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"ARPACK returns ido = "</span> + ido.`<span class="function"><span class="keyword">val</span>` <span class="title">+</span>
</span>            <span class="string">" This flag is not compatible with Mode 1: A*x = lambda*x, A symmetric."</span>)
      }
      <span class="comment">// multiply working vector with the matrix</span>
      <span class="function"><span class="keyword">val</span> <span class="title">inputOffset</span> =</span> ipntr(<span class="number">0</span>) - <span class="number">1</span>
      <span class="function"><span class="keyword">val</span> <span class="title">outputOffset</span> =</span> ipntr(<span class="number">1</span>) - <span class="number">1</span>
      <span class="function"><span class="keyword">val</span> <span class="title">x</span> =</span> w.slice(inputOffset, inputOffset + n)
      <span class="function"><span class="keyword">val</span> <span class="title">y</span> =</span> w.slice(outputOffset, outputOffset + n)
      y := mul(x)
      <span class="comment">// call ARPACK's reverse communication</span>
      arpack.dsaupd(ido, bmat, n, which, nev.`<span class="function"><span class="keyword">val</span>`, <span class="title">tolW</span>, <span class="title">resid</span>, <span class="title">ncv</span>, <span class="title">v</span>, <span class="title">n</span>, <span class="title">iparam</span>, <span class="title">ipntr</span>,
</span>        workd, workl, workl.length, info)
    }

    <span class="keyword">if</span> (info.`<span class="function"><span class="keyword">val</span>` <span class="title">!=</span> 0) {</span>
      info.`<span class="function"><span class="keyword">val</span>` <span class="title">match</span> {</span>
        <span class="keyword">case</span> <span class="number">1</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"ARPACK returns non-zero info = "</span> + info.`<span class="function"><span class="keyword">val</span>` <span class="title">+</span>
</span>            <span class="string">" Maximum number of iterations taken. (Refer ARPACK user guide for details)"</span>)
        <span class="keyword">case</span> <span class="number">3</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"ARPACK returns non-zero info = "</span> + info.`<span class="function"><span class="keyword">val</span>` <span class="title">+</span>
</span>            <span class="string">" No shifts could be applied. Try to increase NCV. "</span> +
            <span class="string">"(Refer ARPACK user guide for details)"</span>)
        <span class="keyword">case</span> _ =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"ARPACK returns non-zero info = "</span> + info.`<span class="function"><span class="keyword">val</span>` <span class="title">+</span>
</span>            <span class="string">" Please refer ARPACK user guide for error message."</span>)
      }
    }

    <span class="function"><span class="keyword">val</span> <span class="title">d</span> =</span> <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Double</span>](nev.`<span class="function"><span class="keyword">val</span>`)
</span>    <span class="function"><span class="keyword">val</span> <span class="title">select</span> =</span> <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Boolean</span>](ncv)
    <span class="comment">// copy the Ritz vectors</span>
    <span class="function"><span class="keyword">val</span> <span class="title">z</span> =</span> java.util.<span class="type">Arrays</span>.copyOfRange(v, <span class="number">0</span>, nev.`<span class="function"><span class="keyword">val</span>` <span class="title">*</span> <span class="title">n</span>)
</span>
    <span class="comment">// call ARPACK's post-processing for eigenvectors</span>
    arpack.dseupd(<span class="literal">true</span>, <span class="string">"A"</span>, select, d, z, n, <span class="number">0.0</span>, bmat, n, which, nev, tol, resid, ncv, v, n,
      iparam, ipntr, workd, workl, workl.length, info)

    <span class="comment">// number of computed eigenvalues, might be smaller than k</span>
    <span class="function"><span class="keyword">val</span> <span class="title">computed</span> =</span> iparam(<span class="number">4</span>)

    <span class="function"><span class="keyword">val</span> <span class="title">eigenPairs</span> =</span> java.util.<span class="type">Arrays</span>.copyOfRange(d, <span class="number">0</span>, computed).zipWithIndex.map { r =&gt;
      (r._1, java.util.<span class="type">Arrays</span>.copyOfRange(z, r._2 * n, r._2 * n + n))
    }

    <span class="comment">// sort the eigen-pairs in descending order</span>
    <span class="function"><span class="keyword">val</span> <span class="title">sortedEigenPairs</span> =</span> eigenPairs.sortBy(- _._1)

    <span class="comment">// copy eigenvectors in descending order of eigenvalues</span>
    <span class="function"><span class="keyword">val</span> <span class="title">sortedU</span> =</span> <span class="type">BDM</span>.zeros[<span class="type">Double</span>](n, computed)
    sortedEigenPairs.zipWithIndex.foreach { r =&gt;
      <span class="function"><span class="keyword">val</span> <span class="title">b</span> =</span> r._2 * n
      <span class="keyword">var</span> i = <span class="number">0</span>
      <span class="keyword">while</span> (i &lt; n) {
        sortedU.data(b + i) = r._1._2(i)
        i += <span class="number">1</span>
      }
    }

    (<span class="type">BDV</span>[<span class="type">Double</span>](sortedEigenPairs.map(_._1)), sortedU)
  }
}
</code></pre><p>最后来到关键的地方，所以其实spark内部的分布式求矩阵都是用svd的近似，不断调用Gramian方法来传递向量，用迭代的方式来求特征值和特征向量。最后返回的也是一个BDV，也就是breeze.linalg.DenseVector。我个人认为这种方法相比mpi，似乎一路上也没有checkpoint，效率可能还不一定比mpi做的高，毕竟MPI，BLAS，SCALAPACK这些库都是久经考验得工业标准了。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/06/21/explore-spark-distributed-mllib/" data-id="cicczjs5y001iwhv4fyhh4wj2" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/mllib/">mllib</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/scala/">scala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-spark-task-and-scheduler" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/06/09/spark-task-and-scheduler/" class="article-date">
  <time datetime="2015-06-09T13:27:45.000Z" itemprop="datePublished">2015-06-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/06/09/spark-task-and-scheduler/">spark 的调度和任务</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>一晃快一个月过去了，这个月有点小忙做了很多项目一直都没空闲重新研究spark的代码。今天晚上下决心看完。花了一点时间追了一下代码。基本算是搞清楚了spark的调度。<br>上次说到，spark会把记录了操作的rdd提交交给调度器等待运行。那调度器具体是怎么执行计算的呢？随手提交了一个作业，有如下日志</p>
<pre><code><span class="number">15</span><span class="regexp">/06/</span><span class="number">10</span> <span class="number">03</span>:<span class="number">05</span>:<span class="number">09</span> INFO <span class="string">DAGScheduler:</span> Submitting <span class="number">1</span> missing tasks from Stage <span class="number">1</span> (MapPartitionsRDD[<span class="number">2</span>] at map at Task2a.<span class="string">scala:</span><span class="number">8</span>)
<span class="number">15</span><span class="regexp">/06/</span><span class="number">10</span> <span class="number">03</span>:<span class="number">05</span>:<span class="number">09</span> INFO <span class="string">TaskSchedulerImpl:</span> Adding task set <span class="number">1.0</span> with <span class="number">1</span> tasks
<span class="number">15</span><span class="regexp">/06/</span><span class="number">10</span> <span class="number">03</span>:<span class="number">05</span>:<span class="number">09</span> INFO <span class="string">TaskSetManager:</span> Starting task <span class="number">0.0</span> <span class="keyword">in</span> stage <span class="number">1.0</span> (TID <span class="number">1</span>, localhost, PROCESS_LOCAL, <span class="number">1348</span> bytes)
<span class="number">15</span><span class="regexp">/06/</span><span class="number">10</span> <span class="number">03</span>:<span class="number">05</span>:<span class="number">09</span> INFO <span class="string">Executor:</span> Running task <span class="number">0.0</span> <span class="keyword">in</span> stage <span class="number">1.0</span> (TID <span class="number">1</span>)
<span class="number">15</span><span class="regexp">/06/</span><span class="number">10</span> <span class="number">03</span>:<span class="number">05</span>:<span class="number">09</span> INFO <span class="string">HadoopRDD:</span> Input <span class="string">split:</span> <span class="string">file:</span><span class="regexp">/home/</span>cloudera/<span class="number">1</span>millionTweets.<span class="string">tsv:</span><span class="number">0</span>+<span class="number">28775886</span>
<span class="number">15</span><span class="regexp">/06/</span><span class="number">10</span> <span class="number">03</span>:<span class="number">05</span>:<span class="number">10</span> INFO <span class="string">BlockManager:</span> Removing broadcast <span class="number">1</span>
<span class="number">15</span><span class="regexp">/06/</span><span class="number">10</span> <span class="number">03</span>:<span class="number">05</span>:<span class="number">10</span> INFO <span class="string">BlockManager:</span> Removing block broadcast_1_piece0
<span class="number">15</span><span class="regexp">/06/</span><span class="number">10</span> <span class="number">03</span>:<span class="number">05</span>:<span class="number">10</span> INFO <span class="string">MemoryStore:</span> Block broadcast_1_piece0 of size <span class="number">2039</span> dropped from memory (free <span class="number">280032800</span>)
</code></pre><p>我们可以看到，DAGScheduler将任务提交了以后其实是TaskSetManager执行任务的</p>
<pre><code>logInfo(<span class="string">"Starting <span class="variable">%s</span> (TID <span class="variable">%d</span>, <span class="variable">%s</span>, <span class="variable">%s</span>, <span class="variable">%d</span> bytes)"</span>.<span class="keyword">format</span>(
taskName, taskId, host, taskLocality, serializedTask.limit))
sched.dagScheduler.taskStarted(task, info)
<span class="keyword">return</span> Some(new TaskDescription(taskId = taskId, attemptNumber = attemptNum, execId,
taskName, <span class="keyword">index</span>, serializedTask))
</code></pre><p>这里调用了dagScheduler的taskstarted开始任务的而最后到了Executor执行，也就是这一段</p>
<pre><code><span class="comment">// Run the actual task and measure its runtime.</span>
taskStart = System.<span class="function"><span class="title">currentTimeMillis</span><span class="params">()</span></span>
val value = task.<span class="function"><span class="title">run</span><span class="params">(taskAttemptId = taskId, attemptNumber = attemptNumber)</span></span>
val taskFinish = System.<span class="function"><span class="title">currentTimeMillis</span><span class="params">()</span></span>
</code></pre><p>这里调用的是task.run来执行</p>
<pre><code>final <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(taskAttemptId: Long, attemptNumber: Int)</span>:</span> T = {
  context = new TaskContextImpl(stageId = stageId, partitionId = partitionId,
    taskAttemptId = taskAttemptId, attemptNumber = attemptNumber, runningLocally = false)
  TaskContextHelper.setTaskContext(context)
  context.taskMetrics.setHostname(Utils.localHostName())
  taskThread = Thread.currentThread()
  <span class="keyword">if</span> (_killed) {
    kill(interruptThread = false)
  }
  <span class="keyword">try</span> {
    runTask(context)
  } <span class="keyword">finally</span> {
    context.markTaskCompleted()
    TaskContextHelper.unset()
  }
 }
</code></pre><p>这是一个抽象类的final方法，基本可以看出大致的逻辑，调用自己的runTask执行任务完事。<br>例如，对于shuffle任务有一个</p>
<pre><code><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">ShuffleMapTask</span>(</span>
  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">runTask</span>(</span>context: <span class="type">TaskContext</span>): <span class="type">MapStatus</span> = {
    <span class="comment">// Deserialize the RDD using the broadcast variable.</span>
    <span class="function"><span class="keyword">val</span> <span class="title">ser</span> =</span> <span class="type">SparkEnv</span>.get.closureSerializer.newInstance()
    <span class="function"><span class="keyword">val</span> (</span>rdd, dep) = ser.deserialize[(<span class="type">RDD</span>[_], <span class="type">ShuffleDependency</span>[_, _, _])](
      <span class="type">ByteBuffer</span>.wrap(taskBinary.value), <span class="type">Thread</span>.currentThread.getContextClassLoader)

    metrics = <span class="type">Some</span>(context.taskMetrics)
    <span class="keyword">var</span> writer: <span class="type">ShuffleWriter</span>[<span class="type">Any</span>, <span class="type">Any</span>] = <span class="literal">null</span>
    <span class="keyword">try</span> {
      <span class="function"><span class="keyword">val</span> <span class="title">manager</span> =</span> <span class="type">SparkEnv</span>.get.shuffleManager
      writer = manager.getWriter[<span class="type">Any</span>, <span class="type">Any</span>](dep.shuffleHandle, partitionId, context)
      writer.write(rdd.iterator(partition, context).asInstanceOf[<span class="type">Iterator</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">Any</span>, <span class="type">Any</span>]]])
      <span class="keyword">return</span> writer.stop(success = <span class="literal">true</span>).get
    } <span class="keyword">catch</span> {
      <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;
        <span class="keyword">try</span> {
          <span class="keyword">if</span> (writer != <span class="literal">null</span>) {
            writer.stop(success = <span class="literal">false</span>)
          }
        } <span class="keyword">catch</span> {
          <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;
            log.debug(<span class="string">"Could not stop writer"</span>, e)
        }
        <span class="keyword">throw</span> e
    }
  }
</code></pre><p>每一个Task子类都有自己的衍生的Runtask方法，取决于各自的目的。<br>终于算是找到了核心运算单元了，继续看mllib和dataframe的代码</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/06/09/spark-task-and-scheduler/" data-id="cicczjs4y0009whv44mn8fpkd" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/scala/">scala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/scheduler/">scheduler</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/task/">task</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-explore-spark-RDD-source-code" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/15/explore-spark-RDD-source-code/" class="article-date">
  <time datetime="2015-05-15T08:14:17.000Z" itemprop="datePublished">2015-05-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/15/explore-spark-RDD-source-code/">expore spark RDD source code</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>一鼓作气再而衰三而竭。<br>开始阅读spark部分的代码，好在之前在coursera上看过一点scala虽然习题没做完但是不至于全忘。在spark里面，RDD的lazy evaluation也是通过和pyspark差不多的方法实现的，定义一个RDD类，每次操作都返回一个MapParitionsRDD，在这个MapParitionsRDD里记下前一个RDD和对应的函数。</p>
<pre><code>abstract <span class="class"><span class="keyword">class</span> <span class="title">RDD</span>[<span class="title">T</span>:</span> ClassTag](
    <span class="decorator">@transient private var _sc: SparkContext,</span>
    <span class="decorator">@transient private var deps: Seq[Dependency[_]]</span>
  ) extends Serializable <span class="keyword">with</span> Logging {

  <span class="keyword">if</span> (classOf[RDD[_]].isAssignableFrom(elementClassTag.runtimeClass)) {
    // This <span class="keyword">is</span> a warning instead of an exception <span class="keyword">in</span> order to avoid breaking user programs that
    // might have defined nested RDDs without running jobs <span class="keyword">with</span> them.
    logWarning(<span class="string">"Spark does not support nested RDDs (see SPARK-5063)"</span>)
  }

  private <span class="function"><span class="keyword">def</span> <span class="title">sc</span>:</span> SparkContext = {
    <span class="keyword">if</span> (_sc == null) {
      throw new SparkException(
        <span class="string">"RDD transformations and actions can only be invoked by the driver, not inside of other "</span> +
        <span class="string">"transformations; for example, rdd1.map(x =&gt; rdd2.values.count() * x) is invalid because "</span> +
        <span class="string">"the values transformation and count action cannot be performed inside of the rdd1.map "</span> +
        <span class="string">"transformation. For more information, see SPARK-5063."</span>)
    }
    _sc
  }


  <span class="function"><span class="keyword">def</span> <span class="title">map</span>[<span class="title">U</span>:</span> ClassTag](f: T =&gt; U): RDD[U] = {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.map(cleanF))
  }

  <span class="function"><span class="keyword">def</span> <span class="title">filter</span><span class="params">(f: T =&gt; Boolean)</span>:</span> RDD[T] = {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[T, T](
      this,
      (context, pid, iter) =&gt; iter.filter(cleanF),
      preservesPartitioning = true)
  }

  <span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsWithIndex</span>[<span class="title">U</span>:</span> ClassTag](
      f: (Int, Iterator[T]) =&gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = {
    val func = (context: TaskContext, index: Int, iter: Iterator[T]) =&gt; f(index, iter)
    new MapPartitionsRDD(this, sc.clean(func), preservesPartitioning)
  }
</code></pre><p>而MapPartitionsRDD仅仅记录下了动作和父RDD</p>
<pre><code>private[spark] class <span class="type">MapPartitionsRDD</span>[U: <span class="type">ClassTag</span>, T: <span class="type">ClassTag</span>](
    prev: <span class="type">RDD</span>[T],
    f: (<span class="type">TaskContext</span>, <span class="type">Int</span>, <span class="type">Iterator</span>[T]) =&gt; <span class="type">Iterator</span>[U],  // (<span class="type">TaskContext</span>, partition index, <span class="keyword">iterator</span>)
    preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>)
  extends <span class="type">RDD</span>[U](prev) {

  override val partitioner = <span class="keyword">if</span> (preservesPartitioning) firstParent[T].partitioner <span class="keyword">else</span> <span class="type">None</span>

  override def getPartitions: <span class="type">Array</span>[<span class="type">Partition</span>] = firstParent[T].partitions

  override def compute(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>) =
    f(context, split.index, firstParent[T].<span class="keyword">iterator</span>(split, context))
}
</code></pre><p>而只有collect和reduce这类函数是例外的，</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">collect</span><span class="params">()</span>:</span> Array[T] = {
    val results = sc.runJob(this, (iter: Iterator[T]) =&gt; iter.toArray)
    Array.concat(results: _*)
  }

<span class="function"><span class="keyword">def</span> <span class="title">reduce</span><span class="params">(f: <span class="params">(T, T)</span> =&gt; T)</span>:</span> T = {
    val cleanF = sc.clean(f)
    val reducePartition: Iterator[T] =&gt; Option[T] = iter =&gt; {
      <span class="keyword">if</span> (iter.hasNext) {
        Some(iter.reduceLeft(cleanF))
      } <span class="keyword">else</span> {
        <span class="keyword">None</span>
      }
    }
    var jobResult: Option[T] = <span class="keyword">None</span>
    val mergeResult = (index: Int, taskResult: Option[T]) =&gt; {
      <span class="keyword">if</span> (taskResult.isDefined) {
        jobResult = jobResult match {
          case Some(value) =&gt; Some(f(value, taskResult.get))
          case <span class="keyword">None</span> =&gt; taskResult
        }
      }
    }
    sc.runJob(this, reducePartition, mergeResult)
    // Get the final result out of our Option, <span class="keyword">or</span> throw an exception <span class="keyword">if</span> the RDD was empty
    jobResult.getOrElse(throw new UnsupportedOperationException(<span class="string">"empty collection"</span>))
  }
</code></pre><p>看代码可以知道collect这个动作做的时候其实是调用sc.runJob了，不再累积RDD类。而从SparkContext里的runJob里可以看到，scala shell其实是通过调用DAGScheduler.runjob让spark干活的</p>
<pre><code>def runJob[T, U: ClassTag](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) =&gt; U,
      partitions: Seq[<span class="typename">Int</span>],
      allowLocal: <span class="typename">Boolean</span>,
      resultHandler: (<span class="typename">Int</span>, U) =&gt; <span class="typename">Unit</span>) {
    <span class="keyword">if</span> (stopped) {
      <span class="keyword">throw</span> new IllegalStateException(<span class="string">"SparkContext has been shutdown"</span>)
    }
    <span class="variable"><span class="keyword">val</span> callSite</span> = getCallSite
    <span class="variable"><span class="keyword">val</span> cleanedFunc</span> = clean(func)
    logInfo(<span class="string">"Starting job: "</span> + callSite.shortForm)
    <span class="keyword">if</span> (conf.getBoolean(<span class="string">"spark.logLineage"</span>, <span class="literal">false</span>)) {
      logInfo(<span class="string">"RDD's recursive dependencies:\n"</span> + rdd.toDebugString)
    }
    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, allowLocal,
      resultHandler, localProperties.<span class="keyword">get</span>)
    progressBar.foreach(_.finishAll())
    rdd.doCheckpoint()
  }
</code></pre><p>对于dag调度器，他的runJob是用submitJob来提交的</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">runJob</span>[</span><span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](
      rdd: <span class="type">RDD</span>[<span class="type">T</span>],
      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,
      partitions: <span class="type">Seq</span>[<span class="type">Int</span>],
      callSite: <span class="type">CallSite</span>,
      allowLocal: <span class="type">Boolean</span>,
      resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>,
      properties: <span class="type">Properties</span>): <span class="type">Unit</span> = {
    <span class="function"><span class="keyword">val</span> <span class="title">start</span> =</span> <span class="type">System</span>.nanoTime
    <span class="function"><span class="keyword">val</span> <span class="title">waiter</span> =</span> submitJob(rdd, func, partitions, callSite, allowLocal, resultHandler, properties)
    waiter.awaitResult() <span class="keyword">match</span> {
      <span class="keyword">case</span> <span class="type">JobSucceeded</span> =&gt; {
        logInfo(<span class="string">"Job %d finished: %s, took %f s"</span>.format
          (waiter.jobId, callSite.shortForm, (<span class="type">System</span>.nanoTime - start) / <span class="number">1e9</span>))
      }
      <span class="keyword">case</span> <span class="type">JobFailed</span>(exception: <span class="type">Exception</span>) =&gt;
        logInfo(<span class="string">"Job %d failed: %s, took %f s"</span>.format
          (waiter.jobId, callSite.shortForm, (<span class="type">System</span>.nanoTime - start) / <span class="number">1e9</span>))
        <span class="keyword">throw</span> exception
    }
  }

  <span class="function"><span class="keyword">def</span> <span class="title">submitJob</span>[</span><span class="type">T</span>, <span class="type">U</span>](
      rdd: <span class="type">RDD</span>[<span class="type">T</span>],
      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,
      partitions: <span class="type">Seq</span>[<span class="type">Int</span>],
      callSite: <span class="type">CallSite</span>,
      allowLocal: <span class="type">Boolean</span>,
      resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>,
      properties: <span class="type">Properties</span>): <span class="type">JobWaiter</span>[<span class="type">U</span>] = {
    <span class="comment">// Check to make sure we are not launching a task on a partition that does not exist.</span>
    <span class="function"><span class="keyword">val</span> <span class="title">maxPartitions</span> =</span> rdd.partitions.length
    partitions.find(p =&gt; p &gt;= maxPartitions || p &lt; <span class="number">0</span>).foreach { p =&gt;
      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(
        <span class="string">"Attempting to access a non-existent partition: "</span> + p + <span class="string">". "</span> +
          <span class="string">"Total number of partitions: "</span> + maxPartitions)
    }

    <span class="function"><span class="keyword">val</span> <span class="title">jobId</span> =</span> nextJobId.getAndIncrement()
    <span class="keyword">if</span> (partitions.size == <span class="number">0</span>) {
      <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">JobWaiter</span>[<span class="type">U</span>](<span class="keyword">this</span>, jobId, <span class="number">0</span>, resultHandler)
    }

    assert(partitions.size &gt; <span class="number">0</span>)
    <span class="function"><span class="keyword">val</span> <span class="title">func2</span> =</span> func.asInstanceOf[(<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _]
    <span class="function"><span class="keyword">val</span> <span class="title">waiter</span> =</span> <span class="keyword">new</span> <span class="type">JobWaiter</span>(<span class="keyword">this</span>, jobId, partitions.size, resultHandler)
    eventProcessLoop.post(<span class="type">JobSubmitted</span>(
      jobId, rdd, func2, partitions.toArray, allowLocal, callSite, waiter, properties))
    waiter
  }
</code></pre><p>做了一系列安全性检查，起了一个jobwaiter等结果，然后继续调用JobSubmitted比把它提交到DAGSchedulerEventProcessLoop类里去，</p>
<pre><code><span class="keyword">private</span>[scheduler] <span class="class"><span class="keyword">class</span> <span class="title">DAGSchedulerEventProcessLoop</span>(</span>dagScheduler: <span class="type">DAGScheduler</span>)
  <span class="keyword">extends</span> <span class="type">EventLoop</span>[<span class="type">DAGSchedulerEvent</span>](<span class="string">"dag-scheduler-event-loop"</span>) <span class="keyword">with</span> <span class="type">Logging</span> {

  <span class="comment">/**
   * The main event loop of the DAG scheduler.
   */</span>
  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onReceive</span>(</span>event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = event <span class="keyword">match</span> {
    <span class="keyword">case</span> <span class="type">JobSubmitted</span>(jobId, rdd, func, partitions, allowLocal, callSite, listener, properties) =&gt;
      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, allowLocal, callSite,
        listener, properties)

...

  <span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">handleJobSubmitted</span>(</span>jobId: <span class="type">Int</span>,
      finalRDD: <span class="type">RDD</span>[_],
      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,
      partitions: <span class="type">Array</span>[<span class="type">Int</span>],
      allowLocal: <span class="type">Boolean</span>,
      callSite: <span class="type">CallSite</span>,
      listener: <span class="type">JobListener</span>,
      properties: <span class="type">Properties</span>) {
    <span class="keyword">var</span> finalStage: <span class="type">Stage</span> = <span class="literal">null</span>
    <span class="keyword">try</span> {
      <span class="comment">// New stage creation may throw an exception if, for example, jobs are run on a</span>
      <span class="comment">// HadoopRDD whose underlying HDFS files have been deleted.</span>
      finalStage = newStage(finalRDD, partitions.size, <span class="type">None</span>, jobId, callSite)
    } <span class="keyword">catch</span> {
      <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;
        logWarning(<span class="string">"Creating new stage failed due to exception - job: "</span> + jobId, e)
        listener.jobFailed(e)
        <span class="keyword">return</span>
    }
    <span class="keyword">if</span> (finalStage != <span class="literal">null</span>) {
      <span class="function"><span class="keyword">val</span> <span class="title">job</span> =</span> <span class="keyword">new</span> <span class="type">ActiveJob</span>(jobId, finalStage, func, partitions, callSite, listener, properties)
      clearCacheLocs()
      logInfo(<span class="string">"Got job %s (%s) with %d output partitions (allowLocal=%s)"</span>.format(
        job.jobId, callSite.shortForm, partitions.length, allowLocal))
      logInfo(<span class="string">"Final stage: "</span> + finalStage + <span class="string">"("</span> + finalStage.name + <span class="string">")"</span>)
      logInfo(<span class="string">"Parents of final stage: "</span> + finalStage.parents)
      logInfo(<span class="string">"Missing parents: "</span> + getMissingParentStages(finalStage))
      <span class="function"><span class="keyword">val</span> <span class="title">shouldRunLocally</span> =</span>
        localExecutionEnabled &amp;&amp; allowLocal &amp;&amp; finalStage.parents.isEmpty &amp;&amp; partitions.length == <span class="number">1</span>
      <span class="function"><span class="keyword">val</span> <span class="title">jobSubmissionTime</span> =</span> clock.getTimeMillis()
      <span class="keyword">if</span> (shouldRunLocally) {
        <span class="comment">// Compute very short actions like first() or take() with no parent stages locally.</span>
        listenerBus.post(
          <span class="type">SparkListenerJobStart</span>(job.jobId, jobSubmissionTime, <span class="type">Seq</span>.empty, properties))
        runLocally(job)
      } <span class="keyword">else</span> {
        jobIdToActiveJob(jobId) = job
        activeJobs += job
        finalStage.resultOfJob = <span class="type">Some</span>(job)
        <span class="function"><span class="keyword">val</span> <span class="title">stageIds</span> =</span> jobIdToStageIds(jobId).toArray
        <span class="function"><span class="keyword">val</span> <span class="title">stageInfos</span> =</span> stageIds.flatMap(id =&gt; stageIdToStage.get(id).map(_.latestInfo))
        listenerBus.post(
          <span class="type">SparkListenerJobStart</span>(job.jobId, jobSubmissionTime, stageInfos, properties))
        submitStage(finalStage)
      }
    }
    submitWaitingStages()
  }
</code></pre><p>包装了一大堆东西，然后扔到listenerBus里准备运行（这次是真的要运行了吧？）</p>
<pre><code><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">LiveListenerBus</span>
</span>  <span class="keyword">extends</span> <span class="type">AsynchronousListenerBus</span>[<span class="type">SparkListener</span>, <span class="type">SparkListenerEvent</span>](<span class="string">"SparkListenerBus"</span>)
  <span class="keyword">with</span> <span class="type">SparkListenerBus</span> {

  <span class="keyword">private</span> <span class="function"><span class="keyword">val</span> <span class="title">logDroppedEvent</span> =</span> <span class="keyword">new</span> <span class="type">AtomicBoolean</span>(<span class="literal">false</span>)

  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onDropEvent</span>(</span>event: <span class="type">SparkListenerEvent</span>): <span class="type">Unit</span> = {
    <span class="keyword">if</span> (logDroppedEvent.compareAndSet(<span class="literal">false</span>, <span class="literal">true</span>)) {
      <span class="comment">// Only log the following message once to avoid duplicated annoying logs.</span>
      logError(<span class="string">"Dropping SparkListenerEvent because no remaining room in event queue. "</span> +
        <span class="string">"This likely means one of the SparkListeners is too slow and cannot keep up with "</span> +
        <span class="string">"the rate at which tasks are being started by the scheduler."</span>)
    }
  }

}

<span class="keyword">private</span>[spark] <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">AsynchronousListenerBus</span>[</span><span class="type">L</span> &lt;: <span class="type">AnyRef</span>, <span class="type">E</span>](name: <span class="type">String</span>)
  <span class="keyword">extends</span> <span class="type">ListenerBus</span>[<span class="type">L</span>, <span class="type">E</span>] {
  <span class="function"><span class="keyword">def</span> <span class="title">post</span>(</span>event: <span class="type">E</span>) {
    <span class="keyword">if</span> (stopped.get) {
      <span class="comment">// Drop further events to make `listenerThread` exit ASAP</span>
      logError(s<span class="string">"$name has already stopped! Dropping event $event"</span>)
      <span class="keyword">return</span>
    }
    <span class="function"><span class="keyword">val</span> <span class="title">eventAdded</span> =</span> eventQueue.offer(event)
    <span class="keyword">if</span> (eventAdded) {
      eventLock.release()
    } <span class="keyword">else</span> {
      onDropEvent(event)
    }
  }
</code></pre><p>扔完以后开始submitStage把最后状态提交开始运行<br>所以跳到最后就是把作业提交完事，胸闷，还是没有看到RDD是怎么在不同节点间交互的代码。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/05/15/explore-spark-RDD-source-code/" data-id="cicczjs5z001mwhv420l1k7ij" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RDD/">RDD</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/scala/">scala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-spark-machine-learning-lib-for-python" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/15/spark-machine-learning-lib-for-python/" class="article-date">
  <time datetime="2015-05-15T05:29:26.000Z" itemprop="datePublished">2015-05-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/15/spark-machine-learning-lib-for-python/">spark machine learning lib for python</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>今天看了一下pyspark的mllib这个库。作为一个大数据计算平台，spark这点比hadoop向前走了一大步。提供了种类繁多的数据计算的模型，从随机数生成到线性代数到回归聚类。<br>粗粗的看了一下，pyspark在做这个mllib的时候实现有两种，一种是spark原生型的api调用，比如刚才提到的回归聚类，都是直接输入rdd然后不断迭代返回结果，比如</p>
<pre><code><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegressionWithSGD</span><span class="params">(object)</span>:</span>

    <span class="decorator">@classmethod</span>
    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(cls, data, iterations=<span class="number">100</span>, step=<span class="number">1.0</span>, miniBatchFraction=<span class="number">1.0</span>,
              initialWeights=None, regParam=<span class="number">0.01</span>, regType=<span class="string">"l2"</span>, intercept=False)</span>:</span>
        <span class="string">"""
        Train a logistic regression model on the given data.

        :param data:              The training data, an RDD of LabeledPoint.
        :param iterations:        The number of iterations (default: 100).
        :param step:              The step parameter used in SGD
                                  (default: 1.0).
        :param miniBatchFraction: Fraction of data to be used for each SGD
                                  iteration.
        :param initialWeights:    The initial weights (default: None).
        :param regParam:          The regularizer parameter (default: 0.01).
        :param regType:           The type of regularizer used for training
                                  our model.

                                  :Allowed values:
                                     - "l1" for using L1 regularization
                                     - "l2" for using L2 regularization
                                     - None for no regularization

                                     (default: "l2")

        :param intercept:         Boolean parameter which indicates the use
                                  or not of the augmented representation for
                                  training data (i.e. whether bias features
                                  are activated or not).
        """</span>
        <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(rdd, i)</span>:</span>
            <span class="keyword">return</span> callMLlibFunc(<span class="string">"trainLogisticRegressionModelWithSGD"</span>, rdd, int(iterations),
                                 float(step), float(miniBatchFraction), i, float(regParam), regType,
                                 bool(intercept))

        <span class="keyword">return</span> _regression_train_wrapper(train, LogisticRegressionModel, data, initialWeights)


<span class="function"><span class="keyword">def</span> <span class="title">callMLlibFunc</span><span class="params">(name, *args)</span>:</span>
    <span class="string">""" Call API in PythonMLLibAPI """</span>
    sc = SparkContext._active_spark_context
    api = getattr(sc._jvm.PythonMLLibAPI(), name)
    <span class="keyword">return</span> callJavaFunc(sc, api, *args)
</code></pre><p>第一类api全都把工作丢给了spark去做，也就是其实是跨节点并行计算。<br>第二类api就比较奇怪了，linalg里面全是这类函数，如</p>
<pre><code><span class="class"><span class="keyword">class</span> <span class="title">DenseVector</span><span class="params">(Vector)</span>:</span>
    <span class="string">"""
    A dense vector represented by a value array. We use numpy array for
    storage and arithmetics will be delegated to the underlying numpy
    array.

    def dot(self, other):
        """</span>
        Compute the dot product of two Vectors. We support
        (Numpy array, list, SparseVector, <span class="keyword">or</span> SciPy sparse)
        <span class="keyword">and</span> a target NumPy array that <span class="keyword">is</span> either <span class="number">1</span>- <span class="keyword">or</span> <span class="number">2</span>-dimensional.
        Equivalent to calling numpy.dot of the two vectors.
        <span class="keyword">if</span> type(other) == np.ndarray:
            <span class="keyword">if</span> other.ndim &gt; <span class="number">1</span>:
                <span class="keyword">assert</span> len(self) == other.shape[<span class="number">0</span>], <span class="string">"dimension mismatch"</span>
            <span class="keyword">return</span> np.dot(self.array, other)
        <span class="keyword">elif</span> _have_scipy <span class="keyword">and</span> scipy.sparse.issparse(other):
            <span class="keyword">assert</span> len(self) == other.shape[<span class="number">0</span>], <span class="string">"dimension mismatch"</span>
            <span class="keyword">return</span> other.transpose().dot(self.toArray())
        <span class="keyword">else</span>:
            <span class="keyword">assert</span> len(self) == _vector_size(other), <span class="string">"dimension mismatch"</span>
            <span class="keyword">if</span> isinstance(other, SparseVector):
                <span class="keyword">return</span> other.dot(self)
            <span class="keyword">elif</span> isinstance(other, Vector):
                <span class="keyword">return</span> np.dot(self.toArray(), other.toArray())
            <span class="keyword">else</span>:
                <span class="keyword">return</span> np.dot(self.toArray(), other)
</code></pre><p>居然全部都是丢给numpy去做，虽然numpy做这类计算的效率很高不假但这样就没有spark的跨节点并行的优势了，在计算的时候要注意。另外听说spark的运行效率首先于带宽，根据我有限的hpc经验来看，网络的延迟影响可能也会很大。而spark所在的网络最多也就是万兆网络，在做矩阵并行计算这类对网络延迟依赖很大的计算的时候可能会很慢，开发人员会不会因此考虑将不少矩阵计算本地化呢？不得而知，我很好奇下一个版本如果他们给出矩阵求逆以及对焦化或者svd是不是依然也使用本地计算。如果这个问题不解决可能对spark要称霸大数据平台有点影响。</p>
<p>好了，pyspark的代码看的差不多了，接下去就是用scala写的spark了，cross fingers</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/05/15/spark-machine-learning-lib-for-python/" data-id="cicczjs54000qwhv4uik6dv5p" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/mllib/">mllib</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-explore-the-source-of-pyspark-dataframe" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/14/explore-the-source-of-pyspark-dataframe/" class="article-date">
  <time datetime="2015-05-14T07:24:15.000Z" itemprop="datePublished">2015-05-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/14/explore-the-source-of-pyspark-dataframe/">pyspark dataframe的实现</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>dataframe是spark 1.3今年新推出的东西，但其实早期叫做SchemaRDD，这还可以在源代码里看到。不过按照SPARK east summit 2015上的说法，dataframe速度很快，这得益于全部用内核实现计算。<br>随便摘一段出来就能发现，Dataframe这个类里的大多数计算都是交由self._jdf实现的。</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">count</span><span class="params">(self)</span>:</span>
    <span class="string">"""Returns the number of rows in this :class:`DataFrame`.

    &gt;&gt;&gt; df.count()
    2L
    """</span>
    <span class="keyword">return</span> self._jdf.count()

<span class="function"><span class="keyword">def</span> <span class="title">collect</span><span class="params">(self)</span>:</span>
    <span class="string">"""Returns all the records as a list of :class:`Row`.

    &gt;&gt;&gt; df.collect()
    [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]
    """</span>
    <span class="keyword">with</span> SCCallSiteSync(self._sc) <span class="keyword">as</span> css:
        port = self._sc._jvm.PythonRDD.collectAndServe(self._jdf.javaToPython().rdd())
    rs = list(_load_from_socket(port, BatchedSerializer(PickleSerializer())))
    cls = _create_cls(self.schema)
    <span class="keyword">return</span> [cls(r) <span class="keyword">for</span> r <span class="keyword">in</span> rs]

<span class="function"><span class="keyword">def</span> <span class="title">limit</span><span class="params">(self, num)</span>:</span>
    <span class="string">"""Limits the result count to the number specified.

    &gt;&gt;&gt; df.limit(1).collect()
    [Row(age=2, name=u'Alice')]
    &gt;&gt;&gt; df.limit(0).collect()
    []
    """</span>
    jdf = self._jdf.limit(num)
    <span class="keyword">return</span> DataFrame(jdf, self.sql_ctx)
</code></pre><p>而self._jdf来自类初始化传入的值，通常如果用SQLContext创建DataFrame的话都来自这段代码</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">createDataFrame</span><span class="params">(self, data, schema=None, samplingRatio=None)</span>:</span>
   ...
   <span class="keyword">if</span> isinstance(schema, (list, tuple)):
       first = rdd.first()
       <span class="keyword">if</span> <span class="keyword">not</span> isinstance(first, (list, tuple)):
           <span class="keyword">raise</span> ValueError(<span class="string">"each row in `rdd` should be list or tuple, "</span>
                            <span class="string">"but got %r"</span> % type(first))
       row_cls = Row(*schema)
       schema = self._inferSchema(rdd.map(<span class="keyword">lambda</span> r: row_cls(*r)), samplingRatio)

   <span class="comment"># take the first few rows to verify schema</span>
   rows = rdd.take(<span class="number">10</span>)
   <span class="comment"># Row() cannot been deserialized by Pyrolite</span>
   <span class="keyword">if</span> rows <span class="keyword">and</span> isinstance(rows[<span class="number">0</span>], tuple) <span class="keyword">and</span> rows[<span class="number">0</span>].__class__.__name__ == <span class="string">'Row'</span>:
       rdd = rdd.map(tuple)
       rows = rdd.take(<span class="number">10</span>)

   <span class="keyword">for</span> row <span class="keyword">in</span> rows:
       _verify_type(row, schema)

   <span class="comment"># convert python objects to sql data</span>
   converter = _python_to_sql_converter(schema)
   rdd = rdd.map(converter)

   jrdd = self._jvm.SerDeUtil.toJavaArray(rdd._to_java_object_rdd())
   df = self._ssql_ctx.applySchemaToPythonRDD(jrdd.rdd(), schema.json())
   <span class="keyword">return</span> DataFrame(df, self)
</code></pre><p>而这里的self._ssql_ctx来自于同一个类里的</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">_ssql_ctx</span><span class="params">(self)</span>:</span>
    <span class="string">"""Accessor for the JVM Spark SQL context.

    Subclasses can override this property to provide their own
    JVM Contexts.
    """</span>
    <span class="keyword">if</span> self._scala_SQLContext <span class="keyword">is</span> <span class="keyword">None</span>:
        self._scala_SQLContext = self._jvm.SQLContext(self._jsc.sc())
    <span class="keyword">return</span> self._scala_SQLContext
</code></pre><p>是对scala_SQLContext的一个封装。包括sql语句的执行也是直接调用_ssql_ctx</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">sql</span><span class="params">(self, sqlQuery)</span>:</span>
    <span class="string">"""Returns a :class:`DataFrame` representing the result of the given query.

    &gt;&gt;&gt; sqlContext.registerDataFrameAsTable(df, "table1")
    &gt;&gt;&gt; df2 = sqlContext.sql("SELECT field1 AS f1, field2 as f2 from table1")
    &gt;&gt;&gt; df2.collect()
    [Row(f1=1, f2=u'row1'), Row(f1=2, f2=u'row2'), Row(f1=3, f2=u'row3')]
    """</span>
    <span class="keyword">return</span> DataFrame(self._ssql_ctx.sql(sqlQuery), self)
</code></pre><p>而原先的SchemaRDD则不用了，不过看1.2.2的代码似乎也是调用_ssql_ctx来实现计算的，不能明白为什么1.3和1.2能性能差那么多。这还有待于阅读spark的代码来解答。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/05/14/explore-the-source-of-pyspark-dataframe/" data-id="cicczjs5t001dwhv4mp5fod5j" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/dataframe/">dataframe</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-spark-rdd-pipeline-lazyevaluation" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/14/spark-rdd-pipeline-lazyevaluation/" class="article-date">
  <time datetime="2015-05-14T02:28:14.000Z" itemprop="datePublished">2015-05-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/14/spark-rdd-pipeline-lazyevaluation/">spark rdd, pipeline, lazyevaluation</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>一直以来写代码不求甚解，感觉这样不好，从今天开始起读各数据框架的源代码，学习学习再学习  </p>
<p>今天看的是pyspark里lazy evaluation的处理，python和scala不同不是函数式的。那这是怎么办到的呢？  </p>
<p>首先所有的数据集在spark内部都叫做rdd，这在pyspark里也有定义：  </p>
<pre><code><span class="keyword">class</span> RDD(object):

    <span class="string">""</span>"
    A Resilient Distributed Dataset (RDD), the basic abstraction <span class="keyword">in</span> Spark.
    Represents <span class="keyword">an</span> immutable, partitioned collection of elements that can be
    operated <span class="keyword">on</span> <span class="keyword">in</span> parallel.
</code></pre><p>RDD内部实现了很多函数，有map，filter这类一个集合对一个集合的映射，也有collect，reduce这种一个集合到一个值的映射。</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">map</span><span class="params">(self, f, preservesPartitioning=False)</span>:</span>
    <span class="string">"""
    Return a new RDD by applying a function to each element of this RDD.

    &gt;&gt;&gt; rdd = sc.parallelize(["b", "a", "c"])
    &gt;&gt;&gt; sorted(rdd.map(lambda x: (x, 1)).collect())
    [('a', 1), ('b', 1), ('c', 1)]
    """</span>
    <span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(_, iterator)</span>:</span>
        <span class="keyword">return</span> imap(f, iterator)
    <span class="keyword">return</span> self.mapPartitionsWithIndex(func, preservesPartitioning)

<span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsWithIndex</span><span class="params">(self, f, preservesPartitioning=False)</span>:</span>
    <span class="string">"""
    Return a new RDD by applying a function to each partition of this RDD,
    while tracking the index of the original partition.

    &gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4], 4)
    &gt;&gt;&gt; def f(splitIndex, iterator): yield splitIndex
    &gt;&gt;&gt; rdd.mapPartitionsWithIndex(f).sum()
    6
    """</span>
    <span class="keyword">return</span> PipelinedRDD(self, f, preservesPartitioning)
</code></pre><p>对于map filter这类函数来说，他们每次操作都是产生一个叫做PipelinedRDD的对象，那这个PipelinedRDD又是干什么的呢？</p>
<pre><code><span class="class"><span class="keyword">class</span> <span class="title">PipelinedRDD</span><span class="params">(RDD)</span>:</span>

    <span class="string">"""
    Pipelined maps:

    &gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4])
    &gt;&gt;&gt; rdd.map(lambda x: 2 * x).cache().map(lambda x: 2 * x).collect()
    [4, 8, 12, 16]
    &gt;&gt;&gt; rdd.map(lambda x: 2 * x).map(lambda x: 2 * x).collect()
    [4, 8, 12, 16]

    Pipelined reduces:
    &gt;&gt;&gt; from operator import add
    &gt;&gt;&gt; rdd.map(lambda x: 2 * x).reduce(add)
    20
    &gt;&gt;&gt; rdd.flatMap(lambda x: [x, x]).reduce(add)
    20
    """</span>

    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, prev, func, preservesPartitioning=False)</span>:</span>
        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(prev, PipelinedRDD) <span class="keyword">or</span> <span class="keyword">not</span> prev._is_pipelinable():
            <span class="comment"># This transformation is the first in its stage:</span>
            self.func = func
            self.preservesPartitioning = preservesPartitioning
            self._prev_jrdd = prev._jrdd
            self._prev_jrdd_deserializer = prev._jrdd_deserializer
        <span class="keyword">else</span>:
            prev_func = prev.func

            <span class="function"><span class="keyword">def</span> <span class="title">pipeline_func</span><span class="params">(split, iterator)</span>:</span>
                <span class="keyword">return</span> func(split, prev_func(split, iterator))
            self.func = pipeline_func
            self.preservesPartitioning = \
                prev.preservesPartitioning <span class="keyword">and</span> preservesPartitioning
            self._prev_jrdd = prev._prev_jrdd  <span class="comment"># maintain the pipeline</span>
            self._prev_jrdd_deserializer = prev._prev_jrdd_deserializer
        self.is_cached = <span class="keyword">False</span>
        self.is_checkpointed = <span class="keyword">False</span>
        self.ctx = prev.ctx
        self.prev = prev
        self._jrdd_val = <span class="keyword">None</span>
        self._id = <span class="keyword">None</span>
        self._jrdd_deserializer = self.ctx.serializer
        self._bypass_serializer = <span class="keyword">False</span>
        self.partitioner = prev.partitioner <span class="keyword">if</span> self.preservesPartitioning <span class="keyword">else</span> <span class="keyword">None</span>
        self._broadcast = <span class="keyword">None</span>
</code></pre><p>我们可以看到，PipelinedRDD只是记录下当前操作但不执行所以每做一次rdd操作，只是记录下了对应的映射关系，数据集还是在原始状态。只有当使用到了reduce这类函数时才会被执行计算。</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">mean</span><span class="params">(self)</span>:</span>
    <span class="string">"""
    Compute the mean of this RDD's elements.

    &gt;&gt;&gt; sc.parallelize([1, 2, 3]).mean()
    2.0
    """</span>
    <span class="keyword">return</span> self.stats().mean()

<span class="function"><span class="keyword">def</span> <span class="title">stats</span><span class="params">(self)</span>:</span>
    <span class="string">"""
    Return a L{StatCounter} object that captures the mean, variance
    and count of the RDD's elements in one operation.
    """</span>
    <span class="function"><span class="keyword">def</span> <span class="title">redFunc</span><span class="params">(left_counter, right_counter)</span>:</span>
        <span class="keyword">return</span> left_counter.mergeStats(right_counter)

    <span class="keyword">return</span> self.mapPartitions(<span class="keyword">lambda</span> i: [StatCounter(i)]).reduce(redFunc)
</code></pre><p>这里，也是回到了PipelinedRDD，但是这次就不只保存待执行的函数了，而是通过jrdd执行</p>
<pre><code>@property
def _jrdd(self):
    <span class="keyword">if</span> self._jrdd_val:
        return self._jrdd_val
    <span class="keyword">if</span> self._bypass_serializer:
        self._jrdd_deserializer = <span class="function"><span class="title">NoOpSerializer</span><span class="params">()</span></span>

    <span class="keyword">if</span> self<span class="class">.ctx</span><span class="class">.profiler_collector</span>:
        profiler = self<span class="class">.ctx</span><span class="class">.profiler_collector</span><span class="class">.new_profiler</span>(self.ctx)
    <span class="keyword">else</span>:
        profiler = None

    command = (self<span class="class">.func</span>, profiler, self._prev_jrdd_deserializer,
               self._jrdd_deserializer)
    pickled_cmd, bvars, env, includes = _prepare_for_python_RDD(self<span class="class">.ctx</span>, command, self)
    python_rdd = self<span class="class">.ctx</span>._jvm.<span class="function"><span class="title">PythonRDD</span><span class="params">(self._prev_jrdd.rdd()</span></span>,
                                         <span class="function"><span class="title">bytearray</span><span class="params">(pickled_cmd)</span></span>,
                                         env, includes, self<span class="class">.preservesPartitioning</span>,
                                         self<span class="class">.ctx</span><span class="class">.pythonExec</span>,
                                         bvars, self<span class="class">.ctx</span>._javaAccumulator)
    self._jrdd_val = python_rdd.<span class="function"><span class="title">asJavaRDD</span><span class="params">()</span></span>

    <span class="keyword">if</span> profiler:
        self._id = self._jrdd_val.<span class="function"><span class="title">id</span><span class="params">()</span></span>
        self<span class="class">.ctx</span><span class="class">.profiler_collector</span><span class="class">.add_profiler</span>(self._id, profiler)
    return self._jrdd_val
</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/05/14/spark-rdd-pipeline-lazyevaluation/" data-id="cicczjs51000iwhv4crfv9k39" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/pyspark/">pyspark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/rdd/">rdd</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</section>
        
          <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/RDD/">RDD</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/benchmark/">benchmark</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/blog/">blog</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cnblogs/">cnblogs</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cython/">cython</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/dataframe/">dataframe</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/f2py/">f2py</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/github/">github</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/">hexo</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/migrate/">migrate</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mllib/">mllib</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/numba/">numba</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/numpy/">numpy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pyspark/">pyspark</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rdd/">rdd</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rust/">rust</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala/">scala</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scheduler/">scheduler</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/">spark</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/task/">task</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/RDD/" style="font-size: 10px;">RDD</a><a href="/tags/benchmark/" style="font-size: 10px;">benchmark</a><a href="/tags/blog/" style="font-size: 12.5px;">blog</a><a href="/tags/cnblogs/" style="font-size: 10px;">cnblogs</a><a href="/tags/cython/" style="font-size: 10px;">cython</a><a href="/tags/dataframe/" style="font-size: 10px;">dataframe</a><a href="/tags/f2py/" style="font-size: 10px;">f2py</a><a href="/tags/github/" style="font-size: 10px;">github</a><a href="/tags/hexo/" style="font-size: 12.5px;">hexo</a><a href="/tags/migrate/" style="font-size: 10px;">migrate</a><a href="/tags/mllib/" style="font-size: 12.5px;">mllib</a><a href="/tags/numba/" style="font-size: 10px;">numba</a><a href="/tags/numpy/" style="font-size: 10px;">numpy</a><a href="/tags/pyspark/" style="font-size: 10px;">pyspark</a><a href="/tags/python/" style="font-size: 17.5px;">python</a><a href="/tags/rdd/" style="font-size: 10px;">rdd</a><a href="/tags/rust/" style="font-size: 17.5px;">rust</a><a href="/tags/scala/" style="font-size: 15px;">scala</a><a href="/tags/scheduler/" style="font-size: 10px;">scheduler</a><a href="/tags/spark/" style="font-size: 20px;">spark</a><a href="/tags/task/" style="font-size: 10px;">task</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">July 2015</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/06/">June 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/04/">April 2015</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/01/">January 2015</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/09/">September 2014</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/06/">June 2014</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/12/">December 2013</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/11/">November 2013</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/10/">October 2013</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/09/">September 2013</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/08/">August 2013</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/04/">April 2013</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/06/">June 2012</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/04/">April 2012</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/03/">March 2012</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/01/">January 2012</a><span class="archive-list-count">4</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2015/07/20/learning-rust-part-IV/">rust学习笔记(四)</a>
          </li>
        
          <li>
            <a href="/2015/07/17/learning-rust-part-III/">rust学习笔记(三)</a>
          </li>
        
          <li>
            <a href="/2015/07/17/learning-rust-part-II/">rust学习笔记(二)</a>
          </li>
        
          <li>
            <a href="/2015/07/16/learning-rust-part-I/">rust学习笔记(一)</a>
          </li>
        
          <li>
            <a href="/2015/06/21/explore-spark-distributed-mllib/">spark的mllib到底是不是分布式的</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 Yuankun Shi<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/about/index.html" class="mobile-nav-link">About</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>
</body>
</html>